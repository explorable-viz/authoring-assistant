#Launch prompts workflow
name: PromptExecutor
'on':
  workflow_dispatch:
  push:
    paths:
      - fluidLLMTool/**
  pull_request:
    paths:
      - fluidLLMTool/**
jobs:
  prompt-test-ubuntu:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 22
      - uses: actions/setup-java@v4
        with:
          distribution: 'temurin' # See 'Supported distributions' for available options
          java-version: '22'
      - name: Install Compiler
        working-directory: ./fluidLLMTool
        run: |
          yarn install-fluid-compiler
      - name: Check out Fluid Repo
        working-directory: ./fluidLLMTool/fluid-parser
        run: |
          git clone https://github.com/explorable-viz/fluid.git fluid-repo
          mv fluid-repo/fluid .
      - name: Deploy LLM Tool
        working-directory: ./fluidLLMTool
        run: |
          yarn build-executor
        env:
          GITHUB_TOKEN: ${{ secrets.LLM_PROMPT_EXECUTOR_TOKEN }}
      - name: Install ollama
        working-directory: ./fluidLLMTool
        run: |
          yarn --cwd ./fluidLLMTool ollama-install
      - name: Run ollama
        working-directory: ./fluidLLMTool
        run: |
          yarn ollama-serve
      - name: Pull llama3.1
        working-directory: ./fluidLLMTool
        run: |
          yarn pull-llama3-1
      - name: Launch Prompt Tests (10 sentences)
        working-directory: ./fluidLLMTool
        run: |
          yarn test
#  prompt-test-macos:
#    runs-on: macos-latest
#    steps:
#      - uses: actions/checkout@v4
#      - uses: actions/setup-node@v4
#        with:
#          node-version: 22
#      - uses: actions/setup-java@v4
#        with:
#          distribution: 'temurin' # See 'Supported distributions' for available options
#          java-version: '21'
#      - name: Install ollama
#        run: |
#          brew install ollama
#      - name: Run ollama
#        run: |
#          yarn ollama-serve
#      - name: Pull llama3
#        run: |
#          yarn pull-llama3
#      - name: Invoke via API
#        run: |
#          curl -s http://localhost:11434/api/generate -d '{
#          "model": "llama3",
#          "stream": false,
#          "prompt":"What are the risks of running privileged Docker containers for CI workloads?"
#          }' | jq
#      - name: Test
#        run: |
#          yarn test
