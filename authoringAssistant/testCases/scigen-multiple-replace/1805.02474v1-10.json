{
  "datasets": [
    {
      "var": "tableData",
      "file": "datasets/scigen/1805.02474v1-10"
    }
  ],
  "imports": ["scigen"],
  "variables": {},
  "expected": {
    "num_1": "head (map (fun x -> x.time_s) (filter (fun x -> x.model == \"LSTM\") tableData))",
    "num_2": "head (map (fun x -> x.time_s) (filter (fun x -> x.model == \"BiLSTM\") tableData))",
    "num_3": "head (map (fun x -> x.time_s) (filter (fun x -> x.model == \"2 stacked BiLSTM\") tableData))",
    "num_4": "head (map (fun x -> x.acc) (filter (fun x -> x.model == \"S-LSTM\") tableData))",
    "num_5": "head (map (fun x -> x.time_s) (filter (fun x -> x.model == \"S-LSTM\") tableData))",
    "num_6": "head (map (fun x -> x.acc) (filter (fun x -> x.model == \"3 stacked CNN\") tableData))",
    "model": "head (map (fun x -> x.model) (filter (fun x -> x.time_s == minimum (map (fun y -> y.time_s) tableData)) tableData))",
    "ordinal": "let pos = get_rank \"CNN\" tableData in ordinal pos\n"
  },
  "paragraph": [
    {
      "type": "literal",
      "value": "As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM2, with the training time per epoch growing from [REPLACE id=\"num_1\"] seconds to [REPLACE id=\"num_2\"] seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of [REPLACE id=\"num_3\"] seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of [REPLACE id=\"num_4\"]%, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of [REPLACE id=\"num_5\"] seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), [REPLACE id=\"model\"] is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of [REPLACE id=\"num_6\"]%, which is also the [REPLACE id=\"ordinal\"] compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used  Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperforming BiLSTM significantly."
    }
  ]
}
