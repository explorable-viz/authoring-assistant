\section{Generating Fluid Code}
\label{sec:generate-fluid}

The first problem -- and one potentially without any research component -- is to be able
to generate \kw{fluid} programs that access the aggregate of data reference by the surrounding text.
To solve this problem we will need to accumulate some research on code-generation from LLM's.

\subsection{Possible Modules}
We first need to identify the relevant ``tasks'' from the wider body of NLP/LLM literature,
since each task is likely to comprise a module of the overall system. Here we will attempt
to describe the modules required to completely characterize a language-model code-generation
system.

\paragraph*{Pre-Trained Models and Few-Short Learning}
Models that are pre-trained on a wide range of unlabelled data have been successful in LLM
code generation with the most notable pre-trained models being BERT \citep{devlin2019}
and GPT \citep{radford2018}. One of the starting points is to begin with a pre-trained model
that has acquired some internal representation of programming concepts. Then we may be able
to fine-tune it to work with a more specific representation of fluid programs.

Fine-tuning for our use-case may not be feasible. Currently there are only 2-3 \kw{fluid}
programmers in the world, limiting the number of examples we can use to fine-tune a model 
(unless we can come up with a data augmentation process for fluid programs). In light of this
it is also possible that we will have to make use of the generalization capability of 
LLM's -- taking a pre-trained code LLM, and using a combination of annotated code and 
well engineered prompts -- to coax out better performance from our comparatively small
set of example programs. The use of a small number of examples is known in the literature
as ``Few-Shot Learning''. For a survey of this in GPT3, see \citet{brown2020neurips}.

\paragraph*{Representation of \kw{fluid} programs}
Our tool will need to combine information from a natural language context (a \kw{LinkedText} list)
with a formal model of \kw{fluid} programs. Moreover, the natural language context (NLC)
\textit{is itself a value in a fluid program}. In recent years, with language models applied to tasks
involving code, code representation learning has revealed itself to be a vitally important task.
It seems like learning of representations is specific to the task-domain to which the language model
is being applied. Will leave further investigation of code representation learning until we've 
established more of the overall structure of the problem we aim to solve.

\paragraph*{Prompt Engineering}
We need to provide specifications to the language models we employ, in the form of what are called
``prompts''. Prompt engineering refers to the task of tailoring the format of a query given to a 
language model. In our case, prompts will consist of a combination of information being sent to the
language model, the shape of which remains to be chosen.
Our prompts will likely include some combination of the following:
\begin{itemize}
   \item \textit{Natural language context:} source code fragments are to be synthesized and spliced into a natural language context. For now it's best to imagine this context as being a textual description of some visualization.
   \item \textit{Source program code:} the natural language context provides context as to the visualization it is helping to describe, the other bit of context is the source file which generates the visualization itself.
   \item \textit{Library code:} source programs very rarely contain the entirety of relevant definitions, so including relevant library code in a prompt seems useful. One key question is how much code should be considered as context for a prompt versus being included in the training of a model. Perhaps this is best to answer empirically when we are further along in the research.
\end{itemize}

It remains to be seen how we format the above information and pass it to the models API.
As far I can tell, this process is empirically driven at the moment, so we need
to get some experiments working first, alongside catching up with the literature.