\section{AI-Assisted Authoring Workflow}
\label{sec:authoring-workflow}

Figure~\ref{fig:overall-architecture} shows the system architecture and an example flow.
The system is composed of two agents:

\begin{itemize}
    \item \textbf{\SuggestionAgent}. LLM-based agent which identifies text fragments potentially computable from data.
    \item \textbf{\InterpretationAgent}. LLM-based agent which, for a given text fragment (provided by
    SuggestionAgent or by the author), attempts to synthesise a Fluid expression computing the target text.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/interpretation-agent}
    \caption{Human-in-the-Loop \InterpretationAgent workflow (Author intervention states in grey)}
    \label{fig:interpretation-agent}
\end{figure}

Figure~\ref{fig:interpretation-agent} shows the overall architecture of the \InterpretationAgent. The author
initially provides the text and accompanying data. The \SuggestionAgent analyses the input and identifies
candidate fragments of natural language which could be replaced by Fluid expressions. The user highlightes a
fragment of interest, which is then sent to the \InterpretationAgent, which generates a corresponding
candidate expression. Internally the \InterpretationAgent uses a compiler-in-the-loop validation process to
ensure the expression is well-formed. The user is then able to accept the expression (in which case it is
incorporated into the program) or reject and leave the text uninterpreted.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-correct}
        \caption{Author Acceptance pathway}
        \label{fig:data-flow-correct}
    \end{subfigure}\hfill
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-error}
        \caption{Author Rejection pathway}
        \label{fig:dataflow-error}
    \end{subfigure}
    \caption{Two possible paths through the basic editing loop: (a) author accepts generated expression after
    interactively verifying its correctness; (b) author rejects proposed expression, having identified an
    error (natural language mentions accuracy, while interaction reveals use of timing data).}
    \label{fig:overall-architecture}
\end{figure}


\subsection{System Prompt}
\label{subsec:system-prompt}

In the system prompt used to guide the Interpretation Assistant in replacing placeholder tags in user-authored
paragraphs with executable Fluid expressions, we include the following information about the transparent
document under construction, the exact task to perform, and constraints on the expected output. The full
system prompt is given in \appref{system-prompt:interpretation-agent}.

\begin{itemize}
\item \textbf{Input structure:} summarises the imported datasets, modules, auxililiary definitions in scope in
the current file, the current Fluid representation of the target paragraph with \kw{[REPLACE â€¦]} tag
indicating the text fragment to replace, plus the full text for the paragraph.
\item \textbf{Task description:} description of the LLM task, namely to identify the placeholder, generate a
Fluid expression that produces the target string using supplied dataset and definitions in scope.
\item \textbf{Output constraints:} specify that the LLM should return a syntactically valid Fluid expression,
with no extraneous content (such as comments).
\end{itemize}

\subsection{Editor Loop}
\label{subsec:editor-loop}
Design of main loop that would be integrated into an IDE. The configuration (state) that is being maintained
is a \kw{Paragraph} the user is authoring in Fluid. Consists of a sequence of text fragments, some of which
are uninterpreted (plain literals), the remainder have underlying expressions linking the text to raw or derived
data.

Workflow:
\begin{enumerate}
\item User selects (a substring of) one of the literal text fragments, indicating they want to link it in this
way
\item Authoring tool generates expression which is either:
  \begin{enumerate}
  \item computes the selected text (e.g.~``greater than'' might be computed by comparing two numbers)
  \item becomes the formal meaning of the selected text (e.g. the text ``carbon intensity of methane
emissions'' can be understood in a specific context as referring to the numerical value ``34 gCO2eq/kWh''
  \end{enumerate}
\item Some kind of validation step, including both automated validation (e.g. checking for runtime errors) and
user validation; if validation fails, goto (2)
\item Partition code into additional definitions and expression; add definitions to main source file, and
incorporate expression into \kw{Paragraph}
\item Goto (1) with updated editor state
\end{enumerate}

\subsection{Loopback System}
\label{subsec:loopback-system}

Expressions generated by the \InterpretationAgent are evaluated by the Fluid interpreter. If evaluation fails,
the error is used to extend the prompt and the extended prompt is sent back to the LLM for regeneration. This
loopback mechanism enables iterative refinement of expressions, improving accuracy and robustness. There are
three kinds of loopback error:

\begin{itemize}
    \item \textbf{Invalid Program}. Evaluation fails due a syntax error, undeclared identifier, division by
    zero, array index out of bounds or other runtime problem.
    \item \textbf{Invalid Expression Type}. The value of the expression is not convertible to a string.
    \item \textbf{Mismatching String}. The string value of the expression is not equal to the target text.
\end{itemize}

\subsection{Paragraph with generated expression}
\label{subsec:paragraph-with-generated-expression}
Figure \ref{fig:fluid-example-paragraph} shows the Fluid code of a Paragraph with the expression generated by
the authoring assistant.

\subsection{Turning validation errors into improved prompts}\label{subsec:turning-validation-errors-into-improved-prompts}
E.g.:
\begin{enumerate}
\item Turn ``Definition not found'' into prompt to generate definition?
\end{enumerate}

\subsection{IDE integration}\label{subsec:ide-integration}

Could then be integrated into desktop IDE like VSCode or online IDE like CodeMirror. Might make a good
internship project, but could also be out-of-scope for this paper.

\subsection{Other potential enhancements}\label{subsec:other-potential-enhancements}

We could also think about using an LLM in a couple of other complementary ways:
\begin{itemize}
\item identifying text fragments which might be linked;
\item validating generated expressions (perhaps by proposing test cases)
\end{itemize}


