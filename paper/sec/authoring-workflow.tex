\section{AI-Assisted Authoring Workflow}
\label{sec:authoring-workflow}

Our authoring tool is composed of two LLM-based agents:

\begin{itemize}
    \item \textbf{\SuggestionAgent}. Identifies text fragments potentially computable from data.
    \item \textbf{\InterpretationAgent}. For a given text fragment (provided by SuggestionAgent or author),
    attempts to synthesise Fluid expression computing target fragment.
\end{itemize}

The high-level workflow is as follows:

\begin{enumerate}

\item \textbf{Initial configuration.} Author imports text and accompanying data into the system, creating a
programmatic representation of the target document. Initially this representation is isomorphic to the target
text, namely a string literal of the form \kw{"""..."""}, where the triple quotes are Fluid syntax for an
\emph{interpolated string}, i.e.~a string literal where expressions of the form \kw{\{$e$\}} are permitted
within the string. The \SuggestionAgent analyses these inputs and identifies any fragments of natural language
which are candidates for being computed instead of remaining as literal substrings.

\item \textbf{Authoring workflow.} The system then enters the human-in-the loop authoring workflow shown in
Figure~\ref{fig:interpretation-agent}, where the author interacts with the \InterpretationAgent. The system
initially waits for the author to select a fragment of text $s$ to interpret, a fragment which may or may not
have been highlighted previously by the \SuggestionAgent. The system then generates a candidate Fluid
expression $e$ by prompting the LLM (\secref{prompt-design} below). The expression is validated through a
closed loop program synthesis step which checks that it is well-formed and evaluates to the target fragment
$s$. If this process succeeds, the system replaces the selected substring $s$ with the interpolation
expression \kw{\{$e$\}}, creating a new document configuration. This is the primary successful path through
the workflow and returns the system to the entry state where it is waiting for another top-level interaction
from the author.

\end{enumerate}

Otherwise, the Fluid command-line either returned an error (either a syntax error or runtime error), or
succeeded but $e$ evaluated to some other string $s' \neq s$. Both cases trigger prompt augmentation with an
appropriate error message, and the system retries generation. Failure of the code synthesis step means that,
even after multiple retries with an augmented prompt, $e$ remained invalid or evaluated to $s' \neq s$. If $e$
was invalid (resulting in an error), no remedial action is open to the author. This is considered an
unsuccessful path through the workflow and returns the system to the start state. In the $s' \neq s$ case, the
can choose to manually abort and return to the entry state, or optionally to \emph{revise the goal}, replacing
$s$ with $s'$ in the target document and retaining $e$ as the candidate expression. This is intended to cover
the situation where the author has made a claim which is \emph{incorrect}, but the data set and surrounding
natural language has caused the LLM to synthesise an expression which generates a different value from the one
specified by the user.

\paragraph{Manual validation step.} Once a valid expression is approved, the updated text is spliced back into
the document, and any downstream interactions (e.g. reloading the web page) are checked.

This human-in-the-loop design combines automated synthesis with validation and author oversight, providing a
substantial level of automation, but requiring the author to intervene at key steps to ensure correctness.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/interpretation-agent}
    \caption{Human-in-the-Loop workflow (states requiring human intervention in grey)}
    \label{fig:interpretation-agent}
\end{figure}

\subsection{Prompt Design}
\label{sec:prompt-design}

\paragraph{System Prompt.}
For the Interpretation Assistant, the system prompt guides the LLM in replacing placeholder tags in the
supplied paragraph with executable Fluid expressions. It includes the following information about the document
under construction, the exact task to perform, and constraints on the expected output. The full system prompt
is given in \appref{system-prompt:interpretation-agent}.

\begin{itemize}
\item \textbf{Input structure:} summarises the imported datasets, modules, auxililiary definitions in scope in
the current file, the current Fluid representation of the target paragraph with \kw{[REPLACE …]} tag
indicating the text fragment to replace, plus the full text for the paragraph.
\item \textbf{Task description:} description of the LLM task, namely to identify \kw{[REPLACE …]} placeholder
and generate Fluid expression that produces target string using supplied dataset and definitions in scope.
\item \textbf{Output constraints:} specify that the LLM should return a syntactically valid Fluid expression,
with no extraneous content (such as comments).
\end{itemize}

\rpnote{Now something about the rest of the prompt.}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-correct}
        \caption{Author Acceptance pathway}
        \label{fig:data-flow-correct}
    \end{subfigure}\hfill
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-error}
        \caption{Author Rejection pathway}
        \label{fig:dataflow-error}
    \end{subfigure}
    \caption{Two possible paths through the basic editing loop: (a) author accepts generated expression after
    interactively verifying its correctness; (b) author rejects proposed expression, having identified an
    error (natural language mentions accuracy, while interaction reveals use of timing data).}
    \label{fig:overall-architecture}
\end{figure}

\subsection{Loopback System}
\label{subsec:loopback-system}

Expressions generated by the \InterpretationAgent are evaluated by the Fluid interpreter. If evaluation fails,
the error is used to extend the prompt and the extended prompt is sent back to the LLM for regeneration. This
loopback mechanism enables iterative refinement of expressions, improving accuracy and robustness. There are
three kinds of loopback error:

\begin{itemize}
    \item \textbf{Invalid Program}. Evaluation fails due a syntax error, undeclared identifier, division by
    zero, array index out of bounds or other runtime problem.
    \item \textbf{Invalid Expression Type}. The value of the expression is not convertible to a string.
    \item \textbf{Mismatching String}. The string value of the expression is not equal to the target text.
\end{itemize}
