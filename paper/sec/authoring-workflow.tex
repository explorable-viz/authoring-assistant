\section{AI-Assisted Authoring Workflow}
\label{sec:authoring-workflow}

Figure~\ref{fig:overall-architecture} shows the system architecture and an example flow.
The system is composed of two LLM-based agents:

\begin{itemize}
    \item \textbf{\SuggestionAgent}. Identifies text fragments potentially computable from data.
    \item \textbf{\InterpretationAgent}. For a given text fragment (provided by SuggestionAgent or author),
    attempts to synthesise Fluid expression computing target fragment.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/interpretation-agent}
    \caption{Human-in-the-Loop \InterpretationAgent workflow (Author intervention states in grey)}
    \label{fig:interpretation-agent}
\end{figure}

The author initially provides the text and accompanying data. The \SuggestionAgent analyses the inputs and
identifies candidate fragments of natural language which could be replaced by Fluid expressions.
Figure~\ref{fig:interpretation-agent} then shows the human-in-the loop workflow of the \InterpretationAgent.
The entry point is the selection by the author of a fragment of text $s$ to interpret, a fragment which may or
may not have been highlighted previously by the \SuggestionAgent. The system then generates a candidate Fluid
expression $e$ by prompting the LLM (\secref{prompt-design} below). The expression is validated through a
closed loop program synthesis step to check that it is well-formed, and, if so, that it evaluates to the
target fragment of text $s$. If $e$ indeed evaluates to $s$, then the system replaces $s$ in the original
top-level string with the interpolation expression \kw{\{$e$\}}. This is the primary successful path through
the loop and returns the system to the ideal state where it is waiting for another top-level interaction from
the author.

Otherwise, the Fluid command-line either returned an error (either a syntax error or runtime error), or it
succeeded but $e$ evaluated to some other string $s' \neq s$. Both cases trigger prompt augmentation with an
appropriate error message, and the system retries generation. In cases where the expression remains invalid or
mismatched, the user may intervene by resolving the mismatch manually or rejecting the candidate. Once a valid
expression is approved, the updated text is spliced back into the document, and any downstream interactions
(e.g. reloading the web page) are checked. This closed-loop design combines automated synthesis with
validation and author oversight, ensuring both correctness and usability of the generated code.

The user highlights a fragment of interest, which is then sent to the \InterpretationAgent, which generates a
corresponding candidate expression. Internally the \InterpretationAgent uses a compiler-in-the-loop validation
process to ensure the expression is well-formed. The user is then able to accept the expression (in which case
it is incorporated into the program) or reject and leave the text uninterpreted.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-correct}
        \caption{Author Acceptance pathway}
        \label{fig:data-flow-correct}
    \end{subfigure}\hfill
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-error}
        \caption{Author Rejection pathway}
        \label{fig:dataflow-error}
    \end{subfigure}
    \caption{Two possible paths through the basic editing loop: (a) author accepts generated expression after
    interactively verifying its correctness; (b) author rejects proposed expression, having identified an
    error (natural language mentions accuracy, while interaction reveals use of timing data).}
    \label{fig:overall-architecture}
\end{figure}


\subsection{Prompt Design}
\label{sec:prompt-design}

In the system prompt used to guide the Interpretation Assistant in replacing placeholder tags in user-authored
paragraphs with executable Fluid expressions, we include the following information about the transparent
document under construction, the exact task to perform, and constraints on the expected output. The full
system prompt is given in \appref{system-prompt:interpretation-agent}.

\begin{itemize}
\item \textbf{Input structure:} summarises the imported datasets, modules, auxililiary definitions in scope in
the current file, the current Fluid representation of the target paragraph with \kw{[REPLACE …]} tag
indicating the text fragment to replace, plus the full text for the paragraph.
\item \textbf{Task description:} description of the LLM task, namely to identify \kw{[REPLACE …]} placeholder
and generate Fluid expression that produces target string using supplied dataset and definitions in scope.
\item \textbf{Output constraints:} specify that the LLM should return a syntactically valid Fluid expression,
with no extraneous content (such as comments).
\end{itemize}

\subsection{Editor Loop}
\label{subsec:editor-loop}
Design of main loop that would be integrated into an IDE. The configuration (state) that is being maintained
is a \kw{Paragraph} the user is authoring in Fluid. Consists of a sequence of text fragments, some of which
are uninterpreted (plain literals), the remainder have underlying expressions linking the text to raw or derived
data.

Workflow:
\begin{enumerate}
\item User selects (a substring of) one of the literal text fragments, indicating they want to link it in this
way
\item Authoring tool generates expression which is either:
  \begin{enumerate}
  \item computes the selected text (e.g.~``greater than'' might be computed by comparing two numbers)
  \item becomes the formal meaning of the selected text (e.g. the text ``carbon intensity of methane
emissions'' can be understood in a specific context as referring to the numerical value ``34 gCO2eq/kWh''
  \end{enumerate}
\item Some kind of validation step, including both automated validation (e.g. checking for runtime errors) and
user validation; if validation fails, goto (2)
\item Partition code into additional definitions and expression; add definitions to main source file, and
incorporate expression into \kw{Paragraph}
\item Goto (1) with updated editor state
\end{enumerate}

\subsection{Loopback System}
\label{subsec:loopback-system}

Expressions generated by the \InterpretationAgent are evaluated by the Fluid interpreter. If evaluation fails,
the error is used to extend the prompt and the extended prompt is sent back to the LLM for regeneration. This
loopback mechanism enables iterative refinement of expressions, improving accuracy and robustness. There are
three kinds of loopback error:

\begin{itemize}
    \item \textbf{Invalid Program}. Evaluation fails due a syntax error, undeclared identifier, division by
    zero, array index out of bounds or other runtime problem.
    \item \textbf{Invalid Expression Type}. The value of the expression is not convertible to a string.
    \item \textbf{Mismatching String}. The string value of the expression is not equal to the target text.
\end{itemize}

\subsection{Paragraph with generated expression}
\label{subsec:paragraph-with-generated-expression}
Figure \ref{fig:fluid-example-paragraph} shows the Fluid code of a Paragraph with the expression generated by
the authoring assistant.

\subsection{Turning validation errors into improved prompts}\label{subsec:turning-validation-errors-into-improved-prompts}
E.g.:
\begin{enumerate}
\item Turn ``Definition not found'' into prompt to generate definition?
\end{enumerate}

\subsection{IDE integration}\label{subsec:ide-integration}

Could then be integrated into desktop IDE like VSCode or online IDE like CodeMirror. Might make a good
internship project, but could also be out-of-scope for this paper.

\subsection{Other potential enhancements}\label{subsec:other-potential-enhancements}

We could also think about using an LLM in a couple of other complementary ways:
\begin{itemize}
\item identifying text fragments which might be linked;
\item validating generated expressions (perhaps by proposing test cases)
\end{itemize}


