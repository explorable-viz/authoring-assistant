\section{AI-Assisted Authoring Workflow}
\label{sec:authoring-workflow}

Figure~\ref{fig:overall-architecture} shows the system architecture and an example flow.
The system is composed of two LLM-based agents:

\begin{itemize}
    \item \textbf{\SuggestionAgent}. Identifies text fragments potentially computable from data.
    \item \textbf{\InterpretationAgent}. For a given text fragment (provided by SuggestionAgent or author),
    attempts to synthesise Fluid expression computing target fragment.
\end{itemize}

The author initially imports the text and accompanying data into the system, which creates a Fluid
representation of the target document. Initially that representation is just a string literal of the form
\kw{"""..."""}, where the triple quotes are the Fluid syntax for an \emph{interpolated string}, i.e.~a string
literal where expressions of the form \kw{\{$e$\}} are permitted within the string. The \SuggestionAgent
analyses the inputs and identifies candidate fragments of natural language which could be replaced by Fluid
expressions. Figure~\ref{fig:interpretation-agent} then shows the human-in-the loop workflow of the
\InterpretationAgent. The entry point is the selection by the author of a fragment of text $s$ to interpret, a
fragment which may or may not have been highlighted previously by the \SuggestionAgent. The system then
generates a candidate Fluid expression $e$ by prompting the LLM (\secref{prompt-design} below). The expression
is validated through a closed loop program synthesis step to check that it is well-formed, and, if so, that it
evaluates to the target fragment of text $s$. If $e$ indeed evaluates to $s$, then the system replaces $s$ in
the original top-level string with the interpolation expression \kw{\{$e$\}}. This is the primary successful
path through the loop and returns the system to the ideal state where it is waiting for another top-level
interaction from the author.

Otherwise, the Fluid command-line either returned an error (either a syntax error or runtime error), or it
succeeded but $e$ evaluated to some other string $s' \neq s$. Both cases trigger prompt augmentation with an
appropriate error message, and the system retries generation. In cases where the expression remains invalid or
mismatched, the user may intervene by resolving the mismatch manually or rejecting the candidate. Once a valid
expression is approved, the updated text is spliced back into the document, and any downstream interactions
(e.g. reloading the web page) are checked. This closed-loop design combines automated synthesis with
validation and author oversight, ensuring both correctness and usability of the generated code.

The user highlights a fragment of interest, which is then sent to the \InterpretationAgent, which generates a
corresponding candidate expression. Internally the \InterpretationAgent uses a compiler-in-the-loop validation
process to ensure the expression is well-formed. The user is then able to accept the expression (in which case
it is incorporated into the program) or reject and leave the text uninterpreted.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/interpretation-agent}
    \caption{Human-in-the-Loop workflow (states requiring human intervention in grey)}
    \label{fig:interpretation-agent}
\end{figure}

\subsection{Prompt Design}
\label{sec:prompt-design}

\paragraph{System Prompt.}
For the Interpretation Assistant, the system prompt guides the LLM in replacing placeholder tags in the
supplied paragraph with executable Fluid expressions. It includes the following information about the document
under construction, the exact task to perform, and constraints on the expected output. The full system prompt
is given in \appref{system-prompt:interpretation-agent}.

\begin{itemize}
\item \textbf{Input structure:} summarises the imported datasets, modules, auxililiary definitions in scope in
the current file, the current Fluid representation of the target paragraph with \kw{[REPLACE …]} tag
indicating the text fragment to replace, plus the full text for the paragraph.
\item \textbf{Task description:} description of the LLM task, namely to identify \kw{[REPLACE …]} placeholder
and generate Fluid expression that produces target string using supplied dataset and definitions in scope.
\item \textbf{Output constraints:} specify that the LLM should return a syntactically valid Fluid expression,
with no extraneous content (such as comments).
\end{itemize}

\rpnote{Now something about the rest of the prompt.}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-correct}
        \caption{Author Acceptance pathway}
        \label{fig:data-flow-correct}
    \end{subfigure}\hfill
    \begin{subfigure}{0.50\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/data-flow-error}
        \caption{Author Rejection pathway}
        \label{fig:dataflow-error}
    \end{subfigure}
    \caption{Two possible paths through the basic editing loop: (a) author accepts generated expression after
    interactively verifying its correctness; (b) author rejects proposed expression, having identified an
    error (natural language mentions accuracy, while interaction reveals use of timing data).}
    \label{fig:overall-architecture}
\end{figure}

\subsection{Loopback System}
\label{subsec:loopback-system}

Expressions generated by the \InterpretationAgent are evaluated by the Fluid interpreter. If evaluation fails,
the error is used to extend the prompt and the extended prompt is sent back to the LLM for regeneration. This
loopback mechanism enables iterative refinement of expressions, improving accuracy and robustness. There are
three kinds of loopback error:

\begin{itemize}
    \item \textbf{Invalid Program}. Evaluation fails due a syntax error, undeclared identifier, division by
    zero, array index out of bounds or other runtime problem.
    \item \textbf{Invalid Expression Type}. The value of the expression is not convertible to a string.
    \item \textbf{Mismatching String}. The string value of the expression is not equal to the target text.
\end{itemize}

\subsection{Paragraph with generated expression}
\label{subsec:paragraph-with-generated-expression}
Figure \ref{fig:fluid-example-paragraph} shows the Fluid code of a Paragraph with the expression generated by
the authoring assistant.
