\section{Experimental Evaluation}
\label{sec:evaluation}

\subsection{Research questions}

Our evaluation tests the ability of the \InterpretationAgent to translate quantitative and semi-quantitative
expressions from scholarly natural language into executable queries that operate on the underlying dataset.
Beyond raw accuracy, we are also concerned with how performance varies with task complexity, and whether the
generated expressions are robust under changes to data or in the presence of ambiguity or other low data
quality issues. These are captured in two research questions:

\paragraph{RQ1. Interpretation Accuracy across Linguistic Idioms and Complexity.}

To what extent can LLMs accurately interpret quantitative and semi-quantitative claims in scholarly text as
data queries? We examine performance across a range of linguistic idioms (e.g. averages, percentages, min/max,
ranks, as summarised in Table~\ref{tab:natural-language-forms}) and investigate how accuracy varies with task
complexity, measured (somewhat crudely and approximately) by the number of query sub-expressions (e.g.
retrieval, aggregation, or arithmetic) present in the gold solution.

\paragraph{RQ2. Generalisability and Robustness.}

How well do the generated expressions generalise when the underlying data changes, or when the input contains
misleading or ill-specified information? We test whether generated queries continue to produce correct outputs
under a set of hand-generated counterfactual modifications of the dataset, based on expected query results
specific to each test case, and also how counterfactual performance is impacted by the presence of misleading
or adversarial phrasing. able~\ref{tab:problematic_cases} shows some of cases which are ``problematic'' in
this sense; in these cases, producing a valid expression is likely to be challenging because of ambiguities in
the input data or accompanying natural language.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/success_rate_by_category}
        \caption{Success rate by Linguistic Category}
        \label{fig:success_rate_by_category}
    \end{subfigure} \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/success_rate_by_complexity}
        \caption{Success rate by Complexity}
        \label{fig:success_rate_by_complexity}
    \end{subfigure}
    \caption{Success rate of the proposed system, measured over 5 runs with \gptfour}
    \label{fig:success_rate_comparison}
\end{figure}

\subsection{Results}


%\begin{table}[h!]
%    \scriptsize
%
%    \centering
%    \begin{subtable}{0.45\linewidth}
%        \centering
%        \begin{tabular}{lcccc}
%            \hline
%            & \multicolumn{2}{c}{\textbf{\gptfour}} & \multicolumn{2}{c}{\textbf{\gptfive}} \\
%            & \textbf{Success Rate (\%)} & \textbf{SD (\%)} & \textbf{Success Rate} & \textbf{SD (\%)} \\
%            \hline
%            Comparison     & 68.2   & 3.2 & 77.4 & 2 \\
%            Data Retrieval & 77.3   & 3.7 & 77.4 & 2 \\
%            Difference     & 20.0   & 44.7 & 77.4 & 2 \\
%            Min/max        & 97.0   & 5.6 & 77.4 & 2 \\
%            Rank           & 0.0   & 0.0 & 77.4 & 2 \\
%            Sum            & 100   & 0.0 & 77.4 & 2 \\
%            Ratio          & 100   & 0.0 & 77.4 & 2 \\
%            Agerage        & 100   & 0.0 & 77.4 & 2 \\
%            Overall        & 74.9 & 3 & 77.4 & 2 \\
%            \hline
%        \end{tabular}
%        \caption{Results with target-value = true}
%        \label{tab:results_true}
%    \end{subtable}%
%    \hfill
%    \begin{subtable}{0.45\linewidth}
%        \centering
%        \begin{tabular}{lcccc}
%            \hline
%            & \multicolumn{2}{c}{\textbf{\gptfour}} & \multicolumn{2}{c}{\textbf{\gptfive}} \\
%            & \textbf{Success Rate (\%)} & \textbf{SD (\%)} & \textbf{Success Rate} & \textbf{SD (\%)} \\
%            \hline
%            Comparison     & 33.3 & 3.1 & -- & -- \\
%            Data Retrieval & 62.0 & 1.6 & -- & -- \\
%            Difference     & 75.0 & 50.0 & -- & -- \\
%            Min/max        & 90.6 & 12.6 & -- & -- \\
%            Rank           & 0.0 & 0.0 & -- & -- \\
%            Sum            & 100.0 & 0.0 & -- & -- \\
%            Ratio          & 100.0 & 0.0 & -- & -- \\
%            Average        & 100.0 & 0.0 & -- & -- \\
%            Overall        & 57.1 & 0.8 & -- & -- \\
%            \hline
%        \end{tabular}
%        \caption{Results without target-value}
%        \label{tab:results_no_target}
%    \end{subtable}
%    \caption{Success rates (with standard deviation) for \textbf{\gptfour} and \textbf{\gptfive} across different task categories}
%    \label{tab:results_combined}
%\end{table}


%\figref{success_rate_by_category} shows the results obtained in generating expression by categories.


%Table \ref{tab:results_combined} summarizes the performance of \gptfour and the most recent OpenAI model, \gptfive, under two different settings: with target-value sharing enabled and without target-value sharing.
%Overall, the system produced a valid Fluid expression in 74.9\% of the cases (S.D. 3\%).



%Figure~\ref{fig:success_rate_by_category} illustrates the success rate of \gptfour % under two different settings: with target-value sharing enabled and without target-value sharing.
%for each category, both with and without target-value sharing.
%Overall, the system produced a valid Fluid expression in 74.9\% of the cases (S.D. 3\%) \cdnote{this needs to be updated for target sharing and not target sharing or removed}.

%Table \ref{tab:results_combined} summarizes the performance of \gptfour and the most recent OpenAI model,
%\gptfive, under two different settings: with target-value sharing enabled and without target-value sharing.
%Overall, the system produced a valid Fluid expression in 74.9\% of the cases (S.D. 3\%).

To evaluate RQ1, we used a sample of the \SciGen dataset~\citep{moosavi21}, an open source dataset consisting
of tables from scientific articles and their corresponding descriptions. We aggregated the generated
expression in the linguistic categories from Table~\ref{tab:natural-language-forms}.
Figure~\ref{fig:success_rate_by_category} illustrates the success rate for each category, both with and
without target-value sharing. The results show that providing the target value (text fragment or numerical
value) is crucial for achieving a high success rate. Overall the \InterpretationAgent generates a correct
expression in approximately in 74.9\% of the cases (S.D. 3.0\%).

The system achieved performance above 68\% in comparison, 77.3\% in data retrieval, and 97\% in min/max search tasks.
In contrast, accuracy decreased significantly in expressions requiring differences (20\%) and in ranking tasks (0\%).

Figure~\ref{fig:success_rate_by_complexity} reports the success rate as a function of the number of categories assigned to each expression.
Fluid instructions combining multiple operations were associated with multiple categories.
Results show that the success rate was 62\% for simple expressions (one category), increased to 91\% for two categories,
while no correct expression was generated for expression with 3 categories.

In addition to evaluating performance when the target expression is provided, we also assessed the modelâ€™s ability to produce a
correct expression when no reference string was given.
On the \SciGen dataset, the model achieved a 74.9\% success rate when provided with the target string.
Without the target string, performance decreased moderately to 57.1\%, indicating that the model retains substantial
ability to reconstruct valid solutions even without explicit guidance

\subsection{Counterfactual testing}
\label{sec:validation-with-counterfactual-data}

For the \emph{counterfactual testing} part of our analysis, the input data is deliberately modified in order
to verify the consistency of the behaviours. Both the expected and generated expressions are executed again
and the results are compared. If discrepancies are found the \InterpretationAgent ask for a new generation
sending back that the failure is related to a counterfactual error.

%\rpnote{complete}

Across 300 test executions, 121 contained at least one counterfactual error (avg. 3.8), of which 42 ultimately
succeeded. Such tests help detect expressions that may yield the expected output but not the correct solution.
For example, the \InterpretationAgent generated:

\begin{verbatim}
(findWithKey_ "model" "LSTM2" tableData).time_s
\end{verbatim}

intended to retrieve the execution time of the \texttt{LSTM} model, but incorrectly referred to \texttt{LSTM2}.
Counterfactual testing exposed this error and prompted regeneration.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l p{6cm} p{4cm}}
        \hline
        \textbf{Problem Type} & \textbf{Example} & \textbf{Explanation} \\
        \hline
        false comparison &
        BiLSTM is \hl{the most efficient} among all models compared, with the highest model size &
        BiLSTM is not the most efficient, nor does it have the largest size. \\

        wrong numerical value &
        LSTM is the fastest model with overall time taken being \hl{90} seconds &
        It is not 90 but 106. \\

        ambiguous referent &
        LSTM is the fastest model with overall time taken being \hl{90} seconds &
        There are two type of time in the dataset (training\_time, execution\_time), both with a value of 90 seconds. \\

        \hline
    \end{tabular}
    \caption{Categories of problematic example}
    \label{tab:problematic_cases}
\end{table}
