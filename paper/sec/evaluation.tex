\section{Experimental Evaluation}
\label{sec:evaluation}

\subsection{Research questions}

Our evaluation tests the ability of the \InterpretationAgent to translate quantitative and semi-quantitative expressions
from scholarly natural language into executable queries that operate on data. Beyond raw accuracy, we are also concerned
with how performance varies with task complexity, and whether the generated expressions are robust under changes to data
or in the presence of ambiguity or other low data quality issues. These are captured in two research questions:

\paragraph{RQ1. Interpretation Accuracy across Query Categories and Complexity.}

To what extent can LLMs accurately interpret quantitative and semi-quantitative claims in scholarly text as data
queries? We examine performance across the various ``query categories'' summarised in
Table~\ref{tab:natural-language-forms} (namely averages, percentages, min/max, rank, etc), and investigate how accuracy
varies with task complexity --- measured, somewhat crudely, by the cardinality of the multiset of query categories
present in the gold solution. \todo{Two ablation studies to mention here}

\paragraph{RQ2. Generalisability and Robustness.}

As well as the missing or degraded information explored by the ablation studies, also pertinent to our application
domain is how well the generated expressions generalise when the underlying data changes or when the input contains
incorrect or intentionally misleading information. We test this using a set of hand-generated counterfactual
modifications of the test data, based on expected query results specific to each test case. \todo{Revisit}

\begin{figure*}
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/benchmark/gpt-4o/scigen-manual-old/success_rate_by_category_boxplot}
        \caption{Success rate by Linguistic Category}
        \label{fig:success_rate_by_category}
    \end{subfigure} \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/benchmark/gpt-4o/scigen-manual-old/success_rate_by_complexity}
        \caption{Success rate by Complexity}
        \label{fig:success_rate_by_complexity}
    \end{subfigure}
    \caption{Success rate of the proposed system, measured over 5 runs with \gptfour}
    \label{fig:success_rate_comparison}
\end{figure*}

\subsection{Results}

\paragraph{Interpretation Accuracy across Query Categories and Complexity.} To evaluate RQ1, we used a sample of the
\SciGen dataset~\citep{moosavi2021}, an open source dataset consisting of tables from scientific articles and their
corresponding descriptions. \todo{Table summarising overall results first, without disaggregation, including ablation
studies} The results show that \emph{the system is robust when provided with sufficient guidance but degrades when
underspecified \todo{Revisit}.} Overall, the InterpretationAgent produced correct Fluid expressions in 74.9\% (S.D.
3.0\%) of cases, but performance dropped to 57.1\% when the target was withheld. \todo{Generate these summary numbers
into a LaTeX file, or we'll forever be updating the document}

Performance also varied across the linguistic categories in Table~\ref{tab:natural-language-forms};
Figure~\ref{fig:success_rate_by_category} disaggregates by category, under both ablation studies. Success rates exceeded
68\% for comparison, 77.3\% for data retrieval, and 97\% for min/max search tasks. In contrast, accuracy decreased
significantly for expressions requiring differences (20\%) and for ranking tasks (0\%).

The trend for compositional complexity is more nuanced as shown in Figure~\ref{fig:success_rate_by_complexity}, which
reports the success rate as a function of the number of categories assigned to each expression: success rates are 62\%
for single-category expressions, increase to 91\% when two categories are combined, but collapse to 0\% when three
categories are involved. This suggests that \emph{moderate composition can actually aid performance, perhaps by giving
the model clearer structural cues, but that complexity beyond a certain threshold overwhelms the synthesis process
\todo{revisit}}.

\paragraph{Generalisability and Robustness.} As a preliminary attempt to address RQ2, we carried out
\emph{counterfactual testing} to evaluate the robustness of generated expressions under changes to the underlying data.
In this setup, the input tables were modified according to hand-craft test specifications, and both the expected and
generated expressions were re-executed to check whether the behaviours remained consistent. Across 300 test executions,
121 contained at least one counterfactual error (an average of 3.8 per case), of which 42 ultimately still succeeded
\todo{revisit -- not sure what this means}. These tests highlight cases where an expression may coincidentally yield the
correct output on the original data but fails to be extensionally equivalent more generally (i.e.~under perturbation).
For example, in one test the system generated

\hspace{3mm} \kw{(findWithKey\_ "model" "LSTM2" tableData).time\_s}

intended to retrieve the execution time of the \kw{LSTM} model, but incorrectly referred to \kw{LSTM2}.
Counterfactual testing exposed this mismatch, which would otherwise have gone undetected.

At present, counterfactual tests are used only as an evaluation device, not as part of the authoring workflow
itself. For future work (\Secref{conclusion}), we plan to investigate automatic generation of counterfactual
tests, allowing these additional robustness checks to be integrated into the document authoring workflow.

% \input{fig/adversarial-problem-types}
