\section{Introduction: Transparent, Data-Driven Documents}

Making sense of and verifying data-driven claims is hard, even when the evidence base lives in the public
domain. A key challenge lies in tracing specific claims back to the underlying data. In peer review, for
example, empirical claims typically lack author-supplied links to data, making them hard for reviewers to
check directly~\citep{weber20}. Paper retractions, meanwhile, are often attributable not to fraud, but to
simple errors in data management or analysis~\citep{hu25}. Large language models (LLMs) can assist with
fact-checking~\citep{abu-ahmad25} and interpretation of charts and figures~\citep{roberts24}, but current LLM
interfaces do not support direct interrogation of visual or other outputs.

Recent work on data provenance and data visualisation~\citep{psallidas18smoke,bond25}, on the other hand,
takes a more infrastructural approach to the transparency problem. These approaches link computed outputs to
their data sources directly, by tracking dependency information. This allows visual outputs to support
\emph{provenance queries}, user interactions (e.g. mousing over a visual element) that reveal how visual
features relate to data. The advantage of this approach is that the interactions are provided automatically
via trusted infrastructure, namely a query language or general-purpose programming language which tracks how
data flows through a computation. However, these approaches are limited to outputs computed from data, such as
visualisations. What is missing is a way to extend these ``direct interrogation'' features to natural language
itself, where the main claims of most scholarly articles are actually made.

In this paper, we address this gap by combining these two approaches, leveraging the ability of LLMs to
understand technical language and synthesise queries over data, plus the provenance-tracking infrastructure of
an open source programming language called Fluid (\url{https://f.luid.org/}) designed originally for
transparent visualisations~\citep{perera22,bond25}. We show how these two technologies can together support
the creation and deployment of \emph{transparent documents}, web-based scholarly articles where key
quantitative claims expressed in natural language --- such as the claim that one software system is faster
than another at solving a problem --- are computed from the relevant data, and that support interactive
provenance queries that reveal to a reader or reviewer the specific data elements that support the claim.

\figref{scigen-example-website}, adapted from \cite{zhang18}, shows how a transparent document appears to a
user.

\begin{figure}%[h]
    \centering
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-with-pointer.png}
    \vspace{1mm}
    \hrule
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-counterfactual-with-pointer.png}
    \caption{\emph{Transparent document} example, showing data-driven natural language (top
    vs.~bottom)}
    \label{fig:scigen-example-website}
\end{figure}

\rpnote{Need to illustrate counterfactual/generalisability}

\paragraph{Contributions.} Our specific contributions are:

\begin{itemize}
\item a proof-of-concept LLM-based tool for iteratively transforming a preexisting opaque document and
associated data set into a transparent, data-driven counterpart
\item an empirical evaluation of how well state-of-the-art models are able to solve the associated
interpretation and code synthesis problems
\end{itemize}

We leave implementing a full IDE authoring plugin (based on VSCode or Cursor, for example) for future work.

Our contribution is an agent-based tool which identifies text fragments which are candidates for being made
data-driven, generates the corresponding Fluid code, and then incrementally incorporates the code into a
data-driven version of the original document.

We envisage two possible use cases for transparent text:

\begin{enumerate}
\item \textbf{Authoring.} Someone authoring content for an online article, wants to create text
linked to raw data (and derivative data such as charts or tabular summaries), so that the evidence base for
the claims made in the text can be explored \emph{in situ}, by interacting with the text.

\item \textbf{Reading or reviewing.} Someone reading textual claims derived from open data (e.g. a
scientific paper or climate report), wants to retroactively link the text to queries over the available data
and gradually ``rationally reconstruct'' the relationship between the claims in the paper and the evidence
base. Perhaps just to aid their own comprehension, or to provide some kind of justified peer review.
\end{enumerate}

\input{fig/scigen-1805.02474v1-10-src}

\subsection{Target idioms of natural language}

NLP aspect of the problem is potentially a big problem space in itself. We will restrict interest to certain
idiomatic uses of natural language in making/justifying scientific claims. Table~\ref{tab:fluid_examples}

\input{fig/natural-language-forms}

\subsection{Contributions}

\begin{itemize}
    \item Design and proof-of-concept implementation of AI-assisted workflow for authoring transparent text
    (\secref{authoring-workflow})
    \item Empirical evaluation of how effective current LLMs are at providing the ``AI-assisted'' part
\end{itemize}
