\section{Introduction: Transparent, Data-Driven Documents}

Making sense of and verifying data-driven claims is hard, even when the evidence base is open and accessible.
A key challenge lies in tracing specific claims back to the underlying data. In peer review, for example,
empirical claims typically lack author-supplied links to data, making them hard for reviewers to check
directly~\citep{weber20}. Paper retractions, meanwhile, are often attributable not to fraud, but to simple
errors in data management or analysis~\citep{hu25}. The use of large language models (LLMs) to interpret
scholarly documents has seen considerable attention recently, from fact-checking~\citep{abu-ahmad25} to
interpretation of charts and figures~\citep{roberts24}, but current LLM interfaces do not support direct
interrogation of visual or other outputs.

Recent work on data provenance and data visualisation~\citep{psallidas18smoke,bond25}, on the other hand,
takes a more infrastructural approach to tracing outputs back to data. These approaches link computed outputs
to their data sources directly, by tracking dependency information. This allows visual outputs to support
\emph{provenance queries}, user interactions (e.g. mousing over visual elements) that reveal how output
features relate to data. The advantage of this approach is that the relationships to data sources are exposed
automatically via trusted infrastructure, typically a query language or general-purpose programming language
which tracks how data flows through a computation. However, these approaches are limited to outputs computed
from data, such as visualisations. What is missing is a way to extend these ``direct interrogation'' features
to natural language itself, where the main claims of most scholarly articles are actually made.

In this paper, we address this gap by combining two complementary approaches: the ability of LLMs to
understand technical language and synthesise queries over data, plus the provenance-tracking infrastructure of
an open source programming language called Fluid (\url{https://f.luid.org/})~\citep{perera22,bond25}.
Together, these two technologies enable the creation of \emph{transparent documents}, web-based scholarly
articles with two key transparency features:
\begin{enumerate}
\item \textbf{Data-driven:} Quantitative statements expressed in natural language --- e.g.~that system $X$ is
faster than system $Y$ on some task --- are computed from the relevant data, rather than occurring
merely as static fragments of text.

\item \textbf{Data linking:} Readers and reviewers can interactively trace such claims back to the specific
data elements that support them, through embedded provenance queries.
\end{enumerate}

\figref{scigen-example-website}, generated from our implementation, illustrates these two features. The upper
section shows an excerpt from a research paper comparing text encoding techniques~\citep{zhang18}. When a
reader hovers over the phrase ``does not further improve'', the corresponding data entries are highlighted in
blue on the left. Other fragments (e.g. ``better than'', ``further improvements'') that refer to the same data
are also marked, allowing readers to explore supporting and contrasting evidence. The lower section shows a
counterfactual situation where we imagine the authors' experiments had produced different results: here the
phrase ``does not further improve'' is replaced by the ``further improves''. Transparent documents thus turn
static text into interactable, data-driven content able to expose the evidential basis of scholarly claims.

\begin{figure}%[h]
    \centering
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-with-pointer.png}
    \vspace{1mm}
    \hrule
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-counterfactual-with-pointer.png}
    \caption{\emph{Transparent document} example, showing data-driven natural language (top
    vs.~bottom)}
    \label{fig:scigen-example-website}
\end{figure}

\paragraph{Contributions.} Our specific contributions are:

\begin{itemize}
\item a proof-of-concept LLM-based tool for iteratively transforming a preexisting opaque document and
associated data set into a transparent, data-driven counterpart
\item an empirical evaluation of how well state-of-the-art models are able to solve the associated
interpretation and code synthesis problems
\end{itemize}

We leave implementing a full authoring plugin for an IDE such as VSCode or Cursor for future work.

We envisage two possible use cases for transparent text:

\begin{enumerate}
\item \textbf{Authoring.} Someone authoring content for an online article, wants to create text
linked to raw data (and derivative data such as charts or tabular summaries), so that the evidence base for
the claims made in the text can be explored \emph{in situ}, by interacting with the text.

\item \textbf{Reading or reviewing.} Someone reading textual claims derived from open data (e.g. a
scientific paper or climate report), wants to retroactively link the text to queries over the available data
and gradually ``rationally reconstruct'' the relationship between the claims in the paper and the evidence
base. Perhaps just to aid their own comprehension, or to provide some kind of justified peer review.
\end{enumerate}

\input{fig/scigen-1805.02474v1-10-src}

\subsection{Target idioms of natural language}

Table~\ref{tab:fluid_examples} summarises the natural language idioms we study in this paper. When using
state-of-the-art models like gpt4o and gpt5, our system is able to resolve basic table lookups of direct
numerical values, as well as computations of percentages, averages, minima and maxima, and totals, each mapped
to a corresponding aggregation over the source dataset. For example, phrases such as ``the Energy Sector
accounts for 52.80\% of total emissions'' or ``the expected projected methane emissions for 2030 is 13.5'' are
interpreted as division and averaging operations over the relevant data values. Similarly, ``recorded its
highest emissions in 2030'' is interpreted as a \kw{maximumBy} query, while rank-based statements such as
``CNN gives the lowest accuracy'' are interpreted as ranking computations.

We also consider \emph{trend} expressions, which include relative or comparative natural language phrases
describing how some property evolves over time. These include single comparisons (e.g.~``training time growing
from 67 to 106 seconds'') and universally quantified comparisons (e.g.~``SVM outperformed all other
kernels''). Such idioms are mapped to higher-level operators like \kw{trendWord} which are parameterised on
helper functions such as \kw{growShrink} and \kw{betterWorse} which maps comparisons to appropriate natural
language phrases. Taken together, these categories cover a substantial portion of the numerical reasoning
idioms found in empirical scientific writing.

Notably, the current coverage omits several common linguistic forms. We have yet to study the ability of LLMs
to interpret approximate quantitative terms like \textbf{around 50\%} or \textbf{roughly 100 instances}, nor
interval-based descriptions such as \textbf{between 30 and 40}\% or \textbf{within 5â€“10 seconds}. We have no
reason for thinking these will present specific difficulties, however. So-called \emph{graded} modal
adjectives which express comparatives and other adjectives in nuanced or qualified ways -- like
\textbf{slightly better} and \textbf{significantly higher} -- are likely to be more
challenging~\cite{lassiter17}, especially when combined with trends over time, as in \textbf{steady increase}
or \textbf{sharp decline}. Distributional descriptors like ``most models'', ``the majority of cases'', or
``usually performs better than'' present significant challenges to formal interpretation because of their
reliance on non-standard quantifiers. Extending coverage to these idioms would substantially deepen our tool's
ability to bridge natural language reporting with interpretation in terms of the underlying dataset. We
discuss this further in \secref{future-work}.

\input{fig/natural-language-forms}

\subsection{Contributions}

\begin{itemize}
    \item Design and proof-of-concept implementation of AI-assisted workflow for authoring transparent text
    (\secref{authoring-workflow})
    \item Empirical evaluation of how effective current LLMs are at providing the ``AI-assisted'' part
\end{itemize}
