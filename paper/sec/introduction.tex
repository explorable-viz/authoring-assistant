\section{Introduction}

\begin{itemize}
    \item Making the text transparent, with the possibility to highlight the data related to a specific sentence.
    \item Making sense of data-driven claims is hard, even if the evidence base (code/data) is open
    \item We see this in peer review, misinformation, retracted papers, â€¦
\end{itemize}

\rpnote{Small example here}

\subsection{Transparent Text via Interactive Provenance Queries}

Recent work on data provenance and data visualisation~\citep{psallidas18smoke,perera22,bond25} integrating
interactive provenance queries into charts and figures using \emph{transparent programming languages} which
track data provenance automatically as data flows through a computation. Visual outputs produced using these
platforms allow readers to interact with visual elements to see the relevant data. In most article charts and
figures usually only play a supporting role, however, with the main claims usually presented in natural
language. This language makes quantitative claims and refers to ``behind the scenes'' data analyses.

In principle transparent programming languages can also be used to create natural language which is bidirectly
linked to data, by computing key fragments of natural language from data. The following example, adapted from
\cite{moosavi21}, shows how the language presented in this paper can be used to achieve this:

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-with-pointer.png}
    \caption{\emph{Transparent text} links fragments of natural language to underlying data, via queries}\label{fig:scigen-example-website}
\end{figure}

\input{fig/scigen-1805.02474v1-10-src}

\subsection{Use cases}
Two potential scenarios for this sort of technology:

\begin{enumerate}
\item \textbf{Authoring transparent text.} Someone authoring content for an online article, wants to create text
linked to raw data (and derivative data such as charts or tabular summaries), so that the evidence base for
the claims made in the text can be explored \emph{in situ}, by interacting with the text.

\item \textbf{Interpreting text after the fact.} Someone reading textual claims derived from open data (e.g. a
scientific paper or climate report), wants to retroactively link the text to queries over the available data
and gradually ``rationally reconstruct'' the relationship between the claims in the paper and the evidence
base. Perhaps just to aid their own comprehension, or to provide some kind of justified peer review.
\end{enumerate}

\noindent Here we focus on the first one because the second one requires a certain amount of additional setup.

\subsubsection{Target idioms of natural language}

NLP aspect of the problem is potentially a big problem space in itself. We will restrict interest to certain
idiomatic uses of natural language in making/justifying scientific claims.
Examples \ref{tab:fluid_examples}

\input{fig/natural-language-forms}

\subsection{Contributions}

\begin{itemize}
    \item Design and proof-of-concept implementation of AI-assisted workflow for authoring transparent text
    (\secref{authoring-workflow})
    \item Empirical evaluation of how effective current LLMs are at providing the ``AI-assisted'' part
\end{itemize}
