\section{Introduction: Transparent Documents}

When interpreting or verifying data-driven claims, a key challenge lies in tracing specific claims back to the
relevant data. In peer review, for example, empirical claims typically lack author-supplied links to data,
making them hard for reviewers to check directly~\citep{weber2020}. Paper retractions, meanwhile, are often
attributable not to fraud, but to simple errors in data management or analysis~\citep{hu2025}. The use of large
language models (LLMs) to interpret scholarly documents has seen considerable attention recently, from
fact-checking~\citep{abuahmad2025} to interpretation of charts and figures~\citep{roberts2024}, but current LLM
interfaces do not support direct interrogation of visual or other outputs for traceability to inputs.

Recent advances in data provenance and data visualisation~\citep{psallidas2018,bond2025}, on the other hand, have pushed
in this direction using more traditional symbolic techniques. These approaches link computed outputs to their data
sources directly by tracking dependency information. This allows visual outputs to support \emph{provenance queries},
user interactions (e.g. mousing over visual elements) that reveal how output features relate to data. The advantage of
this approach is that the relationships to data sources are exposed automatically via trusted infrastructure, typically
a query language or general-purpose programming language which tracks how data flows through a computation. However,
these approaches are limited to outputs computed from data, such as visualisations. What is missing is a way to extend
these ``direct interrogation'' features to the natural language accompanying such figures, where the main claims of most
scholarly articles are actually made.

In this paper \todo{clarify: necessary precursor to user study}, we address this gap by combining two complementary
approaches: the ability of LLMs to understand technical language and synthesise queries over data, plus the
provenance-tracking infrastructure of an open source programming language called Fluid\footnote{
\url{https://f.luid.org/}} \citep{perera2022,bond2025}. Together, these two technologies enable the creation of
\emph{transparent documents}, web-based scholarly articles with two key transparency features:
\begin{enumerate}
\item \textbf{Data-driven:} Quantitative statements expressed in natural language --- e.g.~that system $X$ is
faster than system $Y$ on some task --- are computed from the relevant data, rather than occurring
merely as static fragments of text.

\item \textbf{Data linking:} Readers and reviewers can interactively trace such claims back to the specific
data elements that support them, through embedded provenance queries.
\end{enumerate}

\figref{scigen-example-website}, generated from our implementation, illustrates these two features. The upper
section shows a ``transparent'' excerpt from \cite{zhang2018}, a scholarly article comparing text encoding
techniques. When a reader hovers over the phrase \textfrag{does not further improve}, the relevant data are
highlighted on the left. Other fragments (e.g. \textfrag{better than}, \textfrag{further improvements}) that
refer to the same data are also marked, allowing the reader to explore supporting and contrasting evidence.
The lower section shows a counterfactual situation where the authors' experiments had produced different
results: here the phrase \textfrag{does not further improve} is replaced by \textfrag{further improves}.

\begin{figure*}%[h]
    \centering
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-with-pointer.png}
    \vspace{1mm}
    \hrule
    \includegraphics[width=\linewidth]{fig/scigen-1805.02474v1-10-counterfactual-with-pointer.png}
    \caption{Two versions of a transparent document, showing text fragments linked to data}
    \label{fig:scigen-example-website}
\end{figure*}

This transparent version of the document was implemented in Fluid. The source code is shown in
\figref{fluid-example-paragraph}, and makes use of several helper functions, a representative subset of which
are shown in \figref{fluid-scigen}. What makes our solution interesting is that the provenance-tracking
runtime of Fluid \emph{and} the LLM-based authoring support are both essential components of the solution,
with Fluid providing the interactions, and the LLM-based tool making the authoring process feasible.
Generating code for a traditional language like Python would still result in a data-driven document, but
crucially without the interactive provenance queries; and without AI-based tooling to support the authoring
process, the author would be faced with creating the code in \figref{fluid-example-paragraph} by hand, which
is unlikely to be feasible as part of the usual scientific writing process.

AI-assisted authoring of transparent documents thus support turning static text into interactable, data-driven
content able to expose the evidential basis of scholarly claims. We envisage two use cases. First, when
\textbf{authoring} content for an online article, a journalist or scientific publisher may wish to provide
text which is linked to the underlying data so that the evidence base for the textual claims can be explored
directly from the article. Second, when \textbf{reading} a document reporting on findings derived from open
data (perhaps a scientific paper or climate report), the reader may want to retroactively interpret parts of
the text as queries over the available data and gradually ``rationally reconstruct'' the relationship between
claims in the paper and the evidence base. This might be just to aid their own comprehension, or part of a
formal peer review process.

\paragraph{Contributions.} Our specific contributions are as follows. We leave implementing a full
Copilot-like authoring plugin for an IDE such as VSCode or Cursor for future work (\secref{conclusion}).

\begin{itemize}
\item A proof-of-concept LLM-based tool for iteratively transforming a preexisting opaque document and
associated dataset into a transparent, data-driven counterpart (\secref{authoring-workflow});
\item A summary of the natural language idioms we have studied (\secref{nl-idioms}) and an empirical
evaluation of how well state-of-the-art models are able to solve the associated interpretation and code
synthesis problems (\secref{evaluation}).
\end{itemize}

\input{fig/scigen-1805.02474v1-10-src}
