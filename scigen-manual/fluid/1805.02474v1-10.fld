import scigen
import util
import datasets.scigen._1805_02474v1_10
let model_ name = model name tableData
in

"""
	As shown in Table 3, BiLSTM gives significantly ${trendWord (model_ "BiLSTM").acc (model_ "LSTM").acc betterWorse} accuracies compared to uni-directional LSTM2, with the training time per epoch ${trendWord (model_ "BiLSTM").time_s (model_ "LSTM").time_s growShrink} from ${(model_ "LSTM").time_s} seconds to ${(model_ "BiLSTM").time_s} seconds. Stacking 2 layers of BiLSTM gives ${trendWord (model_ "2 stacked BiLSTM").acc (model_ "BiLSTM").acc improvements} to development results, with a ${trendWord (model_ "2 stacked BiLSTM").time_s (model_ "BiLSTM").time_s smallerHigher} time of ${(model_ "2 stacked BiLSTM").time_s} seconds. 3 layers of stacked BiLSTM ${trendWord (model_ "3 stacked BiLSTM").acc (model_ "BiLSTM").acc improve} the results. In contrast, S-LSTM gives a development result of ${(model_ "S-LSTM").acc} %, which is significantly ${trendWord (model_ "S-LSTM").acc (model_ "2 stacked BiLSTM").acc betterWorse} compared to 2-layer stacked BiLSTM, with a ${trendWord (model_ "S-LSTM").num_param (model_ "2 stacked BiLSTM").num_param smallerHigher} number of model parameters and a ${trendWord (model_ "S-LSTM").time_s (model_ "2 stacked BiLSTM").time_s shorterLonger} time of ${(model_ "S-LSTM").time_s} seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), ${(findWithKey_ "time_s" (minimum (map (fun y -> y.time_s) tableData)) tableData).model} is the ${rankLabel "most efficient" (findIndex "model" "CNN" (insertionSort (fun a b -> a.time_s < b.time_s) tableData))} among all models compared, with the ${rankLabel "smallest" (findIndex "model" "CNN" (insertionSort (fun a b -> a.num_param < b.num_param) tableData))} model size. On the other hand, a 3-layer stacked CNN gives an accuracy of ${(model "3 stacked CNN" tableData).acc} %, which is also the ${rankLabel "lowest" (findIndex "model" "CNN" (insertionSort (fun a b -> a.time_s < b.time_s) tableData))} compared with BiLSTM, hierarchical attention and S-LSTM. The ${rankLabel "best" (findIndex "model" "S-LSTM+Attention" (insertionSort (fun a b -> b.acc < a.acc) tableData))} performance of hierarchical attention is obtained by S-LSTM+Attention in terms of both accuracy and efficiency. S-LSTM gives significantly ${trendWord (model_ "S-LSTM").acc (model_ "CNN").acc betterWorse} accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used. Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still ${trendWord (model_ "S-LSTM").acc (model_ "BiLSTM").acc underOverPerforming} BiLSTM significantly.
"""
