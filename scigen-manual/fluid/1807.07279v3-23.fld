import util
import scigen
import datasets.scigen._1807_07279v3_23

"""
	The correlation scores for  ${length (filter (fun x -> x._Dataset_EN /= "Average") tableData)} different similarity test sets and their averages are reported in Table VI. We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform ${underOverPerformed (overallComparison [ compareCols "_Proposed" col (findWithKey_ "_Dataset_EN" "Average" tableData) | col <- ["_GloVe", "_OIWE_IPG", "_SOV", "_SPINE", "_Word2Sense"]])} in the correlation values for the proposed algorithm, outperforming all the alternatives except Word2Vec baseline on average. Although Word2Sense performed slightly better on some of the test sets, it should be noted that it is trained on a significantly larger corpus. Categories from Roget's thesaurus are groupings of words that are similar in some sense which the original embedding algorithm may fail to capture. These test results signify that the semantic information injected into the algorithm by the additional cost term is significant enough to result in a measurable improvement. It should also be noted that scores obtained by SPINE is  ${let [avg, dev] = stats [(findWithKey_ "_Dataset_EN" "Average" tableData).[col] | col <- ["_GloVe", "_Word2Vec", "_OIWE_IPG", "_SOV", "_SPINE", "_Word2Sense", "_Proposed"]] in unusuallyHighLow (overallComparison ( map (fun val -> distributionOutlier val avg dev 2) (getColumn "_SPINE" tableData)))} on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions.
"""
