\section{LLM-Based Linked Text Generation}

\subsection{Candidate Research Questions}

\subsubsection{Impact of documentation and naming conventions on the output accuracy}
Analyse how the use of proper naming conventions and the inclusion of comments in each example of the in-context learning dataset affect the accuracy of the LLM's output
\textbf{RQ}: How do documentation, naming conventions, affect performance of LLM?

\subsubsection{Impact of Example Complexity on LLM Generalization in DSLs}
Analyse the impact of the structure of the in-context learning dataset on the accuracy of responses focusing on the complexity of datasets.
By complexity, we mean the number of Fluid elements that the in-context learning dataset contains for each example.

\textbf{RQ}: What is the impact of example complexity on the accuracy of LLM-generated outputs in DSL tasks?

\textbf{Related work (links)}
\begin{itemize}
    \item \href{http://www.lrec-conf.org/proceedings/lrec-coling-2024/pdf/2024.main-1.1251.pdf}{Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment} - not strongly related to dsl, but interesting about diversity in prompt.
\end{itemize}



\subsubsection{Analysis of the Role of Formal Models in Enhancing LLMs for DSL Generation}

Analyse the impact of formal models, such as Context-Free Grammar, on the responses generated by the LLM.

\textbf{RQ}: How does the integration of formal model (e.g., grammars) enhance the ability of LLMs to generate structured and domain-specific languages?

\textbf{Related work (links)}
\begin{itemize}
    \item \href{https://dl.acm.org/doi/10.1145/3652620.3687811}{From a Natural to a Formal Language with DSL Assistant.}
    \item \href{https://proceedings.neurips.cc/paper_files/paper/2023/file/cd40d0d65bfebb894ccc9ea822b47fa8-Paper-Conference.pdf}{Grammar Prompting for Domain Specific Languages}
    \item \href{https://www.sciencedirect.com/science/article/abs/pii/S0920548924001077}{Grammar-obeying program synthesis: A novel approach using large language models and many-objective genetic programming}
\end{itemize}

\subsubsection{Influence of LLM parameters on the accuracy}
Analysis of the influence of the following parameters on the accuracy of responses generated by the LLM:

\begin{itemize}
    \item Temperature
    \item num\_ctx
    \item repeat\_penalty
\end{itemize}

\textbf{RQ}: How do LLM parameters (e.g., temperature, context size) affect the accuracy of responses in domain-specific languages (DSLs)?
\textbf{Related work (links)}
\begin{itemize}
    \item \href{https://ieeexplore.ieee.org/document/10684656}{On the Effectiveness of Large Language Models in Statement-level Code Summarization.} They analysed (among other things) the effectiveness of the temperature parameter for the generation of comments, but in my opinion not in depth.
\end{itemize}
