\section{Problem Overview}
\label{sec:overview}

\subsection{The Linguistic Sorts of Linked Text}
We begin by breaking down the sorts of linked text that the developer may
be interested in generating. 

\begin{figure}
   \includegraphics[width=0.7\textwidth]{fig/text-viz-types.png}
   \caption{Two sorts of linked text}
   \label{fig:linked-text-types}
\end{figure}

\subsubsection{Graded Adjectives}
A graded adjective is an adjective that can be modified to make its form stronger or weaker.
As an example, consider mapping ranges of probabilities onto natural language. If the probability
of an event occurring is $1$, we can say it is \emph{certain} to occur. If instead it is $0.99$
or some other value arbitrarily close to $1$, we will \emph{grade} the adjective to weaken it:
it is \emph{virtually certain} to occur. In \figref{linked-text-types}, this corresponds to the text
highlighted in pink.

\subsubsection{Indexing Visual Elements: Mereology}
Often, visualisations are paired with descriptions of the information the visualisation is attempting to
convey. Even when such text appears to make direct reference to a visual element, it can be unclear what 
it should specifically be linked against. For example, in \figref{linked-text-types}, the text highlighted
in green is referring to one of the bars in the chart, but it is not clear how this should be presented.
In the example, this information is communicated by highlighting the bar itself, but in a sense the text is
instead referencing the index of the bar rather than the bars contents. It could therefore be argued that the
the text should be linked against the index variable underneath the bar, rather than the bar itself.
Our system needs to be able to walk arbitrarily deep into nested structures when performing the linking.

This is an example of the more general linguistic problem of mereology: analysing relationships between the
whole and the parts from which it is composed. In the case of the chart, one can reference: the chart as a whole,
the bars as a whole, individual bars, the fields of the records underlying each of the bars, and so on.
We will need our system to be able to generate linked text that can make these arbitrary walks into the nested
data structures we provide.


\subsubsection{Indirect Summary of Underlying Data}
The second sort of linked text is summarizing some underlying data. In \figref{linked-text-types},
the text that has been highlighted in pink is an example of this. The text has been synthesized
by comparing a (spoofed) probability against a map of calibrated terminology. By this we mean that
probabilities within certain ranges are given simple natural language descriptions. ``Virtually certain'
is a term used by the IPCC to describe probabilities between 99\% and 100\%. The system should be able
to synthesize such a mapping and insert a call to this mapping into a text field. Once such a mapping
has been synthesized, the system should be able to reuse it where appropriate, so in this example if
we wanted to refer to a different probability, we should not synthesize a new mapping, just insert
a new call to the mapping in the appropriate location. For now, we call these mappings "policies".

\subsection{Applications of Self-Certifying Text}
Here we shall present some examples of the type of policy that we might want to synthesize.

\paragraph{Calibrated Terminology:} in scientific reporting, probabilities are often not directly given, but instead
mapped to calibrated terminology. We have already mentioned the term ``virtually certain'' from the IPCC report, but in the beginning
of that document, they make sure to explain the meaning of terms like ``likely'' and ``very likely'' as well. This
sort of simple conversion is an example of the simplest form of policy that one might want to synthesize.

\subsection{Challenges}
Applying language models to the task of code generation is not new, however the aim of generating code that
itself produces natural language appears to be a novel one. We currently anticipate the following challenges,
and will add more as the work progresses.

\paragraph{Small number of examples:} as a language, Fluid is incredibly niche, and so we have a very small corpus
of programs on which to train a language model. Further, we currently only have 2 example programs that actually make
use of the \kw{LinkedText} construct which we intend to use. The problem then is how to best make use of the capabalities
of current language models in this context. For example, should we perform some sort of few-shot learning or prompt-engineering
technique on a large pre-trained model, should we augment our corpus of training examples manually or both? If we take a meta-learning
perspective (\todo{CITE}), we may run into problems of the model preferring syntax and function names from the languages we use to
train the model.

\paragraph{Counter-intuitive dependency relation:} at the moment, dependencies that are computed by Fluid can be counter-intuitive.
Until we revamp the underlying dependency model, we will need to generate code that follows the pattern of ``consuming'' input
data in order to produce the output text. This may be a problem for things like generating direct references to visual elements.

\paragraph{Potentially complex code:} for many purposes, we want to store the data objects being referred to in variables. This 
could mean complicated code, and cluttering a source file. We will potentially need to find a general pattern for this sort of thing,
so that an authors document doesn't get filled with extraneous variable declarations. 