\section{Problem Overview}
\label{sec:overview}

% The scope of the problem can be thought of as follows: we aim to integrate data analytics within a document in
% a manner that maximizes the ability of the reader to understand the core information being presented. Existing
% solutions such as notebooks and literate programming are limited by their labour-intensive nature, and the
% lack of interactivity between data visualizations and accompanying text.
%
% This large scale problem is a difficult one, there are many sub-problems that do not have immediate answers.
% Two of the main issues are scale and abstraction: modern data analysts employ complex models and large
% datasets. Bridging the gap between -- for example -- the results of a regression analysis and the natural
% language description of the results is difficult, due to the presence of steps like feature-selection.
% Furthermore with modern approaches such as neural networks, the analytical methods themselves can become
% black-boxes.
%
% Automating this process would require a system that can generate code which correctly performs the data
% analysis, which is a task well beyond the scope of current research in AI. We shall instead restrict our
% attention to a smaller problem, namely how do we link text and visualizations based on the results of a data
% analysis that has already been performed. The larger problem should be considered a long-term goal for such a
% research program.

\subsection{Potential Users}

\begin{itemize}
\item Data Scientists
\item Science journalists
\item Authors of policy reports
\item Educators
\end{itemize}

% \subsection{Challenges}
%
% We have recreated a prototype of the example from \figref{ipcc-mockup} with our current system, which already
% demonstrates some of the difficulties inherent to this problem.
%
% \begin{figure}
%    \tiny
%    \lstinputlisting[language=Fluid]{listings/bar-chart.fld}
%    \caption{Fluid source code for generating linked text}
% \end{figure}
%
% Applying language models to the task of code generation is not new, however the aim of generating code that
% itself produces natural language appears to be a novel one. We currently anticipate the following challenges,
% and will add more as the work progresses.
%
% \paragraph{Small number of examples:} as a language, Fluid is incredibly niche, and so we have a very small
% corpus of programs on which to train a language model. Further, we currently only have 2 example programs that
% actually make use of the \kw{LinkedText} construct which we intend to use. The problem then is how to best
% make use of the capabalities of current language models in this context. For example, should we perform some
% sort of few-shot learning or prompt-engineering technique on a large pre-trained model, should we augment our
% corpus of training examples manually or both? If we take a meta-learning perspective (\todo{CITE}), we may run
% into problems of the model preferring syntax and function names from the languages we use to train the model.
%
% \paragraph{Counter-intuitive dependency relation:} at the moment, dependencies that are computed by Fluid can
% be counter-intuitive. Until we revamp the underlying dependency model, we will need to generate code that
% follows the pattern of ``consuming'' input data in order to produce the output text. This may be a problem for
% things like generating direct references to visual elements.
%
% \paragraph{Potentially complex code:} for many purposes, we want to store the data objects being referred to
% in variables. This could mean complicated code, and cluttering a source file. We will potentially need to find
% a general pattern for this sort of thing, so that an authors document doesn't get filled with extraneous
% variable declarations.
