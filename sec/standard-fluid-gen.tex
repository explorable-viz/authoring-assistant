\section{Generating Fluid Code}
\label{sec:generate-fluid}

The first problem -- and one potentially without any research component -- is to be able
to generate \kw{fluid} programs that access the aggregate of data reference by the surrounding text.
To solve this problem we will need to accumulate some research on code-generation from LLM's.

\subsection{Modules}
We first need to identify the relevant ``tasks'' from the wider body of NLP/LLM literature,
since each task is likely to comprise a module of the overall system. Here we will attempt
to describe the modules required to completely characterize a language-model code-generation
system.

\paragraph*{Pre-Trained Models}
Models that are pre-trained on a wide range of unlabelled data have been successful in LLM
code generation with the most notable pre-trained models being BERT \citep{devlin2019}
and GPT \citep{radford2018}. One of the starting points is to begin with a pre-trained model
that has acquired some internal representation of programming concepts. Then we can fine-tune
it to work with a more specific representation of fluid programs.

\paragraph*{Representation of \kw{fluid} programs}
Our tool will need to combine information from a natural language context (a \kw{LinkedText} list)
with a formal model of \kw{fluid} programs. Moreover, the natural language context (NLC)
\textit{is itself a value in a fluid program}. In recent years, with language models applied to tasks
involving code, code representation learning has revealed itself to be a vitally important task.
It seems like learning of representations is specific to the task-domain to which the language model
is being applied. Will leave further investigation of code representation learning until we've 
established more of the overall structure of the problem we aim to solve.
