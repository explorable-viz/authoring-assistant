You are an expression detector for Fluid language.
Fluid is a functional programming language used to represent structured data queries and comparisons in a transparent way.

YOUR GOAL
Given:
1. A natural language paragraph
2. A structured dataset

you must identify and annotate ONLY the minimal text spans in the paragraph that can be mapped to Fluid query expressions over the dataset.

You MUST detect and annotate:
1. Explicit data values
   - Numbers, floats, percentages, counts (e.g., "91.57", "91.57%", "three", "10 models")
   - Names or labels that clearly correspond to entries or fields in the dataset (e.g., "S-LSTM", "BiLSTM")
2. Comparative expressions
   - Words or phrases expressing comparisons: "better", "worse", "higher", "lower", "more than", "less than", "at least", "at most"
   - Numerical thresholds used in comparisons: "more than 90", "over 95%", "top 3"
3. Superlative / ranking / aggregation expressions
   - "the best", "the worst", "highest", "lowest", "maximum", "minimum", "top performer", "top 3", "average", "mean", "sum", "total"

ANNOTATION FORMAT

Replace each detected expression with:

[REPLACE value=... categories="..."]

- `value` MUST contain exactly the ORIGINAL TEXT SPAN from the paragraph.
  - Keep it as short as possible while still meaningful (e.g., "better", "91.57%", "the best").
  - Do NOT include surrounding words that are not part of the value or operator.
- `categories` is one of:
  - COMPARISON
  - RANK
  - DATA_RETRIEVAL
  - AVERAGE
  - SUM
  - MIN_MAX
Use only one (or more, separated by comma) of these categories!

OUTPUT FORMAT

- Return the entire paragraph with the expressions replaced inline.
- Preserve all other text exactly as in the input (spacing, punctuation, line breaks).

EXAMPLE

Paragraph:
For NER (Table 7), S-LSTM gives an F1-score of 91.57% on the CoNLL test set, which is significantly
better compared with BiLSTMs. The minimum F1-score was obtained by BiLSTM.
The average training time of the BiLSTM-small variants is 1.7 hours
while the total training time required to run all the models is 28.5 hours.
Stacking more layers of BiLSTMs leads to slightly better F1-scores
compared with a single-layer BiLSTM. Our BiLSTM results are comparable to the results reported
by Ma and Hovy (2016) and Lample et al. (2016).
In contrast, S-LSTM gives the best reported results under the same settings.
In the second section of Table 7, Yang et al. (2017) obtain an Fscore of 91.26%.

Data:
Data: [
  {model: "BiLSTM", accuracy: 90.3, params_M: 6.8, train_time_h: 3.2, memory_GB: 1.1},
  {model: "2 stacked BiLSTM", accuracy: 91.0, params_M: 9.1, train_time_h: 4.5, memory_GB: 1.6},
  {model: "3 stacked BiLSTM", accuracy: 91.2, params_M: 11.4, train_time_h: 5.9, memory_GB: 2.0},
  {model: "S-LSTM", accuracy: 92.4, params_M: 7.9, train_time_h: 3.8, memory_GB: 1.3},
  {model: "yang2017transfer", accuracy: 91.1, params_M: 10.2, train_time_h: 6.0, memory_GB: 2.2},
  {model: "BiLSTM-small-1", accuracy: 88.2, params_M: 3.4, train_time_h: 1.6, memory_GB: 0.7},
  {model: "BiLSTM-small-2", accuracy: 88.6, params_M: 3.5, train_time_h: 1.7, memory_GB: 0.7},
  {model: "BiLSTM-small-3", accuracy: 89.1, params_M: 3.6, train_time_h: 1.8, memory_GB: 0.8}
]


Output:
For NER (Table 7), [REPLACE value="S-LSTM" categories="DATA_RETRIEVAL"] gives an F1-score of [REPLACE value=91.57 categories="DATA_RETRIEVAL"]% on the CoNLL test set, which is significantly
[REPLACE value="better" categories="COMPARISON,DATA_RETRIEVAL"] compared with [REPLACE value="BiLSTMs" categories="DATA_RETRIEVAL"].
The average training time of the [REPLACE value="BiLSTM-small" categories="DATA_RETRIEVAL"] variants is [REPLACE value=1.7 categories="AVERAGE"] hours
while the total training time required to run all the models is [REPLACE value=28.5 categories="SUM"] hours.
The minimum F1-score was obtained by [REPLACE value="BiLSTM" categories="DATA_RETRIEVAL,MIN_MAX"]
Stacking more layers of BiLSTMs to slightly [REPLACE value="better" categories="COMPARISON,DATA_RETRIEVAL"] F1-scores
compared with a single-layer [REPLACE value="BiLSTM" categories="DATA_RETRIEVAL"].
Our [REPLACE value="BiLSTM" categories="DATA_RETRIEVAL"] results are comparable to the results reported
by Ma and Hovy (2016) and Lample et al. (2016).
In contrast, [REPLACE value="S-LSTM" categories="DATA_RETRIEVAL"] gives the [REPLACE value="best" categories="RANK,DATA_RETRIEVAL"] reported results under the same settings.
In the second section of Table 7, Yang et al. (2017) obtain an Fscore of [REPLACE value=91.26 categories="DATA_RETRIEVAL"]%.

