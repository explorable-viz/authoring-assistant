[
  {
    "_Scenario": "Grocery Shopping",
    "_Human_Model_Accuracy": 74.8,
    "_Human_Model_Perplexity": 2.13,
    "_Script_Model_Accuracy": 68.17,
    "_Script_Model_Perplexity": 3.16,
    "_Linguistic_Model_Accuracy": 53.85,
    "_Linguistic_Model_Perplexity": 6.54,
    "_Tily_Model_Accuracy": 32.89,
    "_Tily_Model_Perplexity": 24.48
  },
  {
    "_Scenario": "Repairing a flat bicycle tyre",
    "_Human_Model_Accuracy": 78.34,
    "_Human_Model_Perplexity": 2.72,
    "_Script_Model_Accuracy": 62.09,
    "_Script_Model_Perplexity": 3.89,
    "_Linguistic_Model_Accuracy": 51.26,
    "_Linguistic_Model_Perplexity": 6.38,
    "_Tily_Model_Accuracy": 29.24,
    "_Tily_Model_Perplexity": 19.08
  },
  {
    "_Scenario": "Riding a public bus",
    "_Human_Model_Accuracy": 72.19,
    "_Human_Model_Perplexity": 2.28,
    "_Script_Model_Accuracy": 64.57,
    "_Script_Model_Perplexity": 3.67,
    "_Linguistic_Model_Accuracy": 52.65,
    "_Linguistic_Model_Perplexity": 6.34,
    "_Tily_Model_Accuracy": 32.78,
    "_Tily_Model_Perplexity": 23.39
  },
  {
    "_Scenario": "Getting a haircut",
    "_Human_Model_Accuracy": 71.06,
    "_Human_Model_Perplexity": 2.45,
    "_Script_Model_Accuracy": 58.82,
    "_Script_Model_Perplexity": 3.79,
    "_Linguistic_Model_Accuracy": 42.82,
    "_Linguistic_Model_Perplexity": 7.11,
    "_Tily_Model_Accuracy": 28.7,
    "_Tily_Model_Perplexity": 15.4
  },
  {
    "_Scenario": "Planting a tree",
    "_Human_Model_Accuracy": 71.86,
    "_Human_Model_Perplexity": 2.46,
    "_Script_Model_Accuracy": 59.32,
    "_Script_Model_Perplexity": 4.25,
    "_Linguistic_Model_Accuracy": 47.8,
    "_Linguistic_Model_Perplexity": 7.31,
    "_Tily_Model_Accuracy": 28.14,
    "_Tily_Model_Perplexity": 24.28
  },
  {
    "_Scenario": "Borrowing book from library",
    "_Human_Model_Accuracy": 77.49,
    "_Human_Model_Perplexity": 1.93,
    "_Script_Model_Accuracy": 64.07,
    "_Script_Model_Perplexity": 3.55,
    "_Linguistic_Model_Accuracy": 43.29,
    "_Linguistic_Model_Perplexity": 8.4,
    "_Tily_Model_Accuracy": 33.33,
    "_Tily_Model_Perplexity": 20.26
  },
  {
    "_Scenario": "Taking Bath",
    "_Human_Model_Accuracy": 81.29,
    "_Human_Model_Perplexity": 1.84,
    "_Script_Model_Accuracy": 67.42,
    "_Script_Model_Perplexity": 3.14,
    "_Linguistic_Model_Accuracy": 61.29,
    "_Linguistic_Model_Perplexity": 4.33,
    "_Tily_Model_Accuracy": 43.23,
    "_Tily_Model_Perplexity": 16.33
  },
  {
    "_Scenario": "Going on a train",
    "_Human_Model_Accuracy": 70.79,
    "_Human_Model_Perplexity": 2.39,
    "_Script_Model_Accuracy": 58.73,
    "_Script_Model_Perplexity": 4.2,
    "_Linguistic_Model_Accuracy": 47.62,
    "_Linguistic_Model_Perplexity": 7.68,
    "_Tily_Model_Accuracy": 30.16,
    "_Tily_Model_Perplexity": 35.11
  },
  {
    "_Scenario": "Baking a cake",
    "_Human_Model_Accuracy": 76.43,
    "_Human_Model_Perplexity": 2.16,
    "_Script_Model_Accuracy": 61.79,
    "_Script_Model_Perplexity": 5.11,
    "_Linguistic_Model_Accuracy": 46.4,
    "_Linguistic_Model_Perplexity": 9.16,
    "_Tily_Model_Accuracy": 24.07,
    "_Tily_Model_Perplexity": 23.67
  },
  {
    "_Scenario": "Flying in an airplane",
    "_Human_Model_Accuracy": 62.04,
    "_Human_Model_Perplexity": 3.08,
    "_Script_Model_Accuracy": 61.31,
    "_Script_Model_Perplexity": 4.01,
    "_Linguistic_Model_Accuracy": 48.18,
    "_Linguistic_Model_Perplexity": 7.27,
    "_Tily_Model_Accuracy": 30.9,
    "_Tily_Model_Perplexity": 30.18
  },
  {
    "_Scenario": "Average",
    "_Human_Model_Accuracy": 73.63,
    "_Human_Model_Perplexity": 2.34,
    "_Script_Model_Accuracy": 62.63,
    "_Script_Model_Perplexity": 3.88,
    "_Linguistic_Model_Accuracy": 49.52,
    "_Linguistic_Model_Perplexity": 7.05,
    "_Tily_Model_Accuracy": 31.34,
    "_Tily_Model_Perplexity": 23.22
  }
]
