{
  "datasets": [
    {
      "var": "tableData",
      "file": "datasets/counterfactual/1805.02474v1-10"
    }
  ],
  "imports": [
    "scigen",
    "util"
  ],
  "testing-variables": {
  },
  "variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), "
    },
    {
      "type": "expression",
      "expression": "\"CNN\"",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": " is the most efficient among all models compared, with the "
    },
    {
      "type": "expression",
      "expression": "\"highest\"",
      "categories": ["rank", "data_retrieval", "comparison"]
    },
    {
      "type": "literal",
      "value": " model size. On the other hand, a 3-layer stacked CNN gives an accuracy of "
    },
    {
      "type": "expression",
      "expression": "\"81.46\"",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "%, which is also the "
    },
    {
      "type": "expression",
      "expression": "\"maximum\"",
      "categories": ["rank"]
    },
    {
      "type": "literal",
      "value": " compared with BiLSTM, hierarchical attention and S-LSTM."
    }
  ]
}
