{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We present the Table 4 in the supplementary material and we summarise it as follows: 1. Decoding the next sentence performed "
    },
    {
      "expression": "\"similarly\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to decoding the subsequent contiguous words. 2. Decoding the subsequent "
    },
    {
      "expression": "\"30\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " words, which was adopted from the skip-thought training code3, gave reasonably good performance. More words for decoding didn't give us a significant performance gain, and took longer to train. 3. Adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed "
    },
    {
      "expression": "\"sightly improved\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the performance on the "
    },
    {
      "expression": "\"three\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the minor performance gain. 4. Increasing the dimensionality of the RNN encoder "
    },
    {
      "expression": "\"improved\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the model performance, and the additional training time required was "
    },
    {
      "expression": "\"less than needed\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for increasing the complexity in the CNN decoder. We report results from both "
    },
    {
      "expression": "\"smallest\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"largest\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " models in Table 2.  As the transferability of the models trained in both cases perform "
    },
    {
      "expression": "\"similarly\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the evaluation tasks (see rows 1 and 2 in Table 4), we focus on the simpler predictall-words CNN decoder that learns to reconstruct the next window of contiguous words.  As stated in rows 1, 3, and 4 in Table 4, decoding short target sequences results in a "
    },
    {
      "expression": "\"slightly lower\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " Pearson score on SICK, and decoding longer target sequences lead to a "
    },
    {
      "expression": "\"longer\"",
      "categories": ["min-max"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " training time.  We tweaked the CNN encoder, including different kernel size and activation function, and we report the "
    },
    {
      "expression": "\"best results\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of CNNCNN model at row 6 in Table 4.  The future predictor in (Gan et al., 2017) also applies a CNN as the encoder, but the decoder is still an RNN, listed at row 11 in Table 4. "
    },
    {
      "expression": "\"Compared to\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " our designed CNN-CNN model, their CNN-LSTM model contains "
    },
    {
      "expression": "\"more parameters\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than our model does, but they have similar performance on the evaluation tasks, which is also "
    },
    {
      "expression": "\"worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than our RNNCNN model.  Clearly, we can tell from the comparison between rows 1, 9 and 12 in Table 4, increasing the dimensionality of the RNN encoder leads to "
    },
    {
      "expression": "\"better transferability\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the model.  "
    },
    {
      "expression": "\"Compared with\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the model with larger-size CNN decoder, apparently, we can see that larger encoder size helps "
    },
    {
      "expression": "\"more than\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " larger decoder size does (rows 7,8, and 9 in Table 4)."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1710_10380v3_28"
  ],
  "datasets": ["datasets/scigen/1710.10380v3-28.json"]
}