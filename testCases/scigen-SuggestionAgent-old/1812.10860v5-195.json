{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 4 presents an alternative view of the results of the pretraining experiment (Table 2): The table shows "
    },
    {
      "expression": "\"correlations\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " between pairs of target tasks over the space of pretrained encoders. The "
    },
    {
      "expression": "\"correlations\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " reflect the degree to which the performance on one target task with some encoder predicts performance on another target task with the same encoder. See Appendix D for the full table and similar tables for intermediate ELMo and BERT experiments. Many "
    },
    {
      "expression": "\"correlations\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " are low, suggesting that different tasks benefit from different forms of pretraining to a substantial degree, and bolstering the observation that no single pretraining task yields good performance on all target tasks. For reasons noted earlier, the models that tended to perform "
    },
    {
      "expression": "\"best\"",
      "categories": [
        "min-max",
        "rank"
      ],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " overall also tended to overfit the WNLI training set most, leading to a negative "
    },
    {
      "expression": "\"correlation\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " between WNLI and overall GLUE score. STS also shows a negative "
    },
    {
      "expression": "\"correlation\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", likely due to the observation that it does not benefit from LM pretraining. In contrast, CoLA shows a strong "
    },
    {
      "expression": "\"correlation\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with the overall GLUE scores, but has weak or negative "
    },
    {
      "expression": "\"correlations\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with many tasks: The use of LM pretraining dramatically improves CoLA performance, but most other forms of pretraining have little effect."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1812_10860v5_195"
  ],
  "datasets": ["datasets/scigen/1812.10860v5-195.json"]
}