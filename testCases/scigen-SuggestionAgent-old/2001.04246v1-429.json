{
  "testing-variables": {},
  "paragraph": [{
    "type": "literal",
    "value": "The compression results on the six adopted datasets, including parameter size, inference speedup and classification accuracy, are summarized in Table 1.  Overall speaking, on all the evaluated datasets, the proposed AdaBERT method achieves significant efficiency improvement while maintaining comparable performance. Compared to the BERT12-T, the compressed models are 11.5x to 17.0x smaller in parameter size and 12.7x to 29.3x faster in inference speed with an average performance degradation of 2.79%.  Comparing structureheterogeneous method, BiLSTMSOF  , AdaBERT searches CNN-based models and achieves much another with better improvements, especially on the MNLI dataset.  Comparing with different Transformers-based compression baselines, the proposed AdaBERT method is 1.35x to 3.12x faster than the fastest baseline, TinyBERT4, and achieves comparable performance with the two baselines that have the best averaged accuracy, BERT6-PKD and TinyBERT4."
  }],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_2001_04246v1_429"
  ],
  "datasets": ["datasets/scigen/2001.04246v1-429.json"]
}