{
    "datasets": [
        "datasets/scigenCL/1707.06226v1-487.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigenCL/_1707_06226v1_487"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "Table 2 shows the classification results on the discussion forum dataset. Although a vast majority of the context posts contain 3-4 sentences, around 100 context posts have more than ten sentences and thus we set a cutoff to a maximum of ten sentences for context modeling. For the reply r we considered the entire reply.  The SV Mbl models that are based on discrete features did not perform very well, and adding context actually hurt the performance. Regarding the performance of the neural network models, we observe that modeling context improves the performance using all types of LSTM architectures that read both context (c) and reply (r)  The highest performance when considering both the S and  S classes is achieved by the LSTMconditional model (73.32% F1 for S class and 70.56% F1 for  S, showing a 6% and 3% improvement over LSTMr for S and  S classes, respectively). The LSTM model with sentence-level attentions on both context and reply (LSTMcas +LSTMras ) gives the best F1 score of 73.7% for the S class. For the  S class, while we notice an improvement in precision we notice a drop in recall when compared to the LSTM model with sentence level attention only on reply (LSTMras ).  We observe the performance (69.88% F1 for S category) deteriorates, probably due to the lack of enough training data."
        }
    ]
}