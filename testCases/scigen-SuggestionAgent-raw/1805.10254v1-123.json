{
    "datasets": [
        "datasets/scigenCL/1805.10254v1-123.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigenCL/_1805_10254v1_123"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "As can be seen from Table 3, our models produce better BLEU scores than almost all the comparisons. Especially, our models with separate decoder yield significantly higher BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do. Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments.  Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance. Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing."
        }
    ]
}