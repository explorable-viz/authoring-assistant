{
    "datasets": [
        "datasets/scigenCL/1906.03674v1-176.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigenCL/_1906_03674v1_176"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "We present our results in Section 5 (Table 3)  In Table 3 we use the abbreviations \"baseline\" and \"emb. conc.\" for the two baseline models respectively.  We compare the performance of the three proposed conditioning methods with the two baselines and the state-of-the-art in Table 3.  The results show that incorporating external knowledge in RNN-based architectures consistently improves performance over the baseline for all datasets. Furthermore, feature-based gating improves upon baseline concatenation in the embedding layer across benchmarks, with the exception of PsychExp dataset.  For the Sent17 dataset we achieve state-ofthe-art F1 score using the feature-based gating method; we further improve performance when combining gating with the emb. conc. method. For SST-5, we observe a significant performance boost with combined attentional gating and embedding conditioning (gate + emb. conc.). For PsychExp, we marginally outperform the state-ofthe-art also with the combined method, while for Irony18, feature-based gating yields the best results. Finally, concatenation based conditioning is the top method for SCv1, and the combination method for SCv2."
        }
    ]
}