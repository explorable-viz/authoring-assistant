{
    "datasets": [
        "datasets/scigen_SuggestionAgent/2003.02931v1-355.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigen_SuggestionAgent/_2003_02931v1_355"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "Cross-lingual transfer is powerful (RQ1). Zero-shot learning reaches an F1 score of 58% in the MEDIUM setup, which outperforms training the neural tagger on very limited gold data (plain).  Neural NER is better than traditional HMM-based tagging (TnT) (Brants, 2000) and greatly improves by unsupervised word embedding initialization (+Poly). It is noteworthy that zero-shot transfer benefits only to a limiting degree from more source data (F1 increases by almost 3% when training on all English CoNLL data).  To compare cross-lingual transfer to limited gold data (RQ2), we observe that training the neural system on the small amount of data together with Polyglot embeddings is close to the tiny-shot transfer setup.  Few-shot learning greatly improves over zero-shot learning. The most beneficial way is to add the target data to the source, in comparison to fine-tuning.  In both MEDIUM and LARGE setups are further gains obtained by adding TINY or SMALL amounts of Danish gold data. Interestingly, a) finetuning is less effective; b) it is better to transfer from a medium-sized setup than from the entire CoNLL source data."
        }
    ]
}
