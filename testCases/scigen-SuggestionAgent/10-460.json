{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We compare all approaches across the "
    },
    {
      "expression": "\"InsuranceQA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"WikiPassageQA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " benchmarks as well as the five StackExchange datasets in Table 2. For the cQA answer selection datasets we measure the accuracy, which is the ratio of correctly selected answers, and for the passage retrieval in "
    },
    {
      "expression": "\"WikiPassageQA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " we report MAP/MRR. The results show that "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets. For instance, on the cQA datasets "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " improves by "
    },
    {
      "expression": "\"4.5pp\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over "
    },
    {
      "expression": "\"CA-Wang\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and by "
    },
    {
      "expression": "\"8.8pp\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over the "
    },
    {
      "expression": "\"best semantic similarity method\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on average.  Our extended approach COALA p-means improves the performance of "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on these datasets by an additional "
    },
    {
      "expression": "\"1.6pp\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". The proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases.  The results in Table 2 show that our proposed syntax-aware extension COALA syntax-aware, which incorporates syntactic roles of word sequences to learn syntax-aware aspect representations, improves the results in five out of seven cases.  It thereby achieves an "
    },
    {
      "expression": "\"average improvement\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of "
    },
    {
      "expression": "\"0.7pp\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in our cQA datasets."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_10_460"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/10-460.json"]
}