{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Got it, I'll ensure that each replacement uses only one specified category. Here is the corrected version:\n\nWe compare all approaches across the InsuranceQA and WikiPassageQA benchmarks as well as the five StackExchange datasets in Table 2. For the cQA answer selection datasets we measure "
    },
    {
      "expression": "\"accuracy\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", which is the ratio of correctly selected answers, and for the passage retrieval in WikiPassageQA we report "
    },
    {
      "expression": "\"MAP/MRR\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". The results show that "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets. For instance, on the cQA datasets "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " improves by "
    },
    {
      "expression": "\"4.5pp\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over CA-Wang and by "
    },
    {
      "expression": "\"8.8pp\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over the "
    },
    {
      "expression": "\"best semantic similarity method\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on average.  Our extended approach "
    },
    {
      "expression": "\"COALA p-means\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " improves the performance of "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on these datasets by an additional "
    },
    {
      "expression": "\"1.6pp\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". The proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases.  The results in Table 2 show that our proposed syntax-aware extension "
    },
    {
      "expression": "\"COALA syntax-aware\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", which incorporates syntactic roles of word sequences to learn syntax-aware aspect representations, improves the results in five out of seven cases.  It thereby achieves an "
    },
    {
      "expression": "\"average improvement\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of "
    },
    {
      "expression": "\"0.7pp\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over "
    },
    {
      "expression": "\"COALA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in our cQA datasets."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_10_460"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/10-460.json"]
}