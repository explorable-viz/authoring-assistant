{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "See Table 3 for the "
    },
    {
      "expression": "\"averages\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " across "
    },
    {
      "expression": "\"10\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " scenarios.  As we can see from Table 3, the perplexity scores are consistent with the accuracies: the script model again "
    },
    {
      "expression": "\"outperforms\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " other methods, and, as expected, all the models are "
    },
    {
      "expression": "\"weaker than humans\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1702_03121v1_127"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1702.03121v1-127.json"]
}