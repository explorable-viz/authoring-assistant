{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The "
    },
    {
      "expression": "\"average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performances of "
    },
    {
      "expression": "\"LR−syntax\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"CNN:rand\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " are virtually identical, both for Macro6Described as FscoreM in Sokolova and Lapalme (2009). F1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p ≤ 0.05). Altogether, these two systems exhibit "
    },
    {
      "expression": "\"significantly better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " "
    },
    {
      "expression": "\"average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p ≤ 0.05).  The performance of the learners is quite divergent across datasets, with Macro-F1 scores6 ranging from "
    },
    {
      "expression": "\"60%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ("
    },
    {
      "expression": "\"WTP\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") to "
    },
    {
      "expression": "\"80%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ("
    },
    {
      "expression": "\"MT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "), "
    },
    {
      "expression": "\"average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " 67% (see Table 2).  On all datasets, our "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " systems clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the "
    },
    {
      "expression": "\"most striking\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " finding is the importance of lexical (in our setup, unigram) information."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1704_07203v3_455"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1704.07203v3-455.json"]
}