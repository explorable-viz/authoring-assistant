{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Thank you for clarifying. Here is the corrected annotation:\n\nThe "
    },
    {
      "expression": "\"average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performances of "
    },
    {
      "expression": "\"LR−syntax\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"CNN:rand\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ": virtually identical, both for Macro6Described as FscoreM in Sokolova and Lapalme (2009). F1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant ("
    },
    {
      "expression": "\"p ≤ 0.05\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "). Altogether, these two systems exhibit "
    },
    {
      "expression": "\"significantly better performances\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than all other models surveyed here, both those relying on and those not relying on hand-crafted features ("
    },
    {
      "expression": "\"p ≤ 0.05\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "). The performance of the learners is quite divergent across datasets, with Macro-F1 scores ranging from "
    },
    {
      "expression": "\"60%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (WTP) to "
    },
    {
      "expression": "\"80%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (MT), average 67% (see Table 2). On all datasets, our "
    },
    {
      "expression": "\"best systems\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of "
    },
    {
      "expression": "\"lexical\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (in our setup, unigram) information."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1704_07203v3_455"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1704.07203v3-455.json"]
}