{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 3 displays the results for our model evaluated on cnets for increasingly aggressive pruning levels (discarding only interjections, additionally discarding hypotheses with scores below "
    },
    {
      "expression": "\"0.001\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"0.01\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", respectively). As can be seen, using the full cnet except for interjections does not improve over the baseline.  However, when pruning low-probability hypotheses both pooling strategies improve over the baseline. Yet, "
    },
    {
      "expression": "\"average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " pooling performs "
    },
    {
      "expression": "\"worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the lower pruning threshold, which shows that the model is still affected by noise among the hypotheses.  Weighted pooling performs "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the lower pruning threshold of "
    },
    {
      "expression": "\"0.001\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with which we obtain the "
    },
    {
      "expression": "\"highest\"",
      "categories": ["min-max"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " result overall, improving the joint goals accuracy by 1.6 percentage points "
    },
    {
      "expression": "\"compared to\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the baseline.  Moreover, we see that an ensemble model that averages the predictions of ten cnet models trained with different random seeds also "
    },
    {
      "expression": "\"outperforms\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " an ensemble of ten baseline models.  Our ensemble models "
    },
    {
      "expression": "\"outperform\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " Mrksic et al. (2017) for the joint requests but are a bit "
    },
    {
      "expression": "\"worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the joint goals."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1707_05853v2_230"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1707.05853v2-230.json"]
}