{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The results of evaluating these three models on small (1M tokens) and medium-sized ("
    },
    {
      "expression": "\"17M\u2013 57M\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") tokens) data sets against Char-CNN for different languages are provided in Table 3. The models demonstrate similar performance on small data, but Char-CNN scales "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on medium-sized data. From the three syllable-aware models, Syl-Concat looks the most advantageous as it demonstrates stable results and has the least "
    },
    {
      "expression": "\"least\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " number of parameters. Therefore in what follows we will make a more detailed comparison of SylConcat with Char-CNN."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1707_06480v1_31"
  ],
  "datasets": ["datasets/scigenCL/1707.06480v1-31.json"]
}