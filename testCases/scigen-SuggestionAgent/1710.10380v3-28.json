{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We present the Table 4 in the supplementary material and we summarise it as follows: 1. Decoding the next sentence performed similarly to decoding the subsequent contiguous words. 2. Decoding the subsequent 30 words, which was adopted from the skip-thought training code3, gave "
    },
    {
      "expression": "\"reasonably good\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance. More words for decoding didn't give us a "
    },
    {
      "expression": "\"significant\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance gain, and took longer to train. 3. Adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed "
    },
    {
      "expression": "\"sightly\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " improved the performance on the three downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the "
    },
    {
      "expression": "\"minor\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance gain. 4. Increasing the dimensionality of the RNN encoder improved the model performance, and the additional training time required was less than needed for increasing the complexity in the CNN decoder. We report results from both "
    },
    {
      "expression": "\"smallest\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"largest\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " models in Table 2.  As the transferability of the models trained in both cases perform similarly on the evaluation tasks (see rows 1 and 2 in Table 4), we focus on the simpler predictall-words CNN decoder that learns to reconstruct the next window of contiguous words.  As stated in rows 1, 3, and 4 in Table 4, decoding short target sequences results in a "
    },
    {
      "expression": "\"slightly\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " lower Pearson score on SICK, and decoding longer target sequences lead to a longer training time.  We tweaked the CNN encoder, including different kernel size and activation function, and we report the "
    },
    {
      "expression": "\"best\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results of CNNCNN model at row 6 in Table 4.  The future predictor in (Gan et al., 2017) also applies a CNN as the encoder, but the decoder is still an RNN, listed at row 11 in Table 4. Compared to our designed CNN-CNN model, their CNN-LSTM model contains more parameters than our model does, but they have similar performance on the evaluation tasks, which is also "
    },
    {
      "expression": "\"worse\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than our RNNCNN model.  Clearly, we can tell from the comparison between rows 1, 9 and 12 in Table 4, increasing the dimensionality of the RNN encoder leads to "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " transferability of the model.  Compared with the model with larger-size CNN decoder, apparently, we can see that larger encoder size helps more than larger decoder size does (rows 7,8, and 9 in Table 4)."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1710_10380v3_28"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1710.10380v3-28.json"]
}
