{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Understood, I will correct the categories accordingly. Here is the revised output:\n\nTable 1 shows "
    },
    {
      "expression": "\"micro F1 scores\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on AIDA-B of the "
    },
    {
      "expression": "\"SOTA methods\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and ours, which all use Wikipedia and YAGO mention-entity index. To our knowledge, ours are the only (unsupervisedly) inducing and employing "
    },
    {
      "expression": "\"more than one relations\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on this dataset. The others use only one relation, coreference, which is given by simple heuristics or supervised third-party resolvers. All four our models outperform any previous method, with ment-norm achieving the "
    },
    {
      "expression": "\"best results\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " "
    },
    {
      "expression": "\"0.85%\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " higher than that of Ganea and Hofmann (2017). The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1804_10637v1_90"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1804.10637v1-90.json"]
}