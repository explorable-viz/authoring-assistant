{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1score of "
    },
    {
      "expression": "\"62.47\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%.\r\nAdding character-bigram and softword representations as described in Section 3.1 increases the F1-score to "
    },
    {
      "expression": "\"67.63\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% and "
    },
    {
      "expression": "\"65.71\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, respectively, demonstrating the usefulness of both sources of information.\r\nIn addition, a combination of both gives a "
    },
    {
      "expression": "\"69.64\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% F1-score, which is "
    },
    {
      "expression": "\"the best\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " among various character representations.\r\nTable 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a "
    },
    {
      "expression": "\"64.12\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% F1-score, which is "
    },
    {
      "expression": "\"higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared to the character-based baseline.\r\nThis demonstrates that both word information and character information are useful for Chinese NER. The two methods of word+char LSTM and word+char LSTM(cid:48), lead to similar improvements.\r\nA CNN representation of character sequences gives a "
    },
    {
      "expression": "\"slightly higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score compared to LSTM character representations.\r\nOn the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.\r\nAs shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of "
    },
    {
      "expression": "\"71.62\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, which is "
    },
    {
      "expression": "\"significantly higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1805_02023v4_203"
  ],
  "datasets": ["datasets/scigenCL/1805.02023v4-203.json"]
}