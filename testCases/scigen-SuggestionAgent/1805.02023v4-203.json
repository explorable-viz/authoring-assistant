{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As shown in Table 4, without using word segmentation, a "
    },
    {
      "expression": "\"characterbased LSTM-CRF model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " gives a development F1score of "
    },
    {
      "expression": "\"62.47%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to "
    },
    {
      "expression": "\"67.63%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"65.71%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", respectively, demonstrating the usefulness of both sources of information. In addition, a combination of both gives a "
    },
    {
      "expression": "\"69.64%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score, which is the "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " among various character representations.  Table 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a "
    },
    {
      "expression": "\"64.12%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score, which is "
    },
    {
      "expression": "\"higher\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared to the character-based baseline. This demonstrates that both word information and character information are useful for Chinese NER. The two methods of  "
    },
    {
      "expression": "\"word+char LSTM\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"word+char LSTM(cid:48)\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", lead to similar improvements.  A CNN representation of character sequences gives a "
    },
    {
      "expression": "\"slightly higher\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score compared to LSTM character representations. On the other hand, further using character bigram information leads to "
    },
    {
      "expression": "\"increased\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score over word+char LSTM, but "
    },
    {
      "expression": "\"decreased\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score over word+char CNN.  As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of "
    },
    {
      "expression": "\"71.62%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", which is "
    },
    {
      "expression": "\"significantly higher\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1805_02023v4_203"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1805.02023v4-203.json"]
}