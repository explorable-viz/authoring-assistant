{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As expected, "
    },
    {
      "expression": "\"Table 4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " shows that people with less domain knowledge are "
    },
    {
      "expression": "\"more easily deceived\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Specifically, non-CS human judges fail at "
    },
    {
      "expression": "\"more than half\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the "
    },
    {
      "expression": "\"1\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "-to-"
    },
    {
      "expression": "\"1\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " sets for the same titles, which suggests that most of our system generated abstracts follow correct grammar and consistent writing style. Domain experts fail on "
    },
    {
      "expression": "\"1 or 2\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " sets, mostly because the human written abstracts in those sets don't seem very "
    },
    {
      "expression": "\"topically relevant\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Additionally, the more abstracts that we provided to human judges, the easier it is to conceal the system generated abstract amongst human generated ones."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1805_06064v1_257"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1805.06064v1-257.json"]
}