{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As can be seen from Table 3, our models produce "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU scores than almost all the comparisons. Especially, our models with separate decoder yield significantly "
    },
    {
      "expression": "\"higher\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do. "
    },
    {
      "expression": "\"Better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments. Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance. Interestingly, utilizing system retrieved evidence yields "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU scores than using oracle retrieval for testing."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1805_10254v1_123"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1805.10254v1-123.json"]
}