{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Finally, for the last setting, we tested the system using the "
    },
    {
      "expression": "\"three\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " auto-annotation modes (i.e., "
    },
    {
      "expression": "\"FA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"HFA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ",  and "
    },
    {
      "expression": "\"UFA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") as shown in Table 3. While the auto-annotation mode can allow us to reduce up to "
    },
    {
      "expression": "\"87%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the data pool, this drastic saving also reduces the accuracy of the learned model, achieving, on average, around "
    },
    {
      "expression": "\"81%\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F-Score. Overall, our framework presents a trade off between coverage and annotation cost. The "
    },
    {
      "expression": "\"HFA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " auto-annotation mode shows the benefit, especially in a realistic enterprise setting, when we need to annotate "
    },
    {
      "expression": "\"33%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the data to increase F-Score by only "
    },
    {
      "expression": "\"10%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (when comparing the on average performance of "
    },
    {
      "expression": "\"HFA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with "
    },
    {
      "expression": "\"ESA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") is unreasonable. Table 3 appears to show "
    },
    {
      "expression": "\"FA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " being inferior to "
    },
    {
      "expression": "\"EAL\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in terms of the percentage cut for the Location class, for example. In reality "
    },
    {
      "expression": "\"FA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " reduced sentence annotation by "
    },
    {
      "expression": "\"65%\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to reach "
    },
    {
      "expression": "\"0.99\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F-Score. But as our testing criteria demanded that we either reach "
    },
    {
      "expression": "\"1.0\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F-Score or finish all sentences from the pool, "
    },
    {
      "expression": "\"FA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " tried to finish the pool without any further performance improvement on the "
    },
    {
      "expression": "\"0.99\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F-Score."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1806_09751v2_163"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1806.09751v2-163.json"]
}