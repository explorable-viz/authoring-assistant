{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 1 lists all hyper-parameters which have "
    },
    {
      "expression": "\"all\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " been chosen using only training and validation data. The "
    },
    {
      "expression": "\"two\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " encoders have been implemented using a Bidirectional Long Short-Term Memory (B-LSTM) (Hochreiter and Schmidhuber, 1997) while the decoder uses a unidirectional LSTM. Both the encoders and the decoder use "
    },
    {
      "expression": "\"two\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " hidden layers. For the attention network, we have used the OpenNMT's general option (Luong et al., 2015).\n\nIn this paragraph, there are no explicit values, comparative expressions, or superlative or aggregated expressions that need to be replaced by Fluid expressions as per the provided criteria."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1807_00248v1_171"
  ],
  "datasets": ["datasets/scigenCL/1807.00248v1-171.json"]
}