{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 1 lists all hyper-parameters which have all been chosen using only "
    },
    {
      "expression": "\"training\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"validation\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " data. The two encoders have been implemented using a "
    },
    {
      "expression": "\"Bidirectional Long Short-Term Memory (B-LSTM)\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ("
    },
    {
      "expression": "\"Hochreiter and Schmidhuber, 1997\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") while the decoder uses a unidirectional "
    },
    {
      "expression": "\"LSTM\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Both the encoders and the decoder use "
    },
    {
      "expression": "\"two\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " hidden layers. For the attention network, we have used the OpenNMT's general option ("
    },
    {
      "expression": "\"Luong et al., 2015\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ")."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1807_00248v1_171"
  ],
  "datasets": ["datasets/scigen/1807.00248v1-171.json"]
}