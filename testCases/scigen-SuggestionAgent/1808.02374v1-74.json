{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Understood. I will ensure that each expression is annotated with only one valid category. Here is the corrected paragraph:\n\nTable 2 shows that initializing the model with the pre-trained embeddings gives a significant "
    },
    {
      "expression": "\"4 1.1\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " point increase in F-measure compared to random initialization, due to an increase in precision. Fixing the embeddings gives slightly "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance than using them as initialization, an increase of "
    },
    {
      "expression": "\"0.9\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " point in F-measure, mostly due to "
    },
    {
      "expression": "\"higher\"",
      "categories": ["min-max"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". When extending the loss with the SGLR loss, we gain "
    },
    {
      "expression": "\"1.6\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in F-measure compared to fixing the word embeddings, and also surpass the state of the art by "
    },
    {
      "expression": "\"0.4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " even without specialized resources. If we train our model using the SG loss extension we obtain the "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results, and gain "
    },
    {
      "expression": "\"1.9\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " points in F-measure compared to using pre-trained fixed word embeddings. This setting also exceeds the state of the art (Lin et al., 2017) by "
    },
    {
      "expression": "\"0.7\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " points in F-measure, due to a gain of "
    },
    {
      "expression": "\"1.2\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " points in recall, again without using any specialized clinical NLP tools for feature engineering, in contrast to all state-of-the-art baselines."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1808_02374v1_74"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1808.02374v1-74.json"]
}