{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Got it! I will ensure to use only the specified categories. Here is the revised paragraph:\n\nTables 2 and 3 respectively show results for our pretraining and intermediate training experiments. From Table 2, among target tasks, we find the grammar-related "
    },
    {
      "expression": "\"CoLA\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " task benefits dramatically from LM pretraining: The results achieved with LM pretraining are significantly "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the results achieved without. In contrast, the meaning-oriented "
    },
    {
      "expression": "\"STS\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " sees good results with several kinds of pretraining, but does not benefit substantially from LM pretraining. Among pretraining tasks, language modeling performs "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", followed by "
    },
    {
      "expression": "\"MNLI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". The remaining pretraining tasks yield performance near that of the random baseline. Even our single-task baseline gets less than a one point gain over this simple baseline. The multitask models are tied or outperformed by models trained on one of their constituent tasks, suggesting that our approach to multitask learning does not reliably produce models that productively combine the knowledge taught by each task. However, of the two models that perform "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the development data, the multitask model generalizes "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the single-task model on test data for tasks like "
    },
    {
      "expression": "\"STS\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"MNLI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " where the test set contains out-of-domain data."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1812_10860v5_193"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1812.10860v5-193.json"]
}