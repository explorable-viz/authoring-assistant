{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Tables 2 and 3 respectively show results for our pretraining and intermediate training experiments. Looking to Table 3, using ELMo uniformly improves over "
    },
    {
      "expression": "\"training the encoder from scratch\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". The "
    },
    {
      "expression": "\"ELMo-augmented random baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " is strong, lagging behind the "
    },
    {
      "expression": "\"single-task baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " by less than a point. Most intermediate tasks beat the "
    },
    {
      "expression": "\"random baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", but several fail to significantly outperform the "
    },
    {
      "expression": "\"single-task baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". "
    },
    {
      "expression": "\"MNLI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and English\u2013German translation perform "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with ELMo, with SkipThought and DisSent also beating the "
    },
    {
      "expression": "\"single-task baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Intermediate multitask training on all the non-GLUE tasks produces our "
    },
    {
      "expression": "\"best-performing ELMo model\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Using BERT consistently outperforms "
    },
    {
      "expression": "\"ELMo\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"pretraining from scratch\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". We find that intermediate training on each of "
    },
    {
      "expression": "\"MNLI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"QQP\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and "
    },
    {
      "expression": "\"STS\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " leads to improvements over no intermediate training, while intermediate training on the other tasks harms transfer performance. The improvements gained via "
    },
    {
      "expression": "\"STS\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", a small-data task, versus the negative impact of fairly large-data tasks (e.g. QNLI), suggests that the benefit of intermediate training is not solely due to additional training, but that the signal provided by the intermediate task complements the original language modeling objective. Intermediate training on generation tasks such as "
    },
    {
      "expression": "\"MT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and SkipThought significantly impairs BERT's transfer ability. We speculate that this degradation may be due to catastrophic forgetting in fine-tuning for a task substantially different from the tasks "
    },
    {
      "expression": "\"BERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " was originally trained on. This phenomenon might be mitigated in our ELMo models via the frozen encoder and skip connection. On the test set, we lag slightly behind the "
    },
    {
      "expression": "\"BERT base results\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " from Devlin et al. (2019), likely due in part to our limited hyperparameter tuning."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1812_10860v5_194"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1812.10860v5-194.json"]
}