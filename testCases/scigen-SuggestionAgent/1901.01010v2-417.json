{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Experimental Results Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations. On Wikipedia, we observe that the performance of BILSTM, INCEPTION, and JOINT is "
    },
    {
      "expression": "\"much better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than that of all four baselines. INCEPTION achieves "
    },
    {
      "expression": "\"2.9\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy than BILSTM. The performance of JOINT achieves an accuracy of "
    },
    {
      "expression": "\"59.4\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, which is "
    },
    {
      "expression": "\"5.3\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher than using textual features alone (BILSTM) and "
    },
    {
      "expression": "\"2.4\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher than using visual features alone (INCEPTION). Based on a one-tailed Wilcoxon signed-rank test, the performance of JOINT is "
    },
    {
      "expression": "\"state-of-the-art\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results in combination. For arXiv, baseline methods MAJORITY, BENCHMARK, and INCEPTIONFIXED outperform BILSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, INCEPTIONFIXED is "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than MAJORITY and BENCHMARK over the arXiv cs.lg subset, which verifies the usefulness of visual features, even when only the last layer is fine-tuned. Table 3 also shows that INCEPTION and BILSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: INCEPTION and BILSTM are "
    },
    {
      "expression": "\"indistinguishable\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over cs.cl; BILSTM achieves "
    },
    {
      "expression": "\"1.8\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy over cs.lg, while INCEPTION achieves "
    },
    {
      "expression": "\"1.3\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy over cs.ai. Once again, the JOINT model achieves "
    },
    {
      "expression": "\"the highest\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " accuracy on cs.ai and cs.cl by combining textual and visual representations (at a level of statistical significance for cs.ai). This, again, confirms that textual and visual features achieve "
    },
    {
      "expression": "\"state-of-the-art\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results. On arXiv cs.lg, JOINT achieves a "
    },
    {
      "expression": "\"0.6\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy than INCEPTION by combining visual features and textual features, but BILSTM achieves "
    },
    {
      "expression": "\"the highest\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " accuracy. One characteristic of cs.lg documents is"
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1901_01010v2_417"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1901.01010v2-417.json"]
}
