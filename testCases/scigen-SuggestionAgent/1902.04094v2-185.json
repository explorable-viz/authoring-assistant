{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3. We find that, compared to GPT, the BERT generations are of "
    },
    {
      "expression": "\"worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " quality, but are "
    },
    {
      "expression": "\"more diverse\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Surprisingly, the outside language model, which was trained on Wikipedia, is "
    },
    {
      "expression": "\"less perplexed\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " by the GPT generations than the BERT generations, even though GPT was only trained on romance novels and BERT was trained on romance novels and Wikipedia. On actual data from TBC, the outside language model is about as "
    },
    {
      "expression": "\"perplexed\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " as on the BERT generations, which suggests that domain shift is an issue in using a trained language model for evaluating generations and that the GPT generations might have collapsed to fairly generic and simple sentences. The perplexity on BERT samples is not "
    },
    {
      "expression": "\"absurdly high\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and in reading the samples, we find that many are fairly coherent."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1902_04094v2_185"
  ],
  "datasets": ["datasets/scigen/1902.04094v2-185.json"]
}