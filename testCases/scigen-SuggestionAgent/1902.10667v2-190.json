{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We perform hyperparameter optimisation and make comparisons among our systems, including GCN + Bi-LSTM (GCN-based), CNN + attention + Bi-LSTM (Attbased), and their combination using a highway layer (H-combined) in Table 1. Systems are evaluated using two types of precision, recall and F-score measures: strict MWE-based scores (every component of an MWE should be correctly tagged to be considered as true positive), and token-based scores (a partial match between a predicted and a gold MWE would be considered as true positive). We report results for all MWEs as well as discontinuous ones specifically. GCN-based "
    },
    {
      "expression": "\"outperforms\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " Att-based and they both "
    },
    {
      "expression": "\"outperform\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the strong baseline in terms of MWE-based F-score in three out of four languages. Combining GCN with attention using highway networks results in further improvements for EN, FR and FA. The H-combined model consistently "
    },
    {
      "expression": "\"exceeds\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the baseline for all languages. GCN and H-combined models each show "
    },
    {
      "expression": "\"significant improvement\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with regard to discontinuous MWEs, regardless of the proportion of such expressions. The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1902_10667v2_190"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1902.10667v2-190.json"]
}
