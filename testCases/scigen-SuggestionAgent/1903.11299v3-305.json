{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The table 1 shows the caption retrieval recall on COCO dataset. The first two lines show the state-of-the-art results. The second pair of lines present the results of our model, with "
    },
    {
      "expression": "\"W2V\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"FastText\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " embeddings used as the baseline.  We can see that our model is close to the "
    },
    {
      "expression": "\"Deep SemanticVisual Embedding (DSVE)\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " method  while the "
    },
    {
      "expression": "\"W2V\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " method is slightly "
    },
    {
      "expression": "\"worst\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", as the representation power of the word embedding is reduced.  The "
    },
    {
      "expression": "\"BIVEC English-French\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " method is used in English and on both languages simultaneously. If trained only on English, i.e. only on the COCO dataset like the two previous methods, it shows performance similar to the one of the state-of-the-art. This means training using "
    },
    {
      "expression": "\"BIVEC\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " does not weaken the English representation. When trained on English and French together, the recall is increased by "
    },
    {
      "expression": "\"3.35\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " %, going from "
    },
    {
      "expression": "\"65.58\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % to "
    },
    {
      "expression": "\"67.78\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " %. We can also see an improvement for recall@5 and recall@10, with respectively "
    },
    {
      "expression": "\"1.17\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % and "
    },
    {
      "expression": "\"0.85\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % of increase. This implies that the similarity learning with French captions increases English recognition when using "
    },
    {
      "expression": "\"BIVEC\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ".  First of all, when training with "
    },
    {
      "expression": "\"MUSE\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for English only, we can see a sharp decrease in performance, with a recall going from "
    },
    {
      "expression": "\"66.08\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % to "
    },
    {
      "expression": "\"63.10\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " %. By comparing the model trained with "
    },
    {
      "expression": "\"W2V\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", we obtain similar results. This could come from the fact that both "
    },
    {
      "expression": "\"MUSE\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"W2V\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " embeddings do not have representation for out of vocabulary words like the "
    },
    {
      "expression": "\"FastText\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ones. Moreover, rare words have much more chance to be wrongly projected because of space transformation. When we train the model with additional languages, we can see a slight decrease in performance in English. The "
    },
    {
      "expression": "\"maximum\"",
      "categories": ["min-max"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " decrease is "
    },
    {
      "expression": "\"1.01\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % for recall@10, but it is counterbalanced by an increase of "
    },
    {
      "expression": "\"0.29\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % for the recall@1.  We evaluated our method on the COCO dataset for English-only results and shown that using "
    },
    {
      "expression": "\"BIVEC\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " embeddings enables the use of another language in order to improve the performance. The obtained improvement is a "
    },
    {
      "expression": "\"3.35\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % increase in performance on the COCO dataset, and a "
    },
    {
      "expression": "\"15.15\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % increase on the Multi30K dataset."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1903_11299v3_305"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1903.11299v3-305.json"]
}