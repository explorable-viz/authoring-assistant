{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "In Table 1 we compare our model ("
    },
    {
      "expression": "\"UTDSM\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") with our baseline ("
    },
    {
      "expression": "\"Global-DSM\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") and other state-of-the-art multi-prototype approaches for the contextual semantic similarity task. It is clear that all different setups of "
    },
    {
      "expression": "\"UTDSM\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " perform "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the baseline for both contextual semantic similarity metrics. Using a single Gaussian distribution ("
    },
    {
      "expression": "\"UTDSM + GMM (1)\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") at the smoothing step of our method produces similar results to the baseline model.  We also observe that random anchoring performs "
    },
    {
      "expression": "\"slightly worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than "
    },
    {
      "expression": "\"UTDSM\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with respect to AvgSimC.  Furthermore, we observe that GMM smoothing has a different effect on the MaxSimC and AvgSimC metrics. Specifically, for AvgSimC we consistently report "
    },
    {
      "expression": "\"lower\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results when GMM smoothing is applied for different number of components.  At the same time, our smoothing technique highly improves the performance of MaxSimC for all possible configurations.  Overall, the performance of our model is "
    },
    {
      "expression": "\"highly competitive\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to the state-of-the-art models in terms of AvgSimC, for "
    },
    {
      "expression": "\"500 dimensions\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". We also achieve "
    },
    {
      "expression": "\"state-of-the-art\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance for the MaxSimC metric, using smoothed topic embeddings of "
    },
    {
      "expression": "\"300 or 500 dimensions\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with "
    },
    {
      "expression": "\"2 or 3 Gaussian components\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1904_05674v1_217"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1904.05674v1-217.json"]
}