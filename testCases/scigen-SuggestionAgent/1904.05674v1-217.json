{
  "testing-variables": {},
  "paragraph": [{
    "type": "literal",
    "value": "In Table 1 we compare our model (UTDSM) with our baseline (Global-DSM) and other state-ofthe-art multi-prototype approaches for the contextual semantic similarity task. It is clear that all different setups of UTDSM perform better than the baseline for both contextual semantic similarity metrics. Using a single Gaussian distribution (UTDSM + GMM (1)) at the smoothing step of our method produces similar results to the baseline model.  We also observe that random anchoring performs slightly worse than UTDSM with respect to AvgSimC.  Furthermore, we observe that GMM smoothing has a different effect on the MaxSimC and AvgSimC metrics. Specifically, for AvgSimC we consistently report lower results when GMM smoothing is applied for different number of components.  At the same time, our smoothing technique highly improves the performance of MaxSimC for all possible configurations.  Overall, the performance of our model is highly competitive to the state-of-the-art models in terms of AvgSimC, for 500-dimensional topic embeddings. We also achieve state-of-the-art performance for the MaxSimC metric, using smoothed topic embeddings of 300 or 500 dimensions with 2 or 3 Gaussian components."
  }],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1904_05674v1_217"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1904.05674v1-217.json"]
}