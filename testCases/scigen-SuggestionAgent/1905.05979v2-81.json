{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "BLEU scores for our "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and the "
    },
    {
      "expression": "\"baselines\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the "
    },
    {
      "expression": "\"context-agnostic baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " trained only on "
    },
    {
      "expression": "\"1.5m dataset\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to show how the performance is influenced by the amount of data. We observe that our "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " is no "
    },
    {
      "expression": "\"worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in BLEU than the "
    },
    {
      "expression": "\"baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " despite the second-pass model  being trained only on a fraction of the data. In contrast, the "
    },
    {
      "expression": "\"concatenation baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", trained on a mixture of data with and without context is about "
    },
    {
      "expression": "\"1\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU below the "
    },
    {
      "expression": "\"context-agnostic baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "] and our "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when using all "
    },
    {
      "expression": "\"3\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " context sentences. CADec's performance remains the same independently from the number of context sentences ("
    },
    {
      "expression": "\"1, 2 or 3\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") as measured with BLEU. s-hier-to-2.tied performs "
    },
    {
      "expression": "\"worst\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in terms of BLEU, but note that this is a shallow recurrent model, while others are Transformer-based. It also suffers from the asymmetric data setting, like the concatenation baseline."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1905_05979v2_81"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1905.05979v2-81.json"]
}