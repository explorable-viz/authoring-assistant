{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Thank you for the clarification. Here is the corrected output:\n\nBLEU scores for our "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and the "
    },
    {
      "expression": "\"baselines\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the "
    },
    {
      "expression": "\"context-agnostic baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " trained only on "
    },
    {
      "expression": "\"1.5m\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " dataset to show how the performance is influenced by the amount of data. We observe that our "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " is no worse in BLEU than the "
    },
    {
      "expression": "\"baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " despite the second-pass "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "  being trained only on a fraction of the data. In contrast, the "
    },
    {
      "expression": "\"concatenation baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", trained on a mixture of data with and without context is about "
    },
    {
      "expression": "\"1\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU below the "
    },
    {
      "expression": "\"context-agnostic baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and our "
    },
    {
      "expression": "\"model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when using all "
    },
    {
      "expression": "\"3\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " context sentences. "
    },
    {
      "expression": "\"CADec's performance\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " remains the same independently from the number of context sentences ("
    },
    {
      "expression": "\"1\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"2\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " or "
    },
    {
      "expression": "\"3\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") as measured with BLEU. "
    },
    {
      "expression": "\"s-hier-to-2.tied\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performs "
    },
    {
      "expression": "\"worst\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in terms of BLEU, but note that this is a shallow recurrent model, while others are Transformer-based. It also suffers from the asymmetric data setting, like the "
    },
    {
      "expression": "\"concatenation baseline\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1905_05979v2_81"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1905.05979v2-81.json"]
}