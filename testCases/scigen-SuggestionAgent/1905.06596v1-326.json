{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The entry "
    },
    {
      "expression": "\"Joint Self-attention\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " corresponds to the results of our implementation of (He et al., 2018), that significantly improves the original results by "
    },
    {
      "expression": "\"0.7\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU point on the WMT14 de-en benchmark, and "
    },
    {
      "expression": "\"0.2\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on IWSLT. The same architecture with the proposed locality constraints (Local Joint Self-attention) establishes a new state of the art in IWSLT'14 de-en with "
    },
    {
      "expression": "\"35.7 BLEU\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", surpassing all previous published results by at least "
    },
    {
      "expression": "\"0.5 BLEU\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and our results with the unconstrained version by "
    },
    {
      "expression": "\"0.4\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". The Table 1 presents a comparison of the translation quality measured via BLEU score between the currently dominant "
    },
    {
      "expression": "\"Transformer\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (Vaswani et al., 2017) and "
    },
    {
      "expression": "\"Dynamic Convolutions\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (Wu et al., 2019) models, as well as the work by He et al. (2018), which also proposes a joint encoder-decoder structure, and also other refinements over the transformer architecture like (Ahmed et al., 2017), (Chen et al., 2018), (Shaw et al., 2018) and (Ott et al., 2018). The Joint Self-attention model obtains "
    },
    {
      "expression": "\"the same SoTA\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " BLEU score of (Wu et al., 2019) on WMT'14 en-de, and the same "
    },
    {
      "expression": "\"SoTA\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " score of (Ott et al., 2018) and (Wu et al., 2019) on WMT'14 enfr. The local attention constraints do not provide a significant gain on these bigger models, but it improves the BLEU score on WMT'14 en-fr to a new "
    },
    {
      "expression": "\"SoTA\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of "
    },
    {
      "expression": "\"43.3\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1905_06596v1_326"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1905.06596v1-326.json"]
}