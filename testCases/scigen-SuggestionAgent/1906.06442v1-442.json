{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "To understand how the model treats the tag and what biases it learns from the data, we investigate the entropy of the attention probability distribution, as well as the attention captured by the tag.  For the tagged variants, there is heavy attention on the "
    },
    {
      "expression": "\"tag\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when it is present (Table 6), indicating that the model relies on the information signalled by the "
    },
    {
      "expression": "\"tag\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ".  Table 6 reports "
    },
    {
      "expression": "\"the average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " length-normalized Shannon entropy:  The entropy of the attention probabilities from the model trained on BT data is "
    },
    {
      "expression": "\"the clear outlier\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1906_06442v1_442"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1906.06442v1-442.json"]
}