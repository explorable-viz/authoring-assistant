{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 2 shows that the performance got slightly hurt (comparing \""
    },
    {
      "expression": "\"Processed MT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "\" with \""
    },
    {
      "expression": "\"MT as PE\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "\") with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer ("
    },
    {
      "expression": "\"Base\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") model achieved "
    },
    {
      "expression": "\"the highest\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " single model BLEU score without joint training with the de-noising encoder task.  Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+"
    },
    {
      "expression": "\"0.46\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ")."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1908_03402v1_62"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1908.03402v1-62.json"]
}