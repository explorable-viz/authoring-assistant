{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Our proposed approach sets "
    },
    {
      "expression": "\"a new state of the art\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of "
    },
    {
      "expression": "\"84.38\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% binary accuracy on CMU-MOSI dataset of multimodal sentiment analysis; a significant leap from previous state of "
    },
    {
      "expression": "\"We compare the performance of M-BERT with the following models on the multimodal sentiment analysis task: RMFN (SOTA1)1 fuses multimodal information in multiple stages by focusing on a subset of signals in each stage\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ([Liang et al., 2018]). MFN (SOTA2) synchronizes states of three separate LSTMs with a multi-view gated memory (Zadeh et al., 2018a). MARN (SOTA3) models view-specific interactions using hybrid LSTM memories and cross-modal interactions using a Multi-Attention Block(MAB) (Zadeh et al., 2018c). We perform two different evaluation tasks on CMU-MOSI datset: i) Binary Classification, and ii) Regression. We formulate it as a regression problem and report "
    },
    {
      "expression": "\"Mean-absolute Error\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (MAE) and the correlation of model predictions with true labels. Besides, we convert the regression outputs into categorical values to obtain binary classification accuracy (BA) and F1 score. The performances of M-BERT and BERT are described in Table 1. M-BERT model outperforms all the baseline models (described in Sec.4.4) on every evaluation metrics with large margin. It sets "
    },
    {
      "expression": "\"new state-of-the-art performance\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for this task and achieves "
    },
    {
      "expression": "\"84.38\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, a "
    },
    {
      "expression": "\"5.98%\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " increase with respect to the SOTA1 and "
    },
    {
      "expression": "\"1.02%\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " increase with respect to BERT (text-only). Even BERT (text-only) model achieves "
    },
    {
      "expression": "\"83.36\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% accuracy, an increase of "
    },
    {
      "expression": "\"4.96%\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " from the SOTA1 "
    },
    {
      "expression": "\"78.4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, using text information only. It achieves "
    },
    {
      "expression": "\"higher performance\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in all evaluation metrics compare to SOTA1; reinforcing the expressiveness and utility of BERT contextual representation."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1908_05787v1_433"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1908.05787v1-433.json"]
}