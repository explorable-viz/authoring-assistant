{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The datasets and vision-language task models are described in the appendix, but are referenced in Table 1.  Unsurprisingly, when comparing the first lines of Table 1(a,b), we find that using "
    },
    {
      "expression": "\"Word2Vec\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " rather than an embedding trained from scratch tends to improve performance. This is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt, whose embeddings trained from scratch using their smaller vocabulary compare favorably to "
    },
    {
      "expression": "\"Word2Vec\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ".  "
    },
    {
      "expression": "\"Word2Vec\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " only falls behind within a point or two across all tasks, and even outperforms or performs equally as well as "
    },
    {
      "expression": "\"FastText\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for certain tasks (e.g. text-to-clip, image captioning).  Table 1 also contains a comparison of language model variants across the five vision-language tasks we evaluate on. We see that fine-tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model (e.g. "
    },
    {
      "expression": "\"5-10%\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " increase to "
    },
    {
      "expression": "\"mean recall\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on image-sentence retrieval).  When comparing the architecture choices from Figure 3 we see that for retrieval-based tasks (i.e. where the output is not free-form text) the "
    },
    {
      "expression": "\"Average Embedding\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"SelfAttention\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " models perform "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than a simple "
    },
    {
      "expression": "\"LSTM-based approach\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", with "
    },
    {
      "expression": "\"Self-Attention\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " being "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on average.  The only apparent exception to this is the text-to-clip task.  "
    },
    {
      "expression": "\"InferSent\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"BERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " reach comparable values to the "
    },
    {
      "expression": "\"best Word2Vec models\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for image-sentence retrieval on Flickr30K, performing more poorly for the MSCOCO dataset. For the remaining retrieval tasks, metrics are below the "
    },
    {
      "expression": "\"best performing model and embedding combination\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " within 1-3 points, again noting the unusual exception of "
    },
    {
      "expression": "\"InferSent\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on phrase grounding of Flickr30K Entities, which significantly drops below scratch performance.  While all language models perform closely on ReferIt phrase grounding, this still suggests that there is no need to use the more complex "
    },
    {
      "expression": "\"LSTM language model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " without additional modification.  Lastly, sentence level embeddings "
    },
    {
      "expression": "\"InferSent\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"BERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " are compared in Table 1(d); results are without fine-tuning.  The two are comparable to each other with the exception of phrase grounding accuracy on Flickr30K Entities; "
    },
    {
      "expression": "\"BERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " surprisingly outperforms "
    },
    {
      "expression": "\"InferSent\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " by "
    },
    {
      "expression": "\"11.55%\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Both "
    },
    {
      "expression": "\"InferSent\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"BERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " do not provide the "
    },
    {
      "expression": "\"best results\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " across any task, and thus are not a leading option for vision-language tasks."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1908_06327v1_329"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1908.06327v1-329.json"]
}