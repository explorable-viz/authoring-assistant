{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We use Amazon Mechanical Turk to label around "
    },
    {
      "expression": "\"13,000\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " potentially derogatory tweets  Table 1 compares different labelsets that exist in the literature. For instance, Waseem and Hovy (2016) use "
    },
    {
      "expression": "\"racist, sexist, and normal\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " as labels; Davidson et al. (2017) label their data as "
    },
    {
      "expression": "\"hateful, offensive (but not hateful), and neither\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as "
    },
    {
      "expression": "\"hate and non hate\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Founta et al. (2018) label their data as "
    },
    {
      "expression": "\"offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1908_11049v1_345"
  ],
  "datasets": ["datasets/scigen/1908.11049v1-345.json"]
}