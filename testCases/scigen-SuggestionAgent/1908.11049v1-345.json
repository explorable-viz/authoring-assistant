{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We use Amazon Mechanical Turk to label around "
    },
    {
      "expression": "\"13,000\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " potentially derogatory tweets. Table 1 compares different labelsets that exist in the literature. For instance, Waseem and Hovy (2016) use "
    },
    {
      "expression": "\"racist\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"sexist\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and "
    },
    {
      "expression": "\"normal\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " as labels; Davidson et al. (2017) label their data as "
    },
    {
      "expression": "\"hateful\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"offensive (but not hateful)\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and "
    },
    {
      "expression": "\"neither\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as "
    },
    {
      "expression": "\"hate\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"non hate\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Founta et al. (2018) label their data as "
    },
    {
      "expression": "\"offensive\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"abusive\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"hateful\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"aggressive\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"cyberbullying\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"spam\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and "
    },
    {
      "expression": "\"normal\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1908_11049v1_345"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1908.11049v1-345.json"]
}
