{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Got it, I'll adjust the categories accordingly. Here's the revised output:\n\nWe report test set tokenized "
    },
    {
      "expression": "\"BLEU\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (Papineni et al., 2002) results in Table 1. We can see that replacing "
    },
    {
      "expression": "\"softmax\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " by "
    },
    {
      "expression": "\"entmax\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly "
    },
    {
      "expression": "\"higher\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " "
    },
    {
      "expression": "\"BLEU\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", but their sparsity leads to a better potential for analysis. In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1909_00015v2_376"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1909.00015v2-376.json"]
}