{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Got it. I'll ensure that each annotation uses only one valid category. Here is the corrected output:\n\nWe test an unsupervised setting using (1) the algorithm proposed in Section 4.4 with "
    },
    {
      "expression": "\"EmbDI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " local embeddings, and (2) an existing matching system  with both pre-trained embeddings ("
    },
    {
      "expression": "\"SeepP\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") and our local embeddings ("
    },
    {
      "expression": "\"SeepL\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "). Pre-trained embeddings for tokens and tuples have been obtained from Glove .  Table 3 reports the results w.r.t. manually defined attribute matches. The simple unsupervised method with "
    },
    {
      "expression": "\"EmbDI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " local embeddings outperforms the baseline in terms of Fmeasure in all scenarios. Results of "
    },
    {
      "expression": "\"RefS\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " are the "
    },
    {
      "expression": "\"best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " because of the high overlap between its datasets. The baseline improves when it is executed with "
    },
    {
      "expression": "\"EmbDI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " local embeddings, showing their superior quality w.r.t. pre-trained ones. The Basic local embeddings lead to "
    },
    {
      "expression": "\"0\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " attribute matches.  We also observe that results for "
    },
    {
      "expression": "\"SeepPreTrain\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " depend on the quality of the original attribute labels. If we replace the original (expressive and correct) labels with synthetic ones, "
    },
    {
      "expression": "\"Seep-PreTrain\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " obtains F-measure values between [.30] and [.38]. Local embeddings from "
    },
    {
      "expression": "\"EmbDI\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " do not depend on the presence of the attribute labels.  Similarly, decreasing the size of the walks to "
    },
    {
      "expression": "\"5\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the SM task raises the F-measure for "
    },
    {
      "expression": "\"RefL\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to [.92] (from [.77])."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1909_01120v1_340"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1909.01120v1-340.json"]
}