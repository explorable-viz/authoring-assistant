{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We use two pre-trained deep models: a CNN (Jaderberg et al., 2016) and an LSTM (Ghosh et al., 2017) as baselines ("
    },
    {
      "expression": "\"BL\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") to extract the initial list of word hypotheses. We experimented extracting "
    },
    {
      "expression": "\"kbest\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " hypotheses for "
    },
    {
      "expression": "\"k\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " = 1 . . . 10. Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the "
    },
    {
      "expression": "\"90K\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " words of the CNN dictionary (which correspond to "
    },
    {
      "expression": "\"43.3%\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the "
    },
    {
      "expression": "\"k-best\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR),\nWe compare the results of our encoder with several state-of-the-art sentence encoders, tuned or trained on the same dataset. Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset. Our model FDCLSTM without attention achieves "
    },
    {
      "expression": "\"a better result\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in the case of the second baseline LSTM that full of false-positives and short words. We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve "
    },
    {
      "expression": "\"better performance\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than our TWE previous model (Sabir et al., 2018), that trained a word embedding ("
    },
    {
      "expression": "\"Mikolov et al., 2013\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") from scratch on the same task. As seen in Table 1, the introduction of this unigram lexicon produces "
    },
    {
      "expression": "\"the best results\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_1909_07950v2_7"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/1909.07950v2-7.json"]
}