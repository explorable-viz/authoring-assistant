{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The comparison between BERT embeddings and other models is presented in Table 3. Overall, in-domain fine-tuned BERT delivers "
    },
    {
      "expression": "\"the best\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance. We report new state-of-the-art results on WikiPassageQA ("
    },
    {
      "expression": "\"33\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% improvement in MAP) and InsuranceQA (version 1.0) ("
    },
    {
      "expression": "\"3.6\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance, and ([u, v, u ∗ v, |u − v|]) shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a "
    },
    {
      "expression": "\"1-2\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM (Huang et al., 2013), but still far behind the state-of-the-art interaction-based model SUBMULT+NN (Wang and Jiang, 2016) and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline "
    },
    {
      "expression": "\"significantly\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1910_07973v2_451"
  ],
  "datasets": ["datasets/scigenCL/1910.07973v2-451.json"]
}