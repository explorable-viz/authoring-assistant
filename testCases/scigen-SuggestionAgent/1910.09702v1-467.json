{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We also experimented with stacking BERT embeddings with all or some of the embeddings mentioned above. However, this resulted on "
    },
    {
      "expression": "\"lower\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " 2https://www.urbandictionary.com/ 3https://data.world/jaredfern/urban-dictionary embedding  In FLC, we only show the results of our "
    },
    {
      "expression": "\"best\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " model in Table 5 to focus more on the differences between propaganda techniques. A more elaborate study of performance of different models should follow in future work. The best model is a BiLSTM-CRF with flair and urban glove embed  As we can see in Table 5, we can divide the propaganda techniques into three groups according to the model's performance on the development and test sets. The first group includes techniques with "
    },
    {
      "expression": "\"non-zero\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1 scores on both datasets: Flag-Waving, Loaded Language, Name Calling,Labeling and Slogans. This group has techniques that appear frequently in the data and/or techniques with strong lexical signals (e.g. \"American People\" in Flag-Waving) or punctuation signals (e.g. quotes in Slogans). The second group has the techniques with a "
    },
    {
      "expression": "\"non-zero\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1 score on only one of the datasets but not the other, such as: Appeal to Authority, Appeal to Fear, Doubt, Reduction, and Exaggeration,Minimisation. Two out of these five techniques (Appeal to Fear and Doubt) have very small "
    },
    {
      "expression": "\"non-zero\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1 on the development set which indicates that they are generally challenging on our model and were only tagged due to minor differences between the two datasets. However, the remaining three types show significant drops from development to test sets or vice-versa. This requires further analysis to understand why the model was able to do well on one dataset but get zero on the other dataset, which we leave for future work. The third group has the remaining nine techniques were our sequence tagger fails to correctly tag any text span on either dataset. This group has the most infrequent types as well as types beyond the ability for our tagger to spot by looking at the sentence only such as Repetition.  Overall, our model has the "
    },
    {
      "expression": "\"highest\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " precision among all teams on both datasets, which could be due to adding the UBY  one-hot encoded features that highlighted some strong signals for some propaganda types. This also could be the reason for our model to have the "
    },
    {
      "expression": "\"lowest\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " recall among the top 7 teams on both datasets as having explicit handcrafted signals suffers from the usual sparseness that accompanies these kinds of representations which could have made the model more conservative in tagging text spans."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1910_09702v1_467"
  ],
  "datasets": ["datasets/scigenCL/1910.09702v1-467.json"]
}