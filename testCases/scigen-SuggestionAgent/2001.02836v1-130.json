{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The experimental results are shown in Table 1. As "
    },
    {
      "expression": "\"Keller\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " is created based on the PP distribution and have relatively small size while "
    },
    {
      "expression": "\"SP-10K\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " is created based on random sampling and has a much larger size, we treat the performance on "
    },
    {
      "expression": "\"SP-10K\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " as the major evaluation. Our embeddings significantly outperform other baselines, especially embedding based baselines. The only exception is PP on the "
    },
    {
      "expression": "\"Keller\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " dataset due to its biased distribution. In addition, there are other interesting observations. First, compared with 'dobj' and 'nsubj', 'amod' is simpler for word2vec and GloVe. The reason behind is that conventional embeddings only capture the co-occurrence information, which is enough to predict the selectional preference of"
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_2001_02836v1_130"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/2001.02836v1-130.json"]
}