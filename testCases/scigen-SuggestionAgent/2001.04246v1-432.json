{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The Base-KD is a naive knowledge distillation version in which only the logits of the last layer are distilled without considering hidden layer knowledge and supervised label knowledge. By incorporating the probe models, the performance (line 2 in Table 4) is "
    },
    {
      "expression": "\"consistently improved\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", indicating the benefits from hierarchically decomposed task-oriented knowledge. We then leverage Data Augmentation (DA) to enrich task-oriented knowledge and this technique also improves performance for all tasks, especially for tasks that have a limited scale of data (i.e., MRPC and RTE). DA is also adopted in existing KD-based compression studies (Tang et al., 2019; Jiao et al., 2019). When taking the supervised label knowledge (LCE) into consideration, the performance is "
    },
    {
      "expression": "\"further boosted\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", showing that this term is also important for AdaBERT by providing focused search hints."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_2001_04246v1_432"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/2001.04246v1-432.json"]
}
