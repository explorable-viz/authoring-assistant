{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Our results are given in Table 4. We found that "
    },
    {
      "expression": "\"Wikipedia\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"Twitter\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " datasets contained labels which were "
    },
    {
      "expression": "\"more general\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", evidenced by similar high F1 scores from both annotators ("
    },
    {
      "expression": "\"> 0.8\"",
      "categories": ["ratio"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "). For political speeches, we observed that the human annotators both found many more examples to be check-worthy than were labelled in the dataset."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_2003_02736v1_275"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/2003.02736v1-275.json"]
}