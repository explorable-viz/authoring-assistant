{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "To evaluate the ranking of entity terms we have computed "
    },
    {
      "expression": "\"nDCG@10\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"nDCG@100\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"MAP\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", see Table 4 for the results. We also compute "
    },
    {
      "expression": "\"Recall@k\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of relevant documents for automatically refined queries using the "
    },
    {
      "expression": "\"1st\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"2nd\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"3rd\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ranked entities. The scores can be found in Table 5. Tables 4 and 5 show that the "
    },
    {
      "expression": "\"Relevance Model\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " outperforms the "
    },
    {
      "expression": "\"Rocchio algorithm\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in every aspect. Both models outperform the "
    },
    {
      "expression": "\"auto-encoder approach (auto-ref )\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". We suspect that "
    },
    {
      "expression": "\"summing\"",
      "categories": ["sum"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over the encodings distorts the individual features too much for a correct extraction of relevant entities to be possible. The combination of all three models ("
    },
    {
      "expression": "\"auto-ref + rocchio + relevance\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") outperforms the other models in most cases. Especially the performance for ranking of entity terms is increased using the autoencoded features. However, it is interesting to see that the "
    },
    {
      "expression": "\"rocchio + relevance\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " model outperforms the recall for "
    },
    {
      "expression": "\"second and third best\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " terms. This indicates that for user-evaluated term suggestions, the inclusion of the auto-encoded features is advisable. For automatic query refinement however, in "
    },
    {
      "expression": "\"average\"",
      "categories": ["average"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", this is not the case."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen_SuggestionAgent/_9_468"
  ],
  "datasets": ["datasets/scigen_SuggestionAgent/9-468.json"]
}