{
  "datasets": [ "datasets/scigen/1805.02474v1-10.json" ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1805_02474v1_10"
  ],
  "testing-variables": {
    "time_lstm": [
      67,
      70,
      80,
      90,
      81,
      99,
      100,
      50
    ]
  },
  "variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As shown in Table 3, BiLSTM gives significantly"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"BiLSTM\").acc (model_ \"LSTM\").acc betterWorse",
      "categories": ["comparison"]
    },
    {
      "type": "literal",
      "value": "accuracies compared to uni-directional LSTM2, with the training time per epoch"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"BiLSTM\").time_s (model_ \"LSTM\").time_s growShrink",
      "categories": ["comparison"]
    },
    {
      "type": "literal",
      "value": "from"
    },
    {
      "type": "expression",
      "expression": "numToStr (model_ \"LSTM\").time_s",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "seconds to"
    },
    {
      "type": "expression",
      "expression": "numToStr (model_ \"BiLSTM\").time_s",
      "categories": ["data_retrieval"]

    },
    {
      "type": "literal",
      "value": "seconds. Stacking 2 layers of BiLSTM gives"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"2 stacked BiLSTM\").acc (model_ \"BiLSTM\").acc improvements",
      "categories": ["comparison"]
    },
    {
      "type": "literal",
      "value": "to development results, with a"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"2 stacked BiLSTM\").time_s (model_ \"BiLSTM\").time_s smallerHigher",
      "categories": ["comparison"]
    },
    {
      "type": "literal",
      "value": "time of"
    },
    {
      "type": "expression",
      "expression": "numToStr (model_ \"2 stacked BiLSTM\").time_s",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "seconds. 3 layers of stacked BiLSTM"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"3 stacked BiLSTM\").acc (model_ \"BiLSTM\").acc improve",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "the results. In contrast, S-LSTM gives a development result of"
    },
    {
      "type": "expression",
      "expression": "numToStr (model_ \"S-LSTM\").acc",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "%, which is significantly"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"S-LSTM\").acc (model_ \"2 stacked BiLSTM\").acc betterWorse",
      "categories": ["comparison"]
    },
    {
      "type": "literal",
      "value": "compared to 2-layer stacked BiLSTM, with a"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"S-LSTM\").param (model_ \"2 stacked BiLSTM\").param smallerHigher",
      "categories": ["comparison"]
    },
    {
      "type": "literal",
      "value": "number of model parameters and a"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"S-LSTM\").time_s (model_ \"2 stacked BiLSTM\").time_s shorterLonger",
      "categories": ["comparison", "data_retrieval", "data_retrieval"]
    },
    {
      "type": "literal",
      "value": "time of"
    },
    {
      "type": "expression",
      "expression": "numToStr (model_ \"S-LSTM\").time_s",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows),"
    },
    {
      "type": "expression",
      "expression": "(findWithKey_ \"time_s\" (minimum (map (fun y -> y.time_s) tableData)) tableData).model",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "is the"
    },
    {
      "type": "expression",
      "expression": "rankLabel \"most efficient\" (findIndex \"model\" \"CNN\" (insertionSort (fun a b -> a.time_s < b.time_s) tableData))",
      "categories": ["rank", "data_retrieval", "comparison"]
    },
    {
      "type": "literal",
      "value": "among all models compared, with the"
    },
    {
      "type": "expression",
      "expression": "rankLabel \"smallest\" (findIndex \"model\" \"CNN\" (insertionSort (fun a b -> a.param < b.param) tableData))",
      "categories": ["rank", "data_retrieval", "comparison"]
    },
    {
      "type": "literal",
      "value": "model size. On the other hand, a 3-layer stacked CNN gives an accuracy of"
    },
    {
      "type": "expression",
      "expression": "numToStr (model \"3 stacked CNN\" tableData).acc",
      "categories": ["data_retrieval"]
    },
    {
      "type": "literal",
      "value": "%, which is also the"
    },
    {
      "type": "expression",
      "expression": "rankLabel \"lowest\" (findIndex \"model\" \"CNN\" (insertionSort (fun a b -> a.time_s < b.time_s) tableData))",
      "categories": ["rank"]
    },
    {
      "type": "literal",
      "value": "compared with BiLSTM, hierarchical attention and S-LSTM. The"
    },
    {
      "type": "expression",
      "expression": "rankLabel \"best\" (findIndex \"model\" \"S-LSTM+Attention\" (insertionSort (fun a b -> b.acc < a.acc) tableData))",
      "categories": ["comparison", "data_retrieval"]
    },
    {
      "type": "literal",
      "value": "performance of hierarchical attention is obtained by S-LSTM+Attention in terms of both accuracy and efficiency. S-LSTM gives significantly"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"S-LSTM\").acc (model_ \"CNN\").acc betterWorse",
      "categories": ["comparison", "data_retrieval"]
    },
    {
      "type": "literal",
      "value": "accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used. Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still"
    },
    {
      "type": "expression",
      "expression": "trendWord (model_ \"S-LSTM\").acc (model_ \"BiLSTM\").acc underOverPerforming",
      "categories": ["comparison", "data_retrieval"]
    },
    {
      "type": "literal",
      "value": "BiLSTM significantly."
    }
  ]
}
