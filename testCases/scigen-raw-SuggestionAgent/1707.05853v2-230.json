{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 3 displays the results for our model evaluated on cnets for increasingly aggressive pruning levels (discarding only interjections, additionally discarding hypotheses with scores below "
    },
    {
      "expression": "\"0.001\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"0.01\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", respectively). As can be seen, using the full cnet except for interjections does not improve over the baseline.  However, when pruning low-probability hypotheses both pooling strategies improve over the baseline. Yet, average pooling performs "
    },
    {
      "expression": "\"worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the lower pruning threshold, which shows that the model is still affected by noise among the hypotheses.  Weighted pooling performs "
    },
    {
      "expression": "\"better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the lower pruning threshold of "
    },
    {
      "expression": "\"0.001\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " with which we obtain the "
    },
    {
      "expression": "\"highest\"",
      "categories": [
        "min-max",
        "rank"
      ],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " result overall, improving the joint goals accuracy by "
    },
    {
      "expression": "\"1.6 percentage points\"",
      "categories": ["difference"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared to the baseline.  Moreover, we see that an ensemble model that averages the predictions of "
    },
    {
      "expression": "\"ten\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " cnet models trained with different random seeds also outperforms an ensemble of ten baseline models.  Our ensemble models outperform Mrksic et al. (2017) for the joint requests but are "
    },
    {
      "expression": "\"a bit worse\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the joint goals."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1707_05853v2_230"
  ],
  "datasets": ["datasets/scigen/1707.05853v2-230.json"]
}