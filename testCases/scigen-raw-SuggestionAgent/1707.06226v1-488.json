{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 3 shows the results on the Twitter dataset. As for discussion forums, adding context using the SVM models does not show a statistically significant improvement. For the neural networks model, similar to the results on discussion forums, the LSTM models that read both context and reply "
    },
    {
      "expression": "\"outperform\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the LSTM model that reads only the reply (LSTMr). The "
    },
    {
      "expression": "\"best performing\"",
      "categories": [
        "min-max",
        "rank"
      ],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMcas +LSTMras ). LSTMconditional model shows an improvement of "
    },
    {
      "expression": "\"11%\"",
      "categories": ["difference"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1 on the S class and "
    },
    {
      "expression": "\"4-5%\"",
      "categories": ["difference"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "F1 on the  NS class, compared to LSTMr. For the attentionbased models, the improvement using context is smaller ("
    },
    {
      "expression": "\"âˆ¼2%\"",
      "categories": ["difference"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1)."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1707_06226v1_488"
  ],
  "datasets": ["datasets/scigen/1707.06226v1-488.json"]
}