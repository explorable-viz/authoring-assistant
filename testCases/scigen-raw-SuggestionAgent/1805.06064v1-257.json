{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As expected, Table 4 shows that people with "
    },
    {
      "expression": "\"less\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " domain knowledge are "
    },
    {
      "expression": "\"more easily deceived\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Specifically, non-CS human judges fail at "
    },
    {
      "expression": "\"more than half\"",
      "categories": [
        "ratio",
        "comparison"
      ],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the 1-to-1 sets for the same titles, which suggests that most of our system generated abstracts follow correct grammar and consistent writing style. Domain experts fail on "
    },
    {
      "expression": "\"1 or 2 sets\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", mostly because the human written abstracts in those sets don't seem very "
    },
    {
      "expression": "\"topically relevant\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Additionally, the more abstracts that we provided to human judges, the easier it is to conceal the system generated abstract amongst human generated ones."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1805_06064v1_257"
  ],
  "datasets": ["datasets/scigen/1805.06064v1-257.json"]
}