{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 1: Results on the NIST Chinese-English translation task. \"Params\" denotes the number of model parameters. \"Emb.\" represents the number of parameters used for word representation. \"Red.\" represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. (2018) with the same datasets and vocabulary settings. \"↑\" indicates the result is "
    },
    {
      "expression": "\"significantly better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than that of the vanilla Transformer (p < 0.01), while \"⇑\" indicates the result is "
    },
    {
      "expression": "\"significantly better\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than that of all other Transformer models (p < 0.01). All significance tests are measured by paired bootstrap resampling (Koehn, 2004). Table 1 reports the results on the NIST ChineseEnglish test sets. It is observed that the Transformer models "
    },
    {
      "expression": "\"significantly outperform\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " SMT and RNNsearch models. Therefore, we decide to implement all of our experiments based on Transformer architecture. The direct bridging model can further improve the translation quality of the Transformer baseline. The decoder WT model improves the translation quality while reducing the number of parameters for the word representation. This improved performance happens because there are fewer model parameters, which prevents over-fitting (Press and Wolf, 2017). Finally, the performance is further improved by the proposed method while using even fewer parameters than other models."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1906_03100v1_278"
  ],
  "datasets": ["datasets/scigen/1906.03100v1-278.json"]
}