{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The analysis in Section 4 which shows that our choice of function f satisfies asymmetry and transitive properties, holds true because f satisfies f ((cid:126)x) ≥ (cid:126)x component-wise.  "
    },
    {
      "expression": "\"Table 4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " shows the results for each of these ablation experiments, when evaluated on the unsupervised hypernym detection task across four datasets chosen randomly. Removing the Residual layer and using RELU activation function only, violates the aforementioned component-wise inequality f ((cid:126)x) ≥ (cid:126)x, and has the "
    },
    {
      "expression": "\"worst\"",
      "categories": ["rank"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results out of the three. On the other hand, using Residual connections with Tanh activations may not violate the aforementioned inequality, since, it depends upon the sign of the activation outputs. This argument is supported by the results in "
    },
    {
      "expression": "\"Table 4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", wherein using Tanh activations instead of RELU almost provides identical results, except for the BLESS dataset. Nevertheless, the results in "
    },
    {
      "expression": "\"Table 4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " show that encouraging asymmetry and transitive properties for this task, in fact improves the results as opposed to not doing the same."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1909_10572v2_410"
  ],
  "datasets": ["datasets/scigen/1909.10572v2-410.json"]
}