{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We next conduct a series of experiments in which we train on "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"DBERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and "
    },
    {
      "expression": "\"DRoBERTa\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", and observe how well models can then learn to generalise on the respective test portions of these datasets. Table 5 shows the results, and there is a multitude of observations. First, one clear trend we observe across all training data setups is a "
    },
    {
      "expression": "\"clear negative performance progression\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when evaluated against datasets constructed with a stronger model in the loop. This trend holds true for all but the "
    },
    {
      "expression": "\"BiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " model, in each of the training configurations, and for each of the evaluation datasets. For example, RoBERTa trained on DRoBERTa achieves "
    },
    {
      "expression": "\"71.4\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"53.5\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"48.6\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"38.9 F1\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when evaluated on DSQuAD, DBiDAF, DBERT and DRoBERTa, respectively. Second, we observe that the "
    },
    {
      "expression": "\"BiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " model is not able to generalise well to datasets constructed with a model in the loop, independent of its training setup. In particular it is unable to learn from DBiDAF, thus failing to overcome some of its own blind spots through adversarial training. Both when training only on "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", as well as when adding DSQuAD to "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " during training (cf. Table 6), BiDAF performs poorly across all the adversarial datasets. results in Table 5, where training on "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in several cases led to "
    },
    {
      "expression": "\"better generalisation\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than training on "
    },
    {
      "expression": "\"DRoBERTa\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". we further train each of our three models on either "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"DBERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", or "
    },
    {
      "expression": "\"DRoBERTa\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and test on DSQuAD, with results in the DSQuAD columns of Table 5. First, we observe clear generalisation improvements towards DDROP across all models compared to training on "
    },
    {
      "expression": "\"DSQuAD(10K)\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when using any of the "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"DBERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", or "
    },
    {
      "expression": "\"DRoBERTa\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " datasets for training. That is, including a model in the loop for the training dataset leads to improved transfer towards DDROP. Note that the DROP dataset also makes use of a "
    },
    {
      "expression": "\"BiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " model in the loop during annotation; these results are in line with our prior observations when testing the same setups on "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", "
    },
    {
      "expression": "\"DBERT\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"DRoBERTa\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", compared to training on "
    },
    {
      "expression": "\"DSQuAD(10K)\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Second, we observe overall strong transfer results towards DNQ: up to "
    },
    {
      "expression": "\"71.0F1\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for a BERT model trained on "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". Note that this result is similar and even slightly improves over model training with SQuAD data of the same size. That is, relative to training on "
    },
    {
      "expression": "\"SQuAD\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " data, training on adversarially collected data "
    },
    {
      "expression": "\"DBiDAF\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " does not impede generalisation to the DNQ dataset, which was created without a model in the annotation loop. We then however see a similar negative performance progression as observed before when testing on "
    },
    {
      "expression": "\"DSQuAD\"",
      "categories": ["data-retrieval"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ": the stronger the model in the annotation loop of the training dataset, the "
    },
    {
      "expression": "\"lower\"",
      "categories": ["comparison"],
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the test accuracy on test data from a data distribution composed without using a model in the loop."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_2002_00293v1_335"
  ],
  "datasets": ["datasets/scigen/2002.00293v1-335.json"]
}