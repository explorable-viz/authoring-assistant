{
  "datasets": [
    "datasets/scigen/10-460.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_10_460"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We compare all approaches across the InsuranceQA and WikiPassageQA benchmarks as well as the five StackExchange datasets in Table 2. For the cQA answer selection datasets we measure the accuracy, which is the ratio of correctly selected answers, and for the passage retrieval in WikiPassageQA we report MAP/MRR. The results show that COALA substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets. For instance, on the cQA datasets COALA improves by 4.5pp over CA-Wang and by 8.8pp over the best semantic similarity method on average.  Our extended approach COALA p-means improves the performance of COALA on these datasets by an additional 1.6pp. The proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases.  The results in Table 2 show that our proposed syntax-aware extension COALA syntax-aware, which incorporates syntactic roles of word sequences to learn syntax-aware aspect representations, improves the results in five out of seven cases.  It thereby achieves an an average improvement of 0.7pp over COALA in our cQA datasets."
    }
  ]
}
