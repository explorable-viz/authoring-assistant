{
    "datasets": [
        "datasets/scigen/1704.07203v3-455.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigen/_1704_07203v3_455"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "The average performances of LR−syntax and CNN:rand are virtually identical, both for Macro6Described as FscoreM in Sokolova and Lapalme (2009). F1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p ≤ 0.05). Altogether, these two systems exhibit significantly better average performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p ≤ 0.05).  The performance of the learners is quite divergent across datasets, with Macro-F1 scores6 ranging from 60% (WTP) to 80% (MT), average 67% (see Table 2).  On all datasets, our best systems clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information."
        }
    ]
}
