{
  "datasets": [
    "datasets/scigen/1707.06480v1-31.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1707_06480v1_31"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The results of evaluating these three models on small (1M tokens) and medium-sized (17Mâ€“ 57M tokens) data sets against Char-CNN for different languages are provided in Table 3. The models demonstrate similar performance on small data, but Char-CNN scales significantly better on medium-sized data. From the three syllable-aware models, Syl-Concat looks the most advantageous as it demonstrates stable results and has the least  number of parameters. Therefore in what follows we will make a more detailed comparison of SylConcat with Char-CNN."
    }
  ]
}