{
    "datasets": [
        "datasets/scigen_SuggestionAgent/1805.00456v3-134.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigen_SuggestionAgent/_1805_00456v3_134"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "results in Table 3 show that large batch training can significantly improve the performance of single Transformers, particularly when trained to produce longer sequences. Accumulating the gradient over 8 batches of size 4096 gives a 3 BLEU improvement for the linear derivation model. It has been suggested that decaying the learning rate can have a similar effect to large batch training (Smith et al., 2017), but reducing the initial learning rate by a factor of 8 alone did not give the same improvements."
        }
    ]
}
