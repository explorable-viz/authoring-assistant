{
  "datasets": [
    "datasets/scigen/1805.02023v4-203.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1805_02023v4_203"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1score of 62.47%. Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information. In addition, a combination of both gives a 69.64% F1-score, which is the best  among various character representations.  Table 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a 64.12% F1-score, which is higher compared to the character-based baseline. This demonstrates that both word information and character information are useful for Chinese NER. The two methods of  word+char LSTM and word+char LSTM(cid:48), lead to similar improvements.  A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations. On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.  As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of 71.62%, which is significantly7 higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information."
    }
  ]
}