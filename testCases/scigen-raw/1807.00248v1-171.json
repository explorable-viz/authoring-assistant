{
    "datasets": [
        "datasets/scigen/1807.00248v1-171.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigen/_1807_00248v1_171"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "Table 1 lists all hyper-parameters which have all been chosen using only training and validation data. The two encoders have been implemented using a Bidirectional Long Short-Term Memory (B-LSTM) (Hochreiter and Schmidhuber, 1997) while the decoder uses a unidirectional LSTM. Both the encoders and the decoder use two hidden layers. For the attention network, we have used the OpenNMT's general option (Luong et al., 2015)."
        }
    ]
}
