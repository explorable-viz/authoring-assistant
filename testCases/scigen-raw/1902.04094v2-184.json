{
  "datasets": [
    "datasets/scigen/1902.04094v2-184.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1902_04094v2_184"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3.  This observation is further bolstered by the fact that the GPT generations have a higher corpus-BLEU with TBC than TBC has with itself.  The corpusBLEU between BERT models and the datasets is low, particularly with WT103.  We find that BERT generations are more diverse than GPT generations. GPT has high n-gram overlap (smaller percent of unique n-grams) with TBC, but surprisingly also with WikiText-103, despite being trained on different data. Furthermore, GPT generations have greater n-gram overlap with these datasets than these datasets have with themselves, further suggesting that GPT is relying significantly on generic sentences. BERT has lower n-gram overlap with both corpora, with similar degrees of n-gram overlap as the samples of the data."
    }
  ]
}