{
  "datasets": [
    "datasets/1904.05584v1-244.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/_1904_05584v1_244"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 2 shows the impact that different methods for combining character and word-level word representations have in the quality of the sentence representations produced by our models.  We can observe the same trend mentioned in section 4.1, and highlighted by the difference between bold values, that models trained in MultiNLI performed better than those trained in  SNLI at a statistically significant level,  The two exceptions to the previous trend, SICKE and SICKR, benefited more from models trained on SNLI.  Additionally, there was no method that significantly outperformed the word only baseline in classification tasks.  On the other hand, the vector gate significantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI."
    }
  ]
}