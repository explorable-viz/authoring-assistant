{
    "datasets": [
        "datasets/scigen/1906.01815v1-489.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigen/_1906_01815v1_489"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "Table 2 presents the classification results for sarcasm prediction in the speaker-dependent setup.  The lowest performance is obtained with the Majority baseline which achieves 33.3% weighted Fscore (66.7% F-score for non-sarcastic class and 0% for sarcastic). The pre-trained features for the visual modality provide the best performance among the unimodal variants. The addition of textual features through concatenation improves the unimodal baseline and achieves the best performance. The tri-modal variant is unable to achieve the best score due to a slightly sub-optimal performance from the audio modality. Overall, the combination of visual and textual signals significantly improves over the unimodal variants, with a relative error rate reduction of up to 12.9%.  In multiple evaluations, the multimodal variants were shown to significantly outperform their unimodal counterparts, with relative error rate reductions of up to 12.9%."
        }
    ]
}
