{
    "datasets": [
        "datasets/scigen_SuggestionAgent/1908.03402v1-62.json"
    ],
    "imports": [
        "scigen",
        "util",
        "datasets/scigen_SuggestionAgent/_1908_03402v1_62"
    ],
    "variables": {},
    "testing-variables": {},
    "paragraph": [
        {
            "type": "literal",
            "value": "Table 2 shows that the performance got slightly hurt (comparing \"Processed MT\" with \"MT as PE\") with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer (Base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task.  Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46)."
        }
    ]
}
