{
  "datasets": [
    "datasets/scigen/1909.07950v2-7.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_1909_07950v2_7"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We use two pre-trained deep models: a CNN (Jaderberg et al., 2016) and an LSTM (Ghosh et al., 2017) as baselines (BL) to extract the initial list of word hypotheses.  We experimented extracting kbest hypotheses for k = 1 . . . 10.  Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the 90K words of the CNN dictionary (which correspond to 43.3% of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the k-best produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR),  We compare the results of our encoder with several stateof-the-art sentence encoders, tuned or trained on the same dataset.  Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset.  Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that full of false-positives and short words.  We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve better performance than our TWE previous model (Sabir et al., 2018), that trained a word embedding (Mikolov et al., 2013) from scratch on the same task.  As seen in Table 1, the introduction of this unigram lexicon produces the best results."
    }
  ]
}