{
  "datasets": [
    "datasets/scigen/2001.05316v1-295.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_2001_05316v1_295"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples.  The classification was carried out with 2 author attribution datasets, one with 6 authors  and our dataset with maximum 14 authors. The larger dataset was trained with 6,8,10,12 and 14 authors to analyze the effects of increasing classes on the proposed model.  We evaluate the performance of the proposed architecture in terms of accuracy,  We also try to infer how the character-level model compares with the word level models. All models are compared for the increasing number of authors(classes) on the corpus mentioned to assess the quality of the models.  both word and character levels are summarized in Table II.  From the accuracy comparisons shown in Table II we see that Skip-gram implemented by fastText performs well in the given datasets.  Character level model performs reasonably well in competition with subword level as long as the dataset is big enough.  With larger datasets, this model will be able to perform significantly better compare character embeddings with word embeddings showing that character embeddings perform almost as good as the best word embedding model."
    }
  ]
}