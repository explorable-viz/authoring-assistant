{
  "datasets": [
    "datasets/2002.00293v1-342.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/_2002_00293v1_342"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "For example, RoBERTa trained on DRoBERTa reaches 38.9F1 on DRoBERTa, and this number further increases to 47.2F1 when including SQuAD during training (cf. Table 6).  In Table 6 we show experimental results for the same models and training datasets, but now including SQuAD as additional training data. In this training setup we generally see improved generalisation to DBiDAF, DBERT, and DRoBERTa. Interestingly, the relative differences between DBiDAF, DBERT, and DRoBERTa as training set used in conjunction with SQuAD are now much diminished, and especially DRoBERTa as (part of the) training set now generalises substantially better. RoBERTa achieves the strongest results on any of the DBiDAF, DBERT, and DRoBERTa evaluation sets, in particular when trained on DSQuAD+DRoBERTa.  we identify a risk of datasets constructed with weaker models in the loop becoming outdated. For example, RoBERTa achieves 58.2EM/73.2F1 on DBiDAF, in contrast to 0.0EM/5.5F1 for BiDAF – which is not far from non-expert human performance of 62.6EM/78.5F1.  We furthermore observe a gradual decrease in generalisation to SQuAD when training on DBiDAF towards training on DRoBERTa. This suggests that the stronger the model used in the annotation loop, the more dissimilar the data distribution becomes from the original SQuAD distribution. We will later find further support for this explanation in a qualitative analysis (Section 5). It may however also be due to a limitation of BERT and RoBERTa – similar to BiDAF – in learning from a data distribution designed to beat these models; an even stronger model might learn more e.g. from DRoBERTa."
    }
  ]
}