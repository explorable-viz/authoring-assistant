{
  "datasets": [
    "datasets/scigen/2003.01857v1-420.json"
  ],
  "imports": [
    "scigen",
    "util",
    "datasets/scigen/_2003_01857v1_420"
  ],
  "variables": {},
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The experiment results are presented in Table 2. The best performances of our configurations are highlighted in red, which are 8.37, 3.67, 13.79 and 7.89 for the error rates of our proposed model on AG, Sogou, AG(5k), Sogou(10k) respectively. The bold numbers are the officially reported accuracy of VDCNN, to which our proposed model is close. The numbers in blue are the results coming from our comparison experiment by using VDCNN where we set the sequence length to 256 under word level. From these results we can see that our model outperforms VDCNN on AG News for the official error rate, and is very close to VDCNN's performance on Sogou News. If we set VCDNN with the same input sequence length (256) in word-level, the performance of our proposed model is obviously better.  Most of the contributions come from external matrix From the results of Table 2, in the case of training with a large scale dataset, no matter we use simple LSTM or more complex bi-directional LSTM with self-attention, the testing error rates of different configurations are basically similar to each other. It can be said that such near state-of-the-art performance mainly attributes to the contribution from external memory.  Using abstract to build the external memory is better than contents  From Table 2 we can see that the results of using the description to construct the semantics matrix are better than using the abstract.  SeMemNN can still work on a few-shot learning Table 2 shows that although we have greatly shrank the scale of the training set, our proposed method can still outperform VDCNN. After shrinking the scale of the data, the performance of VDCNN has been greatly decreased, especially for Sogou news, VDCNN has been unable to learn from the training samples."
    }
  ]
}