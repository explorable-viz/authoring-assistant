{"testing-variables":{},"paragraph":[{"type":"literal","value":"First, we report our results on English data (see Table 3, top)  we compare against German data (see Table 3, bottom).  Our new strong Data-Lexicon Baseline reaches a considerable accuracy of "},{"expression":"\"86.32\"","type":"expression"},{"type":"literal","value":" %, which is hard to beat by trained models. Even the most recent state of the art only beats it by about two points: "},{"expression":"\"88.41\"","type":"expression"},{"type":"literal","value":" % (Hermann et al., 2014). However, the accuracy of the baseline drops for ambiguous predicates ("},{"expression":"\"69.73\"","type":"expression"},{"type":"literal","value":" %) and the F1-macro score reveals its weakness toward minority classes (drop from "},{"expression":"\"64.54\"","type":"expression"},{"type":"literal","value":" % to "},{"expression":"\"37.42\"","type":"expression"},{"type":"literal","value":" %).  Our unimodal system trained and evaluated on English data slightly exceeds the accuracy of the previous state of the art ("},{"expression":"\"88.66\"","type":"expression"},{"type":"literal","value":" % on average versus "},{"expression":"\"88.41\"","type":"expression"},{"type":"literal","value":" % for Hermann et al., 2014); our best run's accuracy is "},{"expression":"\"89.35\"","type":"expression"},{"type":"literal","value":" %.  \n\nTable 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni \u2013 our-mm \u2013 with IMAGINED embeddings (and synset visual embeddings for English). "},{"expression":"\"Best results highlighted in bold.\"","type":"expression"},{"type":"literal","value":"\n\nThe best run's results for English were: our uni: acc: "},{"expression":"\"89.35\"","type":"expression"},{"type":"literal","value":" ; acc amb: "},{"expression":"\"76.45\"","type":"expression"},{"type":"literal","value":" ; F1-m: "},{"expression":"\"76.95\"","type":"expression"},{"type":"literal","value":" ; F1-m amb: "},{"expression":"\"54.02\"","type":"expression"},{"type":"literal","value":" (with lexicon) our mm (im, synsV): acc: "},{"expression":"\"89.09\"","type":"expression"},{"type":"literal","value":" ; acc amb: "},{"expression":"\"75.86\"","type":"expression"},{"type":"literal","value":" ; F1-m: "},{"expression":"\"78.17\"","type":"expression"},{"type":"literal","value":" ; F1-m amb: "},{"expression":"\"57.48\"","type":"expression"},{"type":"literal","value":" (with lexicon)  Our system evaluated on German data sets a new state of the art on this corpus with "},{"expression":"\"80.76\"","type":"expression"},{"type":"literal","value":" % accuracy, outperforming the baselines ("},{"expression":"\"77.16\"","type":"expression"},{"type":"literal","value":" %; no other system evaluated on this dataset).  We report results achieved without the lexicon to evaluate independently of its quality (Hartmann et al., 2017). On English data, our systems outperforms Hartmann et al. (2017) by "},{"expression":"\"more than\"","type":"expression"},{"type":"literal","value":" two points in accuracy and we achieve a large improvement over the Data Baseline. Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about twenty points for English data. For German data, the increase of F1-macro with lexicon versus without is small (one point)."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_12_474"],"datasets":["datasets/scigenCL/12-474.json"]}