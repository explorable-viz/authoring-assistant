{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It seems that there was an error in copying over the text, as it includes repetitive sections and some unrelated content (\"non ï¿½ riconosciuto come comando interno o esterno,\" is Italian for \"is not recognized as an internal or external command\"). I'll correct and annotate the meaningful parts of the paragraph:\n\nTable 2 shows how the position of the context window influences the average model performance. Note that symmetric windows, for instance, are in fact "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the 'left' or 'right' windows of the same size, as they consider words both to the left and to the right of the focus word. This is most likely why symmetric windows consistently outperform 'single-sided' ones on the analogy task, as they are able to include twice as much contextual input. However, the average performance on the semantic similarity task (as indicated by the Spearman correlation with the SimLex999 test set) does not exhibit the same trend. 'Left' windows are indeed "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than symmetric ones, but 'right' windows are "
    },
    {
      "expression": "\"only one percent point behind them\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for Gigaword and only one percent point behind them for OpenSubtitles. It means that in many cases (at least with English texts) taking into account only n context words to the right of the focus word is sufficient to achieve the same performance with SimLex999 as by using a model which additionally considers n words to the left, and thus requires significantly more training time.\n\nPlease note that I had to make an assumption about \"only one percent point behind them\" as it appeared twice in the text and was not replaced or clarified. If you intended for this phrase to only appear once (either with OpenSubtitles or Gigaword), please adjust accordingly."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1704_05781v1_263"
  ],
  "datasets": ["datasets/scigenCL/1704.05781v1-263.json"]
}