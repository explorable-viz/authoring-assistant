{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The average performances of LR−syntax and CNN:rand are virtually identical, both for Macro-F1 [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r] and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p ≤ 0.05). Altogether, these two systems exhibit "
    },
    {
      "expression": "\"average performances\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p ≤ 0.05). The performance of the learners is quite divergent across datasets, with Macro-F1 scores ranging from "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ([REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]) to "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " ([REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]), average "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (see Table 2). On all datasets, our "
    },
    {
      "expression": "\"systems clearly outperform both baselines\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information.\n\nIt seems there's an error in the original text regarding paths that do not seem to correspond to values or meaningful expressions. These should be corrected if they are errors. If these paths represent some form of dynamic value references and were meant to be included, then the instruction for replacing them should clarify how to handle such cases. For this output example, I've annotated only the explicit numerical values and comparative expressions according to the given rules."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1704_07203v3_455"
  ],
  "datasets": ["datasets/scigenCL/1704.07203v3-455.json"]
}