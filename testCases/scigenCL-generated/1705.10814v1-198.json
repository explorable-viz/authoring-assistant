{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The experimental results are shown in Table 1, with Int denoting internal comparisons (with three groups) and Ext denoting external comparisons, the \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno, LAS in each group is marked in bold face. In the first group, we compare the LAS of the four single models WORD, W2V, LSTM, and CNN. In macro average of all languages, the CNN model performs "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the WORD model, and "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the W2V model. The LSTM model, however, performs only "
    },
    {
      "expression": "\"worse\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the WORD model and "
    },
    {
      "expression": "\"worse\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the CNN model. In the second group, we observe that the additional word-lookup model does not significantly improve the CNN model (from "
    },
    {
      "expression": "\"% in CNN\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to "
    },
    {
      "expression": "\"% in CNN+WORD on average\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") while the LSTM model is improved by a much larger margin (from "
    },
    {
      "expression": "\"% in LSTM\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to "
    },
    {
      "expression": "\"% in LSTM+WORD on average\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "). This suggests that the CNN model has already learned the most important information from the word forms, while the LSTM model has not. Also, the combined CNN+WORD model is still "
    },
    {
      "expression": "\"worse\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the LSTM+WORD model, despite the large improvement in the latter. While comparing to the published results (Björkelund et al., 2013, 2014), we have to note that their approach uses explicit morphological features, ensemble, ranking, etc., which all can boost parsing performance. We only use a greedy parser with much fewer features, but bridge the "
    },
    {
      "expression": "\"points\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " gap between the previous best greedy parser and the published result by more than one half. On average, the B15-LSTM model improves their own baseline by "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", similar to the "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " improvement of our LSTM model, which is much smaller than the "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " improvement of the CNN model. Furthermore, the CNN model is improved from a strong baseline: our WORD model performs already "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the B15-WORD model. Comparing the individual performances on each language, we observe that the CNN model almost always "
    },
    {
      "expression": "\"outperforms\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " the WORD model except for Hebrew. However, both LSTM and B15-LSTM perform "
    },
    {
      "expression": "\"worse\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than baseline only on the three agglutinative languages (Basque, Hungarian, and Korean), and "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than baseline on the other six."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1705_10814v1_198"
  ],
  "datasets": ["datasets/scigenCL/1705.10814v1-198.json"]
}