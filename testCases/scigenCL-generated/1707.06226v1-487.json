{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 2 shows the classification results on the discussion forum dataset. Although a vast majority of the context posts contain \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r sentences, around "
    },
    {
      "expression": "\"ten\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " sentences have \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r sentences and thus we set a cutoff to a "
    },
    {
      "expression": "\"ten\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of ten sentences for context modeling. For the reply r we considered the entire reply. The SVM models that are based on discrete features did not perform very well, and adding context actually hurt the performance. Regarding the performance of the neural network models, we observe that modeling context improves the performance using all types of LSTM architectures that read both context (c) and reply (r). The "
    },
    {
      "expression": "\"performance\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance when considering both the S and S classes is achieved by the LSTMconditional model ("
    },
    {
      "expression": "\"% F1 for S class\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% F1 for S class and "
    },
    {
      "expression": "\"% F1 for S\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % F1 for S, showing a "
    },
    {
      "expression": "\"% improvement over LSTMr for S and S classes\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " % improvement over LSTMr for S and S classes, respectively). The LSTM model with sentence-level attentions on both context and reply (LSTMcas +LSTMras ) gives the "
    },
    {
      "expression": "\"F1 score of  \"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1 score of "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% for the S class. For the S class, while we notice an improvement in precision we notice a drop in recall when compared to the LSTM model with sentence level attention only on reply (LSTMras ). We observe the performance ("
    },
    {
      "expression": "\"F1 for S category\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ") deteriorates, probably due to the lack of enough training data."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1707_06226v1_487"
  ],
  "datasets": ["datasets/scigenCL/1707.06226v1-487.json"]
}