{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It seems like there's some confusion in your text as it includes error messages rather than actual numerical values or comparative expressions. However, I'll proceed by assuming that the placeholders where Fluid is supposed to return values are intended to represent explicit values (like percentages), and any mentions of improvement without specifics can be considered comparative or superlative expressions.\n\nLet's try to identify and annotate those parts in your paragraph:\n\nTable \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r shows the results on the Twitter dataset. As for discussion forums, adding context using the SVM models does not show a statistically significant improvement. For the neural networks model, similar to the results on discussion forums, the LSTM models that read both context and reply outperform the LSTM model that reads only the reply (\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r). The \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMcas +LSTMras ). LSTMconditional model shows an improvement of "
    },
    {
      "expression": "\"% F1\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the S class and "
    },
    {
      "expression": "\"F1\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the NS class, compared to LSTMr. For the attentionbased models, the improvement using context is smaller ("
    },
    {
      "expression": "\"F1\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ").\n\nSince no explicit values or comparative expressions are provided in your text, it's challenging to make more specific replacements. If you could provide the actual numerical values or comparative phrases, I'd be happy to annotate them accordingly.\n\nFor example, if we assume some hypothetical values:\n- The improvement on S class is 2%.\n- The improvement on NS class is 1.5%.\n- The smaller F1 improvement for attention-based models is 0.8%.\n\nThen the paragraph would look like this:\n\nTable \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r shows the results on the Twitter dataset. As for discussion forums, adding context using the SVM models does not show a statistically significant improvement. For the neural networks model, similar to the results on discussion forums, the LSTM models that read both context and reply outperform the LSTM model that reads only the reply (\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r). The \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMcas +LSTMras ). LSTMconditional model shows an improvement of "
    },
    {
      "expression": "\"2%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the S class and "
    },
    {
      "expression": "\"1.5%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the NS class, compared to LSTMr. For the attentionbased models, the improvement using context is smaller ("
    },
    {
      "expression": "\"0.8%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ")."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1707_06226v1_488"
  ],
  "datasets": ["datasets/scigenCL/1707.06226v1-488.json"]
}