{"testing-variables":{},"paragraph":[{"type":"literal","value":"Table "},{"expression":"\"3\"","type":"expression"},{"type":"literal","value":" shows the results on the Twitter dataset. As for discussion forums, adding context using the SVM models does not show a statistically significant improvement. For the neural networks model, similar to the results on discussion forums, the LSTM models that read both context and reply outperform the LSTM model that reads only the reply ("},{"expression":"\"LSTMr\"","type":"expression"},{"type":"literal","value":"). The "},{"expression":"\"best performing\"","type":"expression"},{"type":"literal","value":" architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMcas +LSTMras ). LSTMconditional model shows an improvement of "},{"expression":"\"11\"","type":"expression"},{"type":"literal","value":"% F1 on the S class and "},{"expression":"\"4-5%\"","type":"expression"},{"type":"literal","value":"F1 on the  NS class, compared to LSTMr. For the attentionbased models, the improvement using context is smaller ("},{"expression":"\"âˆ¼2%\"","type":"expression"},{"type":"literal","value":" F1)."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1707_06226v1_488"],"datasets":["datasets/scigenCL/1707.06226v1-488.json"]}