{"testing-variables":{},"paragraph":[{"type":"literal","value":"The results of the pre-selection are reported in Table 1. All syllable-aware models comfortably outperform the Char-CNN when the budget is limited to "},{"expression":"\"5M\"","type":"expression"},{"type":"literal","value":" parameters. Surprisingly, a pure word-level model,6 LSTM-Word, also beats the character-aware one under such budget. The three "},{"expression":"\"best\"","type":"expression"},{"type":"literal","value":" configurations are Syl-Concat, Syl-Sum, and Syl-CNN-"}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1707_06480v1_30"],"datasets":["datasets/scigenCL/1707.06480v1-30.json"]}