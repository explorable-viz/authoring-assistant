{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "As shown in Table 4, without using word segmentation, a character-based LSTM-CRF model gives a development F1-score of [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]% on the CoNLL test set, which is "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared with the baseline.\nAdding character-bigram and softword representations as described in Section 3.1 increases the F1-score to [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]% and [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]%, respectively, demonstrating the usefulness of both sources of information.\nIn addition, a combination of both gives a [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]% F1-score, which is "
    },
    {
      "expression": "\"higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " among various character representations.\nTable 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]% F1-score, which is "
    },
    {
      "expression": "\"higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared to the character-based baseline.\nThis demonstrates that both word information and character information are useful for Chinese NER. The two methods of word+char LSTM and word+char LSTM(cid:48), lead to similar improvements.\nA CNN representation of character sequences gives a slightly "
    },
    {
      "expression": "\"lower\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " F1-score compared to LSTM character representations.\nOn the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.\nAs shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of [REPLACE value=\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r]%, which is "
    },
    {
      "expression": "\"higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information. \n\nNote: The placeholder text \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r appears to be an error or irrelevant content in the provided paragraph and it should be replaced with actual values. However, based on the instructions, I've annotated the comparative expressions as requested."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1805_02023v4_203"
  ],
  "datasets": ["datasets/scigenCL/1805.02023v4-203.json"]
}