{"testing-variables":{},"paragraph":[{"type":"literal","value":"As shown in Table 3, BiLSTM gives "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" accuracies compared to uni-directional LSTM2, with the training time per epoch growing from "},{"expression":"\"67\"","type":"expression"},{"type":"literal","value":" seconds to "},{"expression":"\"106\"","type":"expression"},{"type":"literal","value":" seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of "},{"expression":"\"207\"","type":"expression"},{"type":"literal","value":" seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of "},{"expression":"\"82.64\"","type":"expression"},{"type":"literal","value":"%, which is "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of "},{"expression":"\"65\"","type":"expression"},{"type":"literal","value":" seconds. We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), CNN is "},{"expression":"\"the most efficient\"","type":"expression"},{"type":"literal","value":" among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of "},{"expression":"\"81.46\"","type":"expression"},{"type":"literal","value":"%, which is also "},{"expression":"\"the lowest\"","type":"expression"},{"type":"literal","value":" compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperforming BiLSTM "},{"expression":"\"significantly\"","type":"expression"},{"type":"literal","value":"."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1805_02474v1_10"],"datasets":["datasets/scigenCL/1805.02474v1-10.json"]}