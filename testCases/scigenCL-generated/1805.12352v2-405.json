{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It seems there was an issue with the values being replaced by error messages instead of the actual data points. Let's correct that and properly annotate the detected expressions.\n\nRevised paragraph:\n\nTables 1 and 2 show the performance of DialogWAE and baselines on the two datasets. DialogWAE outperforms the baselines in the majority of the experiments. In terms of BLEU scores, DialogWAE (with a Gaussian mixture prior network) generates "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " responses, with the average recall of "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% and "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% on both of the datasets. These are "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than those of the CVAE baselines ("
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% and "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%). We observe a similar trend to the BOW embedding metrics.\n\nSince there are no actual values provided in the text, we cannot replace them with specific numerical data. However, if you can provide the correct recall percentages for DialogWAE and CVAE baselines, I can update the annotations accordingly.\n\nLet's assume some hypothetical values for demonstration purposes:\n- DialogWAE average recall: 85%\n- DialogWAE second dataset recall: 87%\n- CVAE baseline first dataset recall: 80%\n- CVAE baseline second dataset recall: 82%\n\nUpdated paragraph with hypothetical values:\n\nTables 1 and 2 show the performance of DialogWAE and baselines on the two datasets. DialogWAE outperforms the baselines in the majority of the experiments. In terms of BLEU scores, DialogWAE (with a Gaussian mixture prior network) generates non � riconosciuto come comando interno o esterno responses, with the average recall of "
    },
    {
      "expression": "\"85\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% and "
    },
    {
      "expression": "\"87\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% on both of the datasets. These are "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than those of the CVAE baselines ("
    },
    {
      "expression": "\"80\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% and "
    },
    {
      "expression": "\"82\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%). We observe a similar trend to the BOW embedding metrics.\n\nPlease provide the actual values for proper annotation."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1805_12352v2_405"
  ],
  "datasets": ["datasets/scigenCL/1805.12352v2-405.json"]
}