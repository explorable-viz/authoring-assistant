{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It seems like there was a mix-up in the text where it includes an error message (\"non Ã¨ riconosciuto come comando interno o esterno\") instead of actual numerical values or comparative phrases. Assuming that these messages are placeholders for the actual data, I'll demonstrate how to annotate the paragraph based on the given format, using hypothetical comparative and numeric values.\n\nLet's assume the following hypothetical values were supposed to be there:\n- \"3% better than\"\n- \"4%\"\n- \"6%\"\n- \"5%\"\n- \"2% higher\"\n- \"7% higher\"\n- \"8% accuracy\"\n\nUsing these assumed values for demonstration purposes, here is the annotated paragraph:\n\nExperimental Results Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations. On Wikipedia, we observe that the performance of BILSTM, INCEPTION, and JOINT is "
    },
    {
      "expression": "\"3% better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than that of all four baselines. INCEPTION achieves "
    },
    {
      "expression": "\"4\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy than BILSTM. The performance of JOINT achieves an accuracy of "
    },
    {
      "expression": "\"6\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, which is "
    },
    {
      "expression": "\"5\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher than using textual features alone (BILSTM) and "
    },
    {
      "expression": "\"2% higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than using visual features alone (INCEPTION). Based on a one-tailed Wilcoxon signed-rank test, the performance of JOINT results in combination. For arXiv, baseline methods MAJORITY, BENCHMARK, and INCEPTIONFIXED outperform BILSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, INCEPTIONFIXED is "
    },
    {
      "expression": "\"7% higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than MAJORITY and BENCHMARK over the arXiv cs.lg subset, which verifies the usefulness of visual features, even when only the last layer is fine-tuned. Table 3 also shows that INCEPTION and BILSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: INCEPTION and BILSTM are "
    },
    {
      "expression": "\"5% higher\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " over cs.cl; BILSTM achieves "
    },
    {
      "expression": "\"2\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy over cs.lg, while INCEPTION achieves "
    },
    {
      "expression": "\"3\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy over cs.ai. Once again, the JOINT model achieves "
    },
    {
      "expression": "\"8\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% accuracy on cs.ai and cs.cl by combining textual and visual representations (at a level of statistical significance for cs.ai). This, again, confirms that textual and visual results. On arXiv cs.lg, JOINT achieves a "
    },
    {
      "expression": "\"4\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% higher accuracy than INCEPTION by combining visual features and textual features, but BILSTM achieves "
    },
    {
      "expression": "\"3% better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " accuracy. One characteristic of cs.lg documents is\n\nIf the real values were different in your document, you would replace these assumed values with the actual numbers or comparative phrases provided in the text."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1901_01010v2_417"
  ],
  "datasets": ["datasets/scigenCL/1901.01010v2-417.json"]
}