{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Different approaches have been used to solve this task. The \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around "
    },
    {
      "expression": "\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " accuracy on test set which outperforms other models "
    },
    {
      "expression": "\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN (Dauphin et al., 2017) for extraction of single encoding for each paragraph. The accuracy is barely above "
    },
    {
      "expression": "\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", which depicts that this method is not very promising. We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. In the case of fine-tuning, we have used different numbers for "
    },
    {
      "expression": "\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " sequence length to test the capability of BERT in this task. we increased the number of tokens and accuracy respectively increases. We found this method very promising and the accuracy "
    },
    {
      "expression": "\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " increases with respect to previous methods (Table 3). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories.\n\nNote: The text \""
    },
    {
      "expression": "\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "\" is repeated multiple times because it is not clear what values should be extracted for these placeholders. If the actual values were provided, they could replace this placeholder appropriately."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1903_10548v1_262"
  ],
  "datasets": ["datasets/scigenCL/1903.10548v1-262.json"]
}