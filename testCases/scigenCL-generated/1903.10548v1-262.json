{"testing-variables":{},"paragraph":[{"type":"literal","value":"Different approaches have been used to solve this task. The "},{"expression":"\"best\"","type":"expression"},{"type":"literal","value":" result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around "},{"expression":"\"84\"","type":"expression"},{"type":"literal","value":"% accuracy on test set which outperforms other models "},{"expression":"\"significantly\"","type":"expression"},{"type":"literal","value":". First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN (Dauphin et al., 2017) for extraction of single encoding for each paragraph. The accuracy is barely above "},{"expression":"\"50\"","type":"expression"},{"type":"literal","value":"%, which depicts that this method is not very promising. We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. In the case of fine-tuning, we have used different numbers for "},{"expression":"\"maximum\"","type":"expression"},{"type":"literal","value":" sequence length to test the capability of BERT in this task. we increased the number of tokens and accuracy respectively increases. We found this method very promising and the accuracy "},{"expression":"\"significantly\"","type":"expression"},{"type":"literal","value":" increases with respect to previous methods (Table 3). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1903_10548v1_262"],"datasets":["datasets/scigenCL/1903.10548v1-262.json"]}