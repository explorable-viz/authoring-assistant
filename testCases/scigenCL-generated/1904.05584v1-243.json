{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "Table 1 shows the quality of word representations in terms of the correlation between word similarity scores obtained by the proposed models and word similarity scores defined by humans. For that can see First, we each task, character only models had "
    },
    {
      "expression": "\"performance\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than every other model trained on the same dataset. Further, bold results show the overall trend that vector gates outperformed the other methods regardless of training dataset. Additionally, results from the MNLI row in general, and underlined results in particular, show that training on MultiNLI produces word representations "
    },
    {
      "expression": "\"at capturing\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " word similarity. Exceptions to the previous rule are models evaluated in MEN and RW. More notably, in the RareWords dataset (Luthe word only, concat, ong et al., 2013), and scalar gate methods performed equally, despite having been trained in different datasets (p > 0.1), and the char only method performed "
    },
    {
      "expression": "\"when trained\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " MultiNLI. The vector gate, however, performed "
    },
    {
      "expression": "\"than its counterpart\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " trained in SNLI. \n\nNote: It seems like there are some typos or missing parts in your input paragraph which I've kept intact while replacing the expressions that were explicitly detectable according to the provided rules and format."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1904_05584v1_243"
  ],
  "datasets": ["datasets/scigenCL/1904.05584v1-243.json"]
}