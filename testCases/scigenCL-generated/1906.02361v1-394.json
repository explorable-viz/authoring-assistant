{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It appears there's an issue with the text formatting and some placeholder errors (\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non ï¿½ riconosciuto come comando interno o esterno,\r%). I'll assume these are placeholders for actual percentage values that should be annotated. Here's the corrected and annotated version of the paragraph:\n\nTable 2 shows results that compare a BERT baseline that uses only the CQA inputs and the same architecture but trained using inputs that contain explanations from CoS-E during training. The BERT baseline model reaches "
    },
    {
      "expression": "\"XX\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% accuracy and adding open-ended human explanations (CoS-E-open-ended) alongside the questions during training results in a "
    },
    {
      "expression": "\"YY\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% boost in accuracy. In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of "
    },
    {
      "expression": "\"ZZ\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be "
    },
    {
      "expression": "\"WW\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "%.\n\nReplace `XX`, `YY`, `ZZ`, and `WW` with the actual percentage values from the dataset when they become available."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1906_02361v1_394"
  ],
  "datasets": ["datasets/scigenCL/1906.02361v1-394.json"]
}