{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The reinforcement learning model of deep-coref, i.e., deep-coref RL, has "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " significant difference when it is evaluated based on "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " vs. "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " spans (about 4 points). The ensemble model of e2e-coref, on the other hand, has "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " difference between "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span scores (1.4 points), which indicates it "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " recognizes "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span boundaries in out-of-domain data. The ranking of systems is very different by using "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " vs. "
    },
    {
      "expression": "\"min\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " Table 4 shows the "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " vs. "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current state-of-the-art system by Lee et al. (2018). The coreference resolver of Peng et al. (2015) has "
    },
    {
      "expression": "\"non � riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " difference between its "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span evaluation scores. Based on "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " spans, Peng et al. (2015) performs "
    },
    {
      "expression": "\"with cort\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " while cort outperforms it by about one percent when they are evaluated based on "
    },
    {
      "expression": "\"spans\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ".\n\nIt appears there was a misinterpretation of the text due to the repetitive placeholder \"non � riconosciuto come comando interno o esterno\" throughout the paragraph. Assuming this is not meaningful data but rather an error, let's clean up the replacements:\n\nThe reinforcement learning model of deep-coref, i.e., deep-coref RL, has a significant difference when it is evaluated based on "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " vs. "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " spans (about 4 points). The ensemble model of e2e-coref, on the other hand, has a difference between "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span scores (1.4 points), which indicates it recognizes "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span boundaries in out-of-domain data. The ranking of systems is very different by using "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " vs. "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " spans. Table 4 shows the "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " vs. "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current state-of-the-art system by Lee et al. (2018). The coreference resolver of Peng et al. (2015) has a difference between its "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " and "
    },
    {
      "expression": "\"minimum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " span evaluation scores. Based on "
    },
    {
      "expression": "\"maximum\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " spans, Peng et al. (2015) performs better while cort outperforms it by about one percent when they are evaluated based on "
    },
    {
      "expression": "\"spans\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1906_06703v1_481"
  ],
  "datasets": ["datasets/scigenCL/1906.06703v1-481.json"]
}