{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It appears that there might be some text copied incorrectly or is not relevant to the task at hand (the repeated phrase about Fluid not being recognized as a command). I'll ignore this and focus on identifying and annotating the parts of the paragraph based on the given criteria.\n\nHere's the annotated version:\n\nTable 2 reports the results on SNLI, with the configurations that performed "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than our unmodified, non-adversarial InferSent baseline on SNLI's test set, since they remove biases that may be useful for performing this task. The difference for AdvCls is minimal, and it even slightly outperforms InferSent on the validation set. While AdvDat's results are "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than the non-adversarial InferSent, the drops are still less than 6% points.\n\nSince there were no explicit values or superlative/aggregated expressions (like \"the best\", \"highest\") in this paragraph, only comparative expressions were annotated."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1907_04389v1_247"
  ],
  "datasets": ["datasets/scigenCL/1907.04389v1-247.json"]
}