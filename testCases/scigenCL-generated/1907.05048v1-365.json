{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The results using the corrected rank eval TransWeight, the composition model proposed in this paper, delivers consistent results, being \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno, performing model across all languages and phrase types. The difference in performance to the runner-up model, FullLex+, translates into "
    },
    {
      "expression": "\"the\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " test phrases being close to the original repachieving a rank ≤ 5. This resentations, i.e. difference ranges from "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the test phrases in the German compounds dataset to less than "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for English adjective-noun phrases.\n\nHowever, it is important to note the substantial difference in the number of parameters used by the two models: all TransWeight models use 100 transformations and have, therefore, a constant number of "
    },
    {
      "expression": "\"the\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " parameters. In contrast the number of parameters used by FullLex+ increases with the size of the training vocabulary, reaching "
    },
    {
      "expression": "\"the\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " parameters in the case of the English adjective-noun dataset.\n\nThe most difficult task for all the composition models in any of the three languages is compound composition. We believe this difficulty can be mainly attributed to the complexity introduced by the position. For example in adjective-noun composition, the adjective always takes the first position, and the noun the second. However, in compounds the same noun can occur in both positions throughout different training examples. Consider for example the compounds boat house and house boat. In boat house \u2013 a house to store boats \u2013 the meaning of house is shifted towards shelter for an inanimate object, whereas house boat selects from house aspects related to human beings and their daily lives happening on the boat. These positionrelated differences can make it more challenging to create composed representations.\n\nThat makes adverbadjective easier is the high dataset frequency of some of the adverbs/adjectives. For example, in the English and adjective-noun datasets Another aspect the adjective-noun dataset a small subset of "
    },
    {
      "expression": "\"the\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " adjectives like new, good, small, public, etc. are extremely frequent, occurring more than "
    },
    {
      "expression": "\"more than\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " times in the training portion of the adjective-noun sample dataset. Because the adjective is always the first element of the composition, the phrases that include these frequent adjectives amount to around "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of the test dataset. Frequent constituents are more likely to be modeled correctly by composition \u2013 thus leading to "
    },
    {
      "expression": "\"the\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results.\n\nThe additive models (Addition, SAddition, VAddition) are the least competitive models in our evaluation, on all datasets. The results strongly argue for the point that additive models are too limited for composition. An adequate composed representation cannot be obtained simply as an (weighted) average of the input components.\n\nThe Matrix model clearly outperforms the additive models. However, its results are "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in comparison to models like WMask+, BiLinear, FullLex+ and TransWeight. This is to be expected: having a single affine transformation limits the model's capacity to adapt to all the possi  ble input vectors u and v. Because of its small number of parameters, the Matrix model can only capture the general trends in the data.\n\nMore interaction between u and v is promoted by the BiLinear model through the d bilinear forms in the tensor ∈ Rn×d×n. This capacity to absorb more information from the training data translates into "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " results \u2014 the BiLinear model outperforms the Matrix model on all datasets.\n\nIn evaluating FullLex we tried to mitigate its treatment of unknown words. Instead of using unknown matrices to model composition of phrases not in the training data, we take a nearest neighbor approach to composition. Take for example the phrase sky-blue dress, where sky-blue does not occur in train. Our implementation, FullLex+, looks for the nearest neighbor of sky-blue that appears in train, blue and uses the matrix associated with it for building the composed representation. The same approach is also used for the WMask model, which is referred to as WMask+.\n\nOn this dataset WMask+ fares only slightly "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than FullLex+ ("
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "), an indication that FullLex+ suffers from data sparsity in such scenarios and cannot produce good results without an adequate amount of training data. By contrast, the gap between the two models increases considerably on datasets with more phrases per word \u2014 e.g. FullLex+ outperforms WMask+ with "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on the English adjective-noun phrase dataset, which has "
    },
    {
      "expression": "\"the\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " phrases per word.\n\nNote: It seems there are several parts of your text that contain an error message (\"non � riconosciuto come comando interno o esterno\"), so those were not annotated as they do not seem to be part of the intended content."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1907_05048v1_365"
  ],
  "datasets": ["datasets/scigenCL/1907.05048v1-365.json"]
}