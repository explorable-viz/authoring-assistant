{"testing-variables":{},"paragraph":[{"type":"literal","value":"Our proposed approach sets a new state of the art of "},{"expression":"\"84.38\"","type":"expression"},{"type":"literal","value":"% binary accuracy on CMU-MOSI dataset of multimodal sentiment analysis; a significant leap from previous state of "},{"expression":"\"We compare\"","type":"expression"},{"type":"literal","value":".\r\nWe compare the performance of M-BERT with the following models on the multimodal sentiment analysis task: RMFN (SOTA1)1 fuses multimodal information in multiple stages by focusing on a subset of signals in each stage (Liang et al., 2018). MFN (SOTA2) synchronizes states of three separate LSTMs with a multi-view gated memory (Zadeh et al., 2018a). MARN (SOTA3) models view-specific interactions using hybrid LSTM memories and cross-modal interactions using a Multi-Attention Block(MAB) (Zadeh et al., 2018c). We perform two different evaluation tasks on CMU-MOSI datset: i) Binary Classification, and ii) Regression. We formulate it as a regression problem and report Mean-absolute Error (MAE) and the correlation of model predictions with true labels. Besides, we convert the regression outputs into categorical values to obtain binary classification accuracy (BA) and F1 score. The performances of M-BERT and BERT are described in Table 1. M-BERT model outperforms all the baseline models (described in Sec.4.4) on every evaluation metrics with large margin. It sets new state-of-the-art performance for this task and achieves "},{"expression":"\"84.38\"","type":"expression"},{"type":"literal","value":"% accuracy, a "},{"expression":"\"5.98\"","type":"expression"},{"type":"literal","value":"% increase with respect to the SOTA1 and "},{"expression":"\"1.02\"","type":"expression"},{"type":"literal","value":"% increase with respect to BERT (text-only). Even BERT (text-only) model achieves "},{"expression":"\"83.36\"","type":"expression"},{"type":"literal","value":"% accuracy, an increase of "},{"expression":"\"4.96\"","type":"expression"},{"type":"literal","value":"% from the SOTA1 "},{"expression":"\"78.4\"","type":"expression"},{"type":"literal","value":"%, using text information only. It achieves "},{"expression":"\"higher\"","type":"expression"},{"type":"literal","value":" performance in all evaluation metrics "},{"expression":"\"compare to\"","type":"expression"},{"type":"literal","value":" SOTA1; reinforcing the expressiveness and utility of BERT contextual representation.\r\n\r\nNote: The phrase \"We compare\" and \"SOTA1\" are not values that can be directly mapped to Fluid expressions. Also, phrases like \"compare to\" cannot be directly replaced as they need context in a structured dataset for comparison. For these cases, no replacement is done."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1908_05787v1_433"],"datasets":["datasets/scigenCL/1908.05787v1-433.json"]}