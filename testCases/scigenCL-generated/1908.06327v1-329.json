{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The datasets and vision-language task models are described in the appendix, but are referenced in Table 1. Unsurprisingly, when comparing the first lines of Table 1(a,b), we find that using Word2Vec rather than an embedding trained from scratch tends to "
    },
    {
      "expression": "\"improve\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance. This is \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt, whose embeddings trained from scratch using their smaller vocabulary "
    },
    {
      "expression": "\"compare favorably\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to Word2Vec. Word2Vec only falls behind within a point or two across all tasks, and even "
    },
    {
      "expression": "\"outperforms\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " or performs equally as well as FastText for certain tasks (e.g. text-to-clip, image captioning). Table 1 also contains a comparison of language model variants across the five vision-language tasks we evaluate on. We see that fine-tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model (e.g. \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r "
    },
    {
      "expression": "\"increase\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to mean recall on image-sentence retrieval). When comparing the architecture choices from Figure 3 we see that for retrieval-based tasks (i.e. where the output is not free-form text) the Average Embedding and SelfAttention models perform "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " than a simple LSTM-based approach, with Self-Attention being "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " on average. The only apparent exception to this is the text-to-clip task. InferSent and BERT reach comparable values to \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r Word2Vec models for image-sentence retrieval on Flickr30K, performing "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for the MSCOCO dataset. For the remaining retrieval tasks, metrics are below \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r within 1-3 points, again noting the unusual exception of InferSent on phrase grounding of Flickr30K Entities, which significantly drops below scratch performance. While all language models perform closely on ReferIt phrase grounding, this still suggests that there is no need to use the more complex LSTM language model without additional modification. Lastly, sentence level embeddings InferSent and BERT are compared in Table 1(d); results are without fine-tuning. The two are comparable to each other with the exception of phrase grounding accuracy on Flickr30K Entities; BERT surprisingly "
    },
    {
      "expression": "\"outperforms\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " InferSent by \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r%. Both InferSent and BERT do not provide \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r across any task, and thus are not a leading option for vision-language tasks."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1908_06327v1_329"
  ],
  "datasets": ["datasets/scigenCL/1908.06327v1-329.json"]
}