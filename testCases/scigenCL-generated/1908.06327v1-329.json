{"testing-variables":{},"paragraph":[{"type":"literal","value":"The datasets and vision-language task models are described in the appendix, but are referenced in Table 1. Unsurprisingly, when comparing the first lines of Table 1(a,b), we find that using Word2Vec rather than an embedding trained from scratch tends to improve performance. This is "},{"expression":"\"more important\"","type":"expression"},{"type":"literal","value":" when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt, whose embeddings trained from scratch using their smaller vocabulary compare favorably to Word2Vec. Word2Vec only falls behind within a point or two across all tasks, and even outperforms or performs equally as well as FastText for certain tasks (e.g. text-to-clip, image captioning). Table 1 also contains a comparison of language model variants across the five vision-language tasks we evaluate on. We see that fine-tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model (e.g. "},{"expression":"\"5-10%\"","type":"expression"},{"type":"literal","value":" increase to mean recall on image-sentence retrieval). When comparing the architecture choices from Figure 3 we see that for retrieval-based tasks (i.e. where the output is not free-form text) the Average Embedding and SelfAttention models perform "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" than a simple LSTM-based approach, with Self-Attention being "},{"expression":"\"best\"","type":"expression"},{"type":"literal","value":" on average. The only apparent exception to this is the text-to-clip task. InferSent and BERT reach comparable values to "},{"expression":"\"the best\"","type":"expression"},{"type":"literal","value":" Word2Vec models for image-sentence retrieval on Flickr30K, performing "},{"expression":"\"more poorly\"","type":"expression"},{"type":"literal","value":" for the MSCOCO dataset. For the remaining retrieval tasks, metrics are below "},{"expression":"\"the best performing model and embedding combination\"","type":"expression"},{"type":"literal","value":" within 1-3 points, again noting the unusual exception of InferSent on phrase grounding of Flickr30K Entities, which significantly drops below scratch performance. While all language models perform closely on ReferIt phrase grounding, this still suggests that there is no need to use the more complex LSTM language model without additional modification. Lastly, sentence level embeddings InferSent and BERT are compared in Table 1(d); results are without fine-tuning. The two are comparable to each other with the exception of phrase grounding accuracy on Flickr30K Entities; BERT surprisingly outperforms InferSent by "},{"expression":"\"11.55\"","type":"expression"},{"type":"literal","value":"%. Both InferSent and BERT do not provide "},{"expression":"\"the best results\"","type":"expression"},{"type":"literal","value":" across any task, and thus are not a leading option for vision-language tasks."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1908_06327v1_329"],"datasets":["datasets/scigenCL/1908.06327v1-329.json"]}