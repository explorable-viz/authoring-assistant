{"testing-variables":{},"paragraph":[{"type":"literal","value":"We see a small, but consistent improvement across most of the vision-language tasks using GrOVLE as seen in "},{"expression":"\"Table 2(b)\"","type":"expression"},{"type":"literal","value":". These changes result in an embedding with comparable performance to the HGLMM 6K-D features, which are reported in "},{"expression":"\"Table 2(e)\"","type":"expression"},{"type":"literal","value":". However, our word embedding tends to perform "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" when embeddings are the same size (i.e. "},{"expression":"\"300\"","type":"expression"},{"type":"literal","value":"-D). For the generation-based tasks (i.e. captioning and VQA), the benefits of using adapted embeddings are less clear. This may simply be an artifact of the challenges in evaluating these tasks (i.e., the captions are improving in a way the metrics don't capture).  Visual Word2Vec performs comparably amongst results for generation tasks (i.e. image captioning and VQA), but these tasks have little variance in results, with "},{"expression":"\"less than a point\"","type":"expression"},{"type":"literal","value":" of difference across the adapted embeddings.  The small gain provided in generation tasks by Visual Word2Vec does not out-weight the drops in performance across other tasks such as the significant mean recall drop of "},{"expression":"\"6.3\"","type":"expression"},{"type":"literal","value":" compared to HGLMM's 6K-D Self-Attention result in line two of "},{"expression":"\"Table 2(c)\"","type":"expression"},{"type":"literal","value":" and "},{"expression":"\"Table 2(e)\"","type":"expression"},{"type":"literal","value":" for image-sentence retrieval of Flickr30K. For comparison, GrOVLE's Self-Attention result in "},{"expression":"\"Table 2(b)\"","type":"expression"},{"type":"literal","value":" is only "},{"expression":"\"3\"","type":"expression"},{"type":"literal","value":" points lower.  Finally, we report results using HGLMM of different dimension. HGLMM "},{"expression":"\"300\"","type":"expression"},{"type":"literal","value":"-D features are used for a more fair comparison to other embeddings. While the HGLMM 6K-D representation primarily results in the "},{"expression":"\"highest\"","type":"expression"},{"type":"literal","value":" performance, it performs more poorly on generation tasks and also results in high variance. For example, column one in "},{"expression":"\"Table 2(e)\"","type":"expression"},{"type":"literal","value":" shows a range of "},{"expression":"\"7.1\"","type":"expression"},{"type":"literal","value":" in mean recall, unlike GrOVLE which has a range of "},{"expression":"\"2.6\"","type":"expression"},{"type":"literal","value":"."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1908_06327v1_330"],"datasets":["datasets/scigenCL/1908.06327v1-330.json"]}