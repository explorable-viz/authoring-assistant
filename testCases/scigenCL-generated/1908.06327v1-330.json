{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "We see a small, but consistent improvement across most of the vision-language tasks using GrOVLE as seen in \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r. These changes result in an embedding with "
    },
    {
      "expression": "\"comparable\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance to the HGLMM 6K-D features, which are reported in \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r. However, our word embedding tends to perform "
    },
    {
      "expression": "\"comparably\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " when embeddings are the same size (i.e. \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r-D). For the generation-based tasks (i.e. captioning and VQA), the benefits of using adapted embeddings are less clear. This may simply be an artifact of the challenges in evaluating these tasks (i.e., the captions are improving in a way the metrics don't capture).  Visual Word2Vec performs "
    },
    {
      "expression": "\"comparably\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " amongst results for generation tasks (i.e. image captioning and VQA), but these tasks have little variance in results, with "
    },
    {
      "expression": "\"little\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " of difference across the adapted embeddings.  The small gain provided in generation tasks by Visual Word2Vec does not out-weight the drops in performance across other tasks such as the significant mean recall drop of "
    },
    {
      "expression": "\"significant\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " compared to HGLMM's 6K-D Self-Attention result in line two of \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r and "
    },
    {
      "expression": "\"significant\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " for image-sentence retrieval of Flickr30K. For comparison, GrOVLE's Self-Attention result in \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r is only "
    },
    {
      "expression": "\"only\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " points lower.  Finally, we report results using HGLMM of different dimension. HGLMM \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r-D features are used for a more fair comparison to other embeddings. While the HGLMM 6K-D representation primarily results in the "
    },
    {
      "expression": "\"primary\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance, it performs more poorly on generation tasks and also results in high variance. For example, column one in \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r shows a range of "
    },
    {
      "expression": "\"range\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " in mean recall, unlike GrOVLE which has a range of "
    },
    {
      "expression": "\"range\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1908_06327v1_330"
  ],
  "datasets": ["datasets/scigenCL/1908.06327v1-330.json"]
}