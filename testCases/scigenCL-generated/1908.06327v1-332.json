{"testing-variables":{},"paragraph":[{"type":"literal","value":"Table 3 reports results of the multi-task training procedure described above. We use "},{"expression":"\"the best performing\"","type":"expression"},{"type":"literal","value":" language model in our comparisons for each task, i.e. Self-Attention for image-sentence retrieval and phrase grounding, and the LSTM language model for text-to-clip, image captioning, and VQA. The first lines of Table 3 report the results of the original fixed GrOVLE embedding, which should be considered the baseline. The second line of Table 3 reports "},{"expression":"\"performance\"","type":"expression"},{"type":"literal","value":" when the four-task pretrained GrOVLE is fixed when used in the target task, i.e. the task currently being run. The third and fourth line of Table 3 report the results of our embedding when they were trained on all five tasks, and kept fixed or fine-tuned for the target task, respectively.  The "},{"expression":"\"results\"","type":"expression"},{"type":"literal","value":" of line three and four demonstrate that our improved embedding tends to transfer "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" when applied with fine-tuning during the target task. We find similar trends in performance improvements across tasks: larger gains occur for image-sentence retrieval with +7.9 mean recall for the Flickr30K dataset and +6.3 for MSCOCO. All other tasks have performance "},{"expression":"\"improvements\"","type":"expression"},{"type":"literal","value":" under one point, showing that while the vision-language tasks appear to transfer well without harming performance, they are leveraged most in image-sentence retrieval, with an exception of phrase grounding accuracy on ReferIt (+2.36%)."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1908_06327v1_332"],"datasets":["datasets/scigenCL/1908.06327v1-332.json"]}