{"testing-variables":{},"paragraph":[{"type":"literal","value":"Table 1 shows that finetuned BERT outperforms pre-trained BERT by a significant margin on all the tasks (with an average of "},{"expression":"\"35.9\"","type":"expression"},{"type":"literal","value":" points of absolute difference).  BERT with weights initialized from normal distribution and further fine-tuned for a given task consistently produces lower scores than the ones achieved with pre-trained BERT. In fact, for some tasks (STS-B and QNLI), initialization with random weights yields "},{"expression":"\"worse\"","type":"expression"},{"type":"literal","value":" performance than pre-trained BERT without fine-tuning."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1908_08593v2_268"],"datasets":["datasets/scigenCL/1908.08593v2-268.json"]}