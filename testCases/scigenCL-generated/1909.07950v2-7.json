{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The provided paragraph does not contain any explicit values, comparative expressions, or superlative/aggregated expressions that can be annotated according to the given task description. It primarily discusses the methods and comparisons of different models but lacks specific metrics or comparative phrases like \"better,\" \"worse,\" \"the best,\" etc.\n\nIf there were specific numbers or comparison phrases in the paragraph, they would be replaced with `"
    },
    {
      "expression": "\"...\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "`. However, based on the current content, no replacements are needed. Here's the paragraph for clarity:\n\nWe use two pre-trained deep models: a CNN and an LSTM as baselines (BL) to extract the initial list of word hypotheses. We experimented extracting k-best hypotheses for k = 1 . . . 10. Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the words of the CNN dictionary (which correspond to a portion of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the k-best produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR), We compare the results of our encoder with several state-of-the-art sentence encoders, tuned or trained on the same dataset. Table are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset. Our model FDCLSTM without attention achieves a result in the case of the second baseline LSTM that full of false-positives and short words. We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve performance than our TWE previous model, that trained a word embedding from scratch on the same task. As seen in Table, the introduction of this unigram lexicon produces results.\n\nSince there are no specific values or comparative expressions to annotate, no further changes are necessary."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1909_07950v2_7"
  ],
  "datasets": ["datasets/scigenCL/1909.07950v2-7.json"]
}