{"testing-variables":{},"paragraph":[{"type":"literal","value":"We use two pre-trained deep models: a CNN ("},{"expression":"\"Jaderberg et al.\"","type":"expression"},{"type":"literal","value":", "},{"expression":"\"2016\"","type":"expression"},{"type":"literal","value":") and an LSTM ("},{"expression":"\"Ghosh et al.\"","type":"expression"},{"type":"literal","value":", "},{"expression":"\"2017\"","type":"expression"},{"type":"literal","value":") as baselines (BL) to extract the initial list of word hypotheses. We experimented extracting k-best hypotheses for k = 1 . . . 10. Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the "},{"expression":"\"90K\"","type":"expression"},{"type":"literal","value":" words of the CNN dictionary (which correspond to "},{"expression":"\"43.3%\"","type":"expression"},{"type":"literal","value":" of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the k-best produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR), We compare the results of our encoder with several state-of-the-art sentence encoders, tuned or trained on the same dataset. Table "},{"expression":"\"1\"","type":"expression"},{"type":"literal","value":" are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset. Our model FDCLSTM without attention achieves a "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" result in the case of the second baseline LSTM that full of false-positives and short words. We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" performance than our TWE previous model ("},{"expression":"\"Sabir et al.\"","type":"expression"},{"type":"literal","value":", "},{"expression":"\"2018\"","type":"expression"},{"type":"literal","value":"), that trained a word embedding ("},{"expression":"\"Mikolov et al.\"","type":"expression"},{"type":"literal","value":", "},{"expression":"\"2013\"","type":"expression"},{"type":"literal","value":") from scratch on the same task. As seen in Table "},{"expression":"\"1\"","type":"expression"},{"type":"literal","value":", the introduction of this unigram lexicon produces "},{"expression":"\"the best\"","type":"expression"},{"type":"literal","value":" results."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1909_07950v2_7"],"datasets":["datasets/scigenCL/1909.07950v2-7.json"]}