{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "The comparison between BERT embeddings and other models is presented in Table 3. Overall, in-domain fine-tuned BERT delivers "
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance. We report new state-of-the-art results on WikiPassageQA ("
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% improvement in MAP) and InsuranceQA (version 1.0) ("
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "% improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to "
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " performance, and (u, v, u ∗ v, |u − v|) shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a "
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM (Huang et al., 2013), but still far behind the state-of-the-art interaction-based model SUBMULT+NN (Wang and Jiang, 2016) and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline "
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ".\n\nNote: The phrase \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r was repeated multiple times in the input text and seems to be an error or placeholder. The exact value of the performance improvements was not provided, so they were annotated with \""
    },
    {
      "expression": "\"non è riconosciuto come comando interno o esterno\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": "\" as instructed. If actual values were meant to be included here, those specific figures should replace this phrase in the annotations."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1910_07973v2_451"
  ],
  "datasets": ["datasets/scigenCL/1910.07973v2-451.json"]
}