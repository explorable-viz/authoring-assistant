{
  "testing-variables": {},
  "paragraph": [{
    "type": "literal",
    "value": "The model has relatively low accuracy on the entailment and contradiction examples while \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r accuracy on the neutral ones. The accuracy of the model on each test set. On our dative alternation dataset, the accuracy on our test sets is \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r on the MultiNLI development set (\"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r% versus \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r% respectively), The model has very high accuracy on the entailment examples, while \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r on the contradiction ones. On the numerical reasoning dataset, the model also seems to fail on this inference type with test set accuracy \"C:\\Users\\Alfy\\Lavoro\\bristol\\transp-text-new\\node_modules\\.bin\\fluid\" non � riconosciuto come comando interno o esterno,\r on the MultiNLI development set,\n\nIt appears that there are repeated instances of an error message in your paragraph which does not seem to be related to any numerical values or comparative expressions. If you have specific accuracy numbers that need replacing with Fluid expressions, please provide those details.\n\nIf you have any specific numbers or comparative phrases you would like me to annotate, feel free to clarify and I will adjust the output accordingly!"
  }],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_1910_09302v1_224"
  ],
  "datasets": ["datasets/scigenCL/1910.09302v1-224.json"]
}