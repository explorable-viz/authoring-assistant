{"testing-variables":{},"paragraph":[{"type":"literal","value":"We present the test results in Table 2. We are able to reproduce the performance of the baseline model ("},{"expression":"\"CNN w/ GloVe\"","type":"expression"},{"type":"literal","value":"), and find that once again, adding the shallow discourse features improves results. Interestingly, we found that replacing the CNN with an LSTM results in improved MAE, but "},{"expression":"\"worse\"","type":"expression"},{"type":"literal","value":" MAPE. Adding discourse features to this model generally has "},{"expression":"\"marginal improvement\"","type":"expression"},{"type":"literal","value":" in all cases. When we replace the word sequence with EDUs ("},{"expression":"\"Bi-LSTM w/ latent\"","type":"expression"},{"type":"literal","value":" and "},{"expression":"\"Bi-LSTM w/ shallow\"","type":"expression"},{"type":"literal","value":"), we see that the latent features "},{"expression":"\"outperform\"","type":"expression"},{"type":"literal","value":" the shallow features."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_1911_06919v1_311"],"datasets":["datasets/scigenCL/1911.06919v1-311.json"]}