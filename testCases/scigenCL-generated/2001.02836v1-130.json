{"testing-variables":{},"paragraph":[{"type":"literal","value":"The experimental results are shown in Table 1. As Keller is created based on the PP distribution and have "},{"expression":"\"relatively small\"","type":"expression"},{"type":"literal","value":" size while SP-10K is created based on random sampling and has a "},{"expression":"\"much larger\"","type":"expression"},{"type":"literal","value":" size, we treat the performance on SP-10K as the major evaluation. Our embeddings significantly outperform other baselines, especially embedding based baselines. The only exception is PP on the Keller dataset due to its "},{"expression":"\"biased\"","type":"expression"},{"type":"literal","value":" distribution. In addition, there are other interesting observations. First, "},{"expression":"\"compared with\"","type":"expression"},{"type":"literal","value":" 'dobj' and 'nsubj', 'amod' is "},{"expression":"\"simpler\"","type":"expression"},{"type":"literal","value":" for word2vec and GloVe. The reason behind is that conventional embeddings only capture the co-occurrence information, which is enough to predict the selectional preference of"}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_2001_02836v1_130"],"datasets":["datasets/scigenCL/2001.02836v1-130.json"]}