{"testing-variables":{},"paragraph":[{"type":"literal","value":"We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE "},{"expression":"\"outperforms\"","type":"expression"},{"type":"literal","value":" ELMo and achieves "},{"expression":"\"comparable results\"","type":"expression"},{"type":"literal","value":" with BERT with smaller embedding dimension and much less training complexities."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_2001_02836v1_132"],"datasets":["datasets/scigenCL/2001.02836v1-132.json"]}