{"testing-variables":{},"paragraph":[{"type":"literal","value":"Various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples. with and without "},{"expression":"\"pre-training\"","type":"expression"},{"type":"literal","value":" character level embedding and comparing the proposed architectures on the held-out dataset. To illustrate the need of pre-trained character embeddings, we see from III that using a "},{"expression":"\"pre-trained\"","type":"expression"},{"type":"literal","value":" embedding increases the accuracy across datasets and the different number of authors, regardless of the amount of data for each author.  increase the performance a few degrees.  we analyzed the importance of "},{"expression":"\"pretrained\"","type":"expression"},{"type":"literal","value":" character embedding for author attribution and showed that pre-training can result in "},{"expression":"\"better\"","type":"expression"},{"type":"literal","value":" performances."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_2001_05316v1_300"],"datasets":["datasets/scigenCL/2001.05316v1-300.json"]}