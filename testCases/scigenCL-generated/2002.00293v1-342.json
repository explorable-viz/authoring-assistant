{
  "testing-variables": {},
  "paragraph": [{
    "type": "literal",
    "value": "It seems there's some confusion with the text as it includes error messages such as \"non Ã¨ riconosciuto come comando interno o esterno\" which means \"is not recognized as an internal or external command.\" This is likely due to a copy-paste mistake, and these parts do not contain any data or expressions that can be replaced by Fluid.\n\nAssuming you would like me to look for explicit values (scores, names, numbers) without the error messages, let's reformat the text:\n\n---\n\nFor example, RoBERTa trained on DRoBERTa reaches F1 on DRoBERTa, and this number further increases to F1 when including SQuAD during training (cf. Table 6). In Table 6 we show experimental results for the same models and training datasets, but now including SQuAD as additional training data. In this training setup we generally see improved generalisation to DBiDAF, DBERT, and DRoBERTa. Interestingly, the relative differences between DBiDAF, DBERT, and DRoBERTa as training set used in conjunction with SQuAD are now much diminished, and especially DRoBERTa as (part of the) training set now generalises. RoBERTa achieves results on any of the DBiDAF, DBERT, and DRoBERTa evaluation sets, in particular when trained on DSQuAD+DRoBERTa. We identify a risk of datasets constructed with weaker models in the loop becoming outdated. For example, RoBERTa achieves EM/F1 on DBiDAF, in contrast to EM/F1 for BiDAF \u2013 which is not far from non-expert human performance of EM/F1. We furthermore observe a gradual decrease in generalisation to SQuAD when training on DBiDAF towards training on DRoBERTa. This suggests that the stronger the model used in the annotation loop, the more dissimilar the data distribution becomes from the original SQuAD distribution. We will later find further support for this explanation in a qualitative analysis (Section 5). It may however also be due to a limitation of BERT and RoBERTa \u2013 similar to BiDAF \u2013 in learning from a data distribution designed to beat these models; an even stronger model might learn more e.g. from DRoBERTa.\n\n---\n\nGiven the current version, there are no explicit values or comparative/superlative expressions to annotate.\n\nIf you have specific values and comparisons that should be included, please provide those details, and I will update the paragraph accordingly."
  }],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_2002_00293v1_342"
  ],
  "datasets": ["datasets/scigenCL/2002.00293v1-342.json"]
}