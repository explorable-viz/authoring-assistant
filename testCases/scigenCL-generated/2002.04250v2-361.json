{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It appears that there's an issue with the paragraph provided, as it seems to be repeatedly using a placeholder or an error message (\"non Ã¨ riconosciuto come comando interno o esterno\") instead of the actual model names. Assuming this is not intended and we proceed with the intended comparison between \"AR\", \"AR+MMI\", and \"non-AR\" models, here's how you would annotate the paragraph:\n\nResults are shown in Table 1. When comparing AR with AR+MMI, AR significantly outperforms AR across all metrics, a lookahead strategy to estimate N-best list for reranking, and thus outperforms AR; AR uses future and thus outperforms backward probability, as well. It's hard to tell which model performs "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ", AR or non-AR: non-AR performs "
    },
    {
      "expression": "\"than\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " non-AR for BLEU and adversarial success, but "
    },
    {
      "expression": "\"for the other metrics\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ". This means comparing with non-AR model, non-AR model tends to generate more diverse responses, but might be less coherent. When comparing non-AR with AR, non-AR has relatively lower distinct score, but significantly scores BLEU and adversarial success.\n\nSince the paragraph contains placeholders instead of actual numbers or explicit values for scores, we can only annotate the comparative expressions found here (\"better\", \"than\"). If you have a corrected version of the paragraph, please provide it so I can assist with more precise annotations."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_2002_04250v2_361"
  ],
  "datasets": ["datasets/scigenCL/2002.04250v2-361.json"]
}