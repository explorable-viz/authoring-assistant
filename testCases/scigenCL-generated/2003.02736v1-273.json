{"testing-variables":{},"paragraph":[{"type":"literal","value":"The results for the tested systems are given in Table 2. Again we saw "},{"expression":"\"large gains\"","type":"expression"},{"type":"literal","value":" from the BERT based models over the baseline from (Zubiaga et al., 2017) and the 2-layer BiLSTM. Compared to training solely on PHEME, fine tuning from basic citation needed detection saw very little improvement ("},{"expression":"\"0.1\"","type":"expression"},{"type":"literal","value":" F1 points). However, fine tuning with a model trained using PU learning led to an increase of "},{"expression":"\"1\"","type":"expression"},{"type":"literal","value":" F1 point over the non-PU learning model, indicating that PU learning enables the Wikipedia data to be useful for transferring to rumour detection. For PUC, we saw an improvement of "},{"expression":"\"0.7\"","type":"expression"},{"type":"literal","value":" F1 points over the baseline and "},{"expression":"\"lower overall variance\"","type":"expression"},{"type":"literal","value":" than vanilla PU learning, meaning that the results with PUC are "},{"expression":"\"more consistent across runs\"","type":"expression"},{"type":"literal","value":". When models are ensembled, pretraining with vanilla PU learning improved over no pretraining by almost "},{"expression":"\"2\"","type":"expression"},{"type":"literal","value":" F1 points."}],"variables":{},"imports":["scigen","util","datasets/scigenCL/_2003_02736v1_273"],"datasets":["datasets/scigenCL/2003.02736v1-273.json"]}