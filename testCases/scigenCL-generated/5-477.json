{
  "testing-variables": {},
  "paragraph": [
    {
      "type": "literal",
      "value": "It appears that there is a repetition of the error message \"non Ã¨ riconosciuto come comando interno o esterno\" in your paragraph, which seems out of context and unrelated to the content you're trying to describe about models' performance on text summarization tasks.\n\nTo annotate the paragraph according to Fluid expression detection rules, I'll first remove the repetitive error messages. Then, I'll identify and replace relevant expressions with Fluid annotations:\n\nFor Table 2 shows that in this sample from the validation data, the fraction of incorrect summaries at first position, when the alternatives are ranked as during beam search, is at "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": ".\nUsing entailment probabilities of ESIM and InferSent, we can slightly improve upon that and reduce incorrect summaries. However, with DA and SSE, more incorrect summaries end up in the first position. Note that these results are not in line with the model's NLI accuracies, underlining that performance on NLI does not directly transfer to our task. Only for BERT, which outperforms the other models on NLI, we also see substantially "
    },
    {
      "expression": "\"better\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " reranking performance. But even for this powerful model, more than half of the errors still remain in the summaries.\nInterestingly, we also find that for ESIM and InferSent, reranking hurts in many cases, leaving just a few cases of net improvement. Given the validation results, we then applied reranking to the CNN-DM test data followed by a post-hoc correctness evaluation as in Section 4. We used the ESIM model and reranked all beam hypotheses generated by FAS.\nIn contrast to the validation sample, the fraction of incorrect summaries increases from "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " to "
    },
    {
      "expression": "\"%\"",
      "type": "expression"
    },
    {
      "type": "literal",
      "value": " (Table 2), demonstrating that the slight improvement on the validation data does not transfer to the test set.\n\nNote: The percentages in this paragraph are represented with a '%' symbol without specific numeric values. To accurately replace these with Fluid annotations, we would need the actual percentage values from the Table 2 dataset."
    }
  ],
  "variables": {},
  "imports": [
    "scigen",
    "util",
    "datasets/scigenCL/_5_477"
  ],
  "datasets": ["datasets/scigenCL/5-477.json"]
}