import scigen
import util
import datasets.scigen._1805_02474v1_10
let model_ name = model name tableData
in

"""
	As shown in Table 3, BiLSTM gives significantly ${trendWord (model_ "BiLSTM")._acc (model_ "LSTM")._acc betterWorse} accuracies compared to uni-directional LSTM2, with the training time per epoch ${trendWord (model_ "BiLSTM")._time (model_ "LSTM")._time growShrink} from ${(model_ "LSTM")._time} seconds to ${(model_ "BiLSTM")._time} seconds. Stacking 2 layers of BiLSTM gives ${trendWord (model_ "2 stacked BiLSTM")._acc (model_ "BiLSTM")._acc improvements} to development results, with a ${trendWord (model_ "2 stacked BiLSTM")._time (model_ "BiLSTM")._time smallerHigher} time of ${(model_ "2 stacked BiLSTM")._time} seconds. 3 layers of stacked BiLSTM ${trendWord (model_ "3 stacked BiLSTM")._acc (model_ "BiLSTM")._acc improve} the results. In contrast, S-LSTM gives a development result of ${(model_ "S-LSTM")._acc} %, which is significantly ${trendWord (model_ "S-LSTM")._acc (model_ "2 stacked BiLSTM")._acc betterWorse} compared to 2-layer stacked BiLSTM, with a ${trendWord (model_ "S-LSTM")._no_param (model_ "2 stacked BiLSTM")._no_param smallerHigher} number of model parameters and a ${trendWord (model_ "S-LSTM")._time (model_ "2 stacked BiLSTM")._time shorterLonger} time of ${(model_ "S-LSTM")._time} seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), ${(findWithKey_ "_time" (minimum (map (fun y -> y._time) tableData)) tableData)._model} is the ${rankLabel "most efficient" (findIndex "_model" "CNN" (insertionSort (fun a b -> a._time < b._time) tableData))} among all models compared, with the ${rankLabel "smallest" (findIndex "_model" "CNN" (insertionSort (fun a b -> a._no_param < b._no_param) tableData))} model size. On the other hand, a 3-layer stacked CNN gives an accuracy of ${(model "3 stacked CNN" tableData)._acc} %, which is also the ${rankLabel "lowest" (findIndex "_model" "CNN" (insertionSort (fun a b -> a._time < b._time) tableData))} compared with BiLSTM, hierarchical attention and S-LSTM. The ${rankLabel "best" (findIndex "_model" "S-LSTM+Attention" (insertionSort (fun a b -> b._acc < a._acc) tableData))} performance of hierarchical attention is obtained by S-LSTM+Attention in terms of both accuracy and efficiency. S-LSTM gives significantly ${trendWord (model_ "S-LSTM")._acc (model_ "CNN")._acc betterWorse} accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used. Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still ${trendWord (model_ "S-LSTM")._acc (model_ "BiLSTM")._acc underOverPerforming} BiLSTM significantly.
"""
