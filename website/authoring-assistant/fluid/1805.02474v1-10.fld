import scigen
import util
import datasets.scigen._1805_02474v1_10
let model_ name = model name tableData in f"""
As shown in Table 3, BiLSTM gives significantly
${trendWord (model_ "BiLSTM").acc (model_ "LSTM").acc betterWorse}
accuracies compared to uni-directional LSTM2, with the training time per epoch
${trendWord (model_ "BiLSTM").time_s (model_ "LSTM").time_s growShrink} from
${(model_ "LSTM").time_s} seconds to ${(model_ "BiLSTM").time_s} seconds.
...
We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017),
shown in Table 3 (the CNN and Transformer rows),
${(findWithKey_ "time_s" (minimum (map_ (fun y -> y.time_s) tableData)) tableData).model} is the
${rankLabel "most efficient" (findIndex "model" "CNN" (sort (fun a b -> a.time_s < b.time_s) tableData))}
among all models compared, with the
${rankLabel "smallest" (findIndex "model" "CNN" (sort (fun a b -> a.param < b.param) tableData))}
model size. On the other hand, a 3-layer stacked CNN gives an accuracy of
${(model "3 stacked CNN" tableData).acc} %, which is also the
${rankLabel "lowest" (findIndex "model" "CNN" (sort (fun a b -> a.time_s < b.time_s) tableData))}
compared with BiLSTM, hierarchical attention and S-LSTM. The
${rankLabel "best" (findIndex "model" "S-LSTM+Attention" (sort (fun a b -> b.acc < a.acc) tableData))}
performance of hierarchical attention is obtained by S-LSTM+Attention in terms of both accuracy
and efficiency. S-LSTM gives significantly
${trendWord (model_ "S-LSTM").acc (model_ "CNN").acc betterWorse}
accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of
BiLSTM and S-LSTM when external attention is used. Attention leads to improved accuracies for both
BiLSTM and S-LSTM in classification, with S-LSTM still
${trendWord (model_ "S-LSTM").acc (model_ "BiLSTM").acc underOverPerforming} BiLSTM significantly.
"""


