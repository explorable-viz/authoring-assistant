
Paragraph([Text " As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM2, with the training time per epoch growing from ",
Text(numToStr(head (map (fun x -> x.time_s) (filter (fun x -> x.model == "LSTM") tableData)))),
Text " seconds to ",
Text(numToStr(head (map (fun x -> x.time_s) (filter (fun x -> x.model == "BiLSTM") tableData)))),
Text " seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of ",
Text(numToStr(head (map (fun x -> x.time_s) (filter (fun x -> x.model == "2 stacked BiLSTM") tableData)))),
Text " seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of ",
Text(numToStr(head (map (fun x -> x.acc) (filter (fun x -> x.model == "S-LSTM") tableData)))),
Text " %, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of  ",
Text(numToStr(head (map (fun x -> x.time_s) (filter (fun x -> x.model == "S-LSTM") tableData)))),
Text " seconds.  We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), ",
Text(head (map (fun x -> x.model) (filter (fun x -> x.time_s == minimum (map (fun y -> y.time_s) tableData)) tableData))),
Text " is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of  ",
Text(numToStr(head (map (fun x -> x.acc) (filter (fun x -> x.model == "3 stacked CNN") tableData)))),
Text " %, which is also the  ",
Text(let pos = get_rank "CNN" tableData in ordinal pos),
Text " compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention. Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used  Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperforming BiLSTM significantly. "])
