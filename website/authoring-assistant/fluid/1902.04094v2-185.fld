import scigen
import util
import datasets.scigen_SuggestionAgent._1902_04094v2_185

"""
	We present sample generations, quality results, and diversity results respectively in Tables  ${"1"} ,  ${"2"} ,  ${"3"} .  We find that, compared to  ${"GPT"} , the  ${"BERT"}  generations are of  ${"worse"}  quality, but are  ${"more diverse"} . Surprisingly, the outside language model, which was trained on Wikipedia, is  ${"less perplexed"}  by the  ${"GPT"}  generations than the  ${"BERT"}  generations, even though GPT was only trained on romance novels and BERT was trained on romance novels and Wikipedia. On actual data from TBC, the outside language model is about as perplexed as on the  ${"BERT"}  generations, which suggests that domain shift is an issue in using a trained language  model for evaluating generations and that the  ${"GPT"}  generations might have collapsed to fairly generic and simple sentences.  The perplexity on BERT samples is not absurdly high, and in reading the samples, we find that many are fairly coherent.
"""
