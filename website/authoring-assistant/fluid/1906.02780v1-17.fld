
Paragraph([Text " Table 4 shows that increasing the number of layers from 1 to 5 results in a BLEU increase of only  ",
Text(numToStr(last (map (fun x -> x.bleu) (filter (fun x -> (x.layers == 5) `and` (x.max_chunk_size == "k=6")) tableData)) - head (map (fun x -> x.bleu) (filter (fun x -> (x.layers == 1) `and` (x.max_chunk_size == "k=6")) tableData)))),
Text " , while the speedup drops from  ",
Text(head (map (fun x -> x.speedup) (filter (fun x -> (x.layers == 1) `and` (x.max_chunk_size == "k=6")) tableData))),
Text " to  ",
Text(last (map (fun x -> x.speedup) (filter (fun x -> x.layers == 5) tableData))),
Text " . The final row of Table 4 shows that exposing the parse decoder to multiple possible chunking of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving BLEU by  ",
Text(numToStr(head (map (fun x -> x.bleu) (filter (fun x -> (x.max_chunk_size == "k in {1...6}")) tableData)) - head (map (fun x -> x.bleu) (filter (fun x -> (x.layers == 1) `and` (x.max_chunk_size == "k=6")) tableData)))),
Text "  while decreasing the speedup from  ",
Text(head (map (fun x -> x.speedup) (filter (fun x -> (x.layers == 1) `and` (x.max_chunk_size == "k=6")) tableData))),
Text "  to  ",
Text(head (map (fun x -> x.speedup) (filter (fun x -> (x.max_chunk_size == "k in {1...6}")) tableData)))])
