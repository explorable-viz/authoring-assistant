
Paragraph [
	Text "We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), ",
	Text ("CNN"),
	Text " is the most efficient among all models compared, with the ",
	Text ("highest"),
	Text " model size. On the other hand, a 3-layer stacked CNN gives an accuracy of ",
	Text ("81.46"),
	Text "%, which is also the ",
	Text ("maximum"),
	Text " compared with BiLSTM, hierarchical attention and S-LSTM."
]
